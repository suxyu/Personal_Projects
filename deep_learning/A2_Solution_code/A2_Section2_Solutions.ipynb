{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVG7Al_4hRHn"
      },
      "source": [
        "## Section 2: Coding Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3R45naihRHq"
      },
      "source": [
        "This section consists of coding questions. The total mark for this section is **85**.\n",
        "The task we are considering in this section is the text classification task. In this section, some code has been provided to help you get started. For each cell marked with **# Insert your code here**, these are placeholders where you **must** supply your own codes when instructed.\n",
        "This section consists of 5 parts:\n",
        "* Part 0: Downloading and preprocessing data. This part has been completed. No marks are allocated for this part.\n",
        "* Part 1: Coding assessment on using Word2Vect to transform texts to vectors (20 marks).\n",
        "* Part 2: Coding assessment on Text CNN for sequence modeling and neural embedding (10 marks).\n",
        "* Part 3: Coding assessment on RNNs for sequence modeling and neural embedding (32 marks).\n",
        "* Part 4: Coding assessment on Transformer for sequence modeling and neural embedding and the overall ranking (23 marks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbry5hvzhRHr"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Set random seeds</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSxNEThGhRHr"
      },
      "source": [
        "We start with importing tensorflow and numpy and setting random seeds for TF and numpy. You can use any seeds you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FMuOtVV2hRHr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(6789)\n",
        "np.random.seed(6789)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdFNjsnFhRHt"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 0: Download and preprocess the data</span>\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCf2B5tBhRHu"
      },
      "source": [
        "The dataset we use for this assignment is a question classification dataset for which the training set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
        "- abbreviation (ABBR),\n",
        "- entity (ENTY),\n",
        "- description (DESC),\n",
        "- human (HUM),\n",
        "- location (LOC) and\n",
        "- numeric (NUM).\n",
        "\n",
        "In this assignment, we will utilize a subset of this dataset, containing $2,000$ questions for training and validation. We will use 80% of those 2000 questions for trainning and the rest for validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzxvX1fahRHu"
      },
      "source": [
        "Preprocessing data is a crucial initial step in any machine learning or deep learning project. The *TextDataManager* class simplifies the process by providing functionalities to download and preprocess data specifically designed for the subsequent questions in this assignment. It is highly recommended to gain a comprehensive understanding of the class's functionality by **carefully reading** the content provided in the *TextDataManager.py* file before proceeding to answer the questions in Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYkQEo0uhRHv",
        "outputId": "6c25fa13-ba4c-42ef-e25d-2b1816b87e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloaded successfully train_2000.label\n",
            "\n",
            "Sample questions and corresponding labels... \n",
            "\n",
            "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
            "['DESC', 'ENTY', 'DESC', 'ENTY', 'ABBR']\n"
          ]
        }
      ],
      "source": [
        "from TextDataManager import SimpleTextDataset as DataManager\n",
        "\n",
        "print('Loading data...')\n",
        "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
        "\n",
        "dm = DataManager(maxlen=100)\n",
        "dm.read_data(\"data/\", [\"train_2000.label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bbf7dSAAhRHw"
      },
      "outputs": [],
      "source": [
        "dm.manipulate_data()\n",
        "dm.train_valid_split(train_ratio=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kKa2hfaThRHx"
      },
      "outputs": [],
      "source": [
        "#print(dm.train_str_questions).  #1600\n",
        "#print(dm.train_numeral_data.shape).   #(1600,100)\n",
        "#print(dm.valid_numeral_data.shape).  #(400,100)\n",
        "#print(dm.train_numeral_labels.shape).    #(1600,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(len(dm.train_str_questions))"
      ],
      "metadata": {
        "id": "UURYMyHVbQav"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(dm.train_numeral_data)#looks like each word corresponds to a unique number"
      ],
      "metadata": {
        "id": "GK9tcAXYYRdE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(dm.train_numeral_labels)\n",
        "#print(dm.train_numeral_data)\n"
      ],
      "metadata": {
        "id": "eVrI4VYmvC_K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBiXEAmihRHx"
      },
      "source": [
        "You now have a data manager, named *dm* containing the training and validiation sets in both text and numeric forms. Your task is to play around and read this code to figure out the meanings of some important attributes that will be used in the next parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv995BFfhRHx"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 1: Using Word2Vect to transform texts to vectors </span>\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 20 marks]<span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1sinhO-hRHy"
      },
      "source": [
        "In this part, you will be assessed on how to use a pretrained Word2Vect model for realizing a machine learning task. Basically, you will use this pretrained Word2Vect to transform the questions in the above dataset stored in the *data manager object dm* to numeric form for training a classifier using Logistic Regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wPAt8m6ohRHy"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdMRPKSYhRHy"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 1.1**</span>\n",
        "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
        "\n",
        "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4vOpKbPphRHy"
      },
      "outputs": [],
      "source": [
        "# Insert your code here\n",
        "word2vect = api.load(\"glove-wiki-gigaword-100\")#load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Hm0kAmpDhRHz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def get_word_vector(word, model):\n",
        "    try:\n",
        "        vector = model[word]\n",
        "    except KeyError:\n",
        "        vector = np.zeros(100)\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##test\n",
        "#word1 = \"computer\"\n",
        "#vector1 = get_word_vector(word1,word2vect)\n",
        "#print(vector1)"
      ],
      "metadata": {
        "id": "UcGSbK0jntkd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJlAK1y_hRHz"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 1.2**</span>\n",
        "\n",
        "**Write the code for the function `get_sentence_vector(sentence, important_score=None, model= None)`. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
        "- $important\\_weight = softmax(important\\_score)$.\n",
        "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[T]\\times v[T]$ where $T$ is the length of the sentence and $v[i]$ is the vector representation of the $i-th$  word in this sentence.\n",
        "\n",
        "**Note that if `important_score=None` is set by default, your function should return the average of all representation vectors corresponding to set `important_score=[1,1,...,1]`.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFoDVwzRhRHz",
        "outputId": "54f9ff36-0e26-4371-a2d6-033b47383c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.09003057 0.24472847 0.66524096]\n"
          ]
        }
      ],
      "source": [
        "from numpy import exp\n",
        "# softmax function\n",
        "def softmax(vector):\n",
        "  e = exp(vector)\n",
        "  return e / e.sum()\n",
        "\n",
        "print(softmax([1,2,3]))#a test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vector(sentence, important_score=None, model=None):\n",
        "    # Insert your code here\n",
        "\n",
        "    words = sentence.split()#split the sentence\n",
        "\n",
        "    #if important_score is None, initialize it with all 1\n",
        "    if important_score is None:\n",
        "        important_score = [1] * len(words)\n",
        "\n",
        "\n",
        "    if len(words) != len(important_score):#make sure list important_score has same length with sentence\n",
        "        raise ValueError(\"Length of sentence and important_score do not match.\")\n",
        "\n",
        "\n",
        "    important_weight = softmax(important_score)\n",
        "\n",
        "\n",
        "    sentence_vector = np.zeros(100)#initialize sentence_vector\n",
        "    for i, word in enumerate(words):\n",
        "        word_vector = get_word_vector(word, model)#get the word_vector for each word in sentence\n",
        "        sentence_vector += important_weight[i] * word_vector\n",
        "\n",
        "    return sentence_vector"
      ],
      "metadata": {
        "id": "4SqgsziDqsqH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##test\n",
        "#sentence1 = 'manner how did serfdom develop in and then leave russia ?'\n",
        "#vector1 = get_sentence_vector(sentence1,model=word2vect)"
      ],
      "metadata": {
        "id": "rUCB77k-sQof"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(vector1)"
      ],
      "metadata": {
        "id": "Ogobw-i_sv-5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJndZrXYhRH0"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 1.3**</span>\n",
        "\n",
        "**Write code to transform questions in *dm.train_str_questions* and *dm.valid_str_questions* to feature vectors. Note that after running the following cells, you must have $X\\_train$ and $X\\_valid$ which are two numpy arrays of the feature vectors and $y\\_train$ and $y\\_valid$ which are two arrays of numeric labels (Hint: *dm.train_numeral_labels* and *dm.valid_numeral_labels*). You can add more lines to the following cells if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[4 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_importance_score(sentence, factor=0.9):#generate important_score list in this situation 1, 0.9,0.81 ...\n",
        "  num_words = len(sentence.split())\n",
        "  importance_scores = [factor**i for i in range(num_words)]\n",
        "  return importance_scores"
      ],
      "metadata": {
        "id": "CgXQ04dCtT83"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###test\n",
        "#print(len(dm.train_str_questions[1]))\n",
        "#score = get_importance_score(dm.train_str_questions[1])\n",
        "#print(score)"
      ],
      "metadata": {
        "id": "nLb__-Lsvwl3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYjjHh7ahRH0",
        "outputId": "018d1522-f440-41d4-e62f-8e73a3b5afdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform training set to feature vectors...\n"
          ]
        }
      ],
      "source": [
        "print(\"Transform training set to feature vectors...\")\n",
        "dm.train_str_questions\n",
        "X_train = np.array([get_sentence_vector(question, get_importance_score(question), word2vect)\n",
        "                    for question in dm.train_str_questions])#transform train str to feature vector\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(len(X_train))"
      ],
      "metadata": {
        "id": "jYareQBTvAld"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(dm.train_numeral_labels)"
      ],
      "metadata": {
        "id": "gS-uGp1ou-Eu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(y_train)"
      ],
      "metadata": {
        "id": "gxbXFjocvXvF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTUa_hhnhRH0",
        "outputId": "3c25da05-c9d4-4bd5-90f6-419d06555b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform validation set to feature vectors...\n"
          ]
        }
      ],
      "source": [
        "print(\"Transform validation set to feature vectors...\")\n",
        "X_valid = np.array([get_sentence_vector(question, get_importance_score(question), word2vect)\n",
        "                    for question in dm.valid_str_questions])#transform valid str to feature vector\n",
        "\n",
        "y_valid = np.array(dm.valid_numeral_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train)\n",
        "#print(X_valid)"
      ],
      "metadata": {
        "id": "Ej1kDqXWxYAo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpSgf_14hRH0"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 1.4**</span>\n",
        "\n",
        "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in scikit-learn to scale both training and validation sets to the range $(-1,1)$.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "MhAieULShRH0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)#transform to (-1,1) range\n",
        "scaler.fit(X_valid)\n",
        "X_valid_scaled = scaler.transform(X_valid)#transform to (-1,1) range"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train_scaled)\n",
        "#print(X_valid_scaled)"
      ],
      "metadata": {
        "id": "eIQ-P4x2xOTR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvlmVw9khRH0"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 1.5**</span>\n",
        "**Train a Logistic Regression model on the training set and then evaluate on the validation set.**\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "yi0XwM_nhRH0",
        "outputId": "b4b235bc-10de-444d-d974-5943c677d7d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Insert your code for training here\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model_1_5 = LogisticRegression(max_iter=1000)\n",
        "\n",
        "model_1_5.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Je-rIjhRH1",
        "outputId": "f4cf94f5-84eb-4af4-8d1e-a761e9d40d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on validation set: 0.8650\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.67      0.73         6\n",
            "           1       0.95      0.92      0.94        87\n",
            "           2       0.77      0.83      0.80        99\n",
            "           3       1.00      0.87      0.93        85\n",
            "           4       0.75      0.92      0.83        60\n",
            "           5       0.89      0.81      0.85        63\n",
            "\n",
            "    accuracy                           0.86       400\n",
            "   macro avg       0.86      0.84      0.84       400\n",
            "weighted avg       0.88      0.86      0.87       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Insert your code for validation here\n",
        "y_pred = model_1_5.predict(X_valid_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_valid, y_pred)#calculate accuracy\n",
        "print(f\"Accuracy on validation set: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "report = classification_report(y_valid, y_pred)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rInjuBADhRH1"
      },
      "source": [
        "### **Embedding visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M88P4VAHhRH1"
      },
      "source": [
        "**As you know, the embedding matrix is a collection of embedding vectors, each is for one word. In this part, you will base on the cosine similarity of the embedding vectors for the words to find the top-k most relevant words for a given word.**\n",
        "\n",
        "**Good embeddings should have words close in meaning near each other by some similarity metrics. The similarity metric we'll use is the `consine` similarity, which is defined for two vector $\\mathbf{u}$ and $\\mathbf{v}$ as $\\cos(\\mathbf{u}, \\mathbf{v})=\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\left\\Vert{\\mathbf{u}}\\right\\Vert\\left\\Vert{\\mathbf{v}}\\right\\Vert}$ where $\\cdot$ means dot product and $\\left\\Vert\\cdot\\right\\Vert$ means the $L^2$ norm.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "yQDHYzhAhRH1"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u,v):\n",
        "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSlLZb-QhRH1"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 1.6** </span>\n",
        "\n",
        "**Write code for the `function find_most_similar(word=None, k=5, model=None)` which returns a list of the top-`k` most similar words (in descending order) for a given word. This similarity is based on the cosine similarity of embedding vectors obtained from the pretrained model `glove-wiki-gigaword-100` (see Question 1.1). The parameter `include_cur_word` specifies whether the given word should be included in the returned output or not. This means that if `include_cur_word=True`, the function will return `k+1` words.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[3 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EIxHV1ufhRH1"
      },
      "outputs": [],
      "source": [
        "def find_most_similar(word=None, k=5, model=None, include_cur_word=False):\n",
        "    try:\n",
        "        word_vector = model[word]\n",
        "\n",
        "\n",
        "        similarities = {}#initialize a similarities list\n",
        "        for vocab_word in model.key_to_index.keys():\n",
        "            if vocab_word == word and not include_cur_word:\n",
        "                continue\n",
        "            vocab_word_vector = model[vocab_word]\n",
        "            cosine_sim = cosine_similarity(word_vector, vocab_word_vector)#find cos similarity\n",
        "            similarities[vocab_word] = cosine_sim\n",
        "\n",
        "        #the top k words\n",
        "        sorted_words = sorted(similarities, key=similarities.get, reverse=True)\n",
        "\n",
        "\n",
        "        return sorted_words[:k + (1 if include_cur_word else 0)]#return the top k, k+1 if include_cur_word is true\n",
        "    except: # Word not in the vocabulary\n",
        "        print(\"Word is not in the dictionary!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ro0UhTZhRH2"
      },
      "source": [
        "Here is the example of the above function. You can check your implementation using the provided `result`. As you can observe, the `result` makes sense which demonstrates that the embedding matrix is meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69aqiubthRH2",
        "outputId": "b018dad7-8120-442f-f6d9-f85fe7b6364e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation is correct.\n"
          ]
        }
      ],
      "source": [
        "result = ['computer','computers','software','technology','pc','hardware','internet','desktop','electronic','systems','computing']\n",
        "output = find_most_similar(word='computer', k=10, model=word2vect, include_cur_word=True)\n",
        "if output == result:\n",
        "    print(\"Your implementation is correct.\")\n",
        "else:\n",
        "    print(\"Your implementation is not correct.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###test\n",
        "#if 'computer' in word2vect.key_to_index.keys():\n",
        "#    print(\"'computer' is in the vocabulary.\")\n",
        "#else:\n",
        "#   print(\"'computer' is NOT in the vocabulary.\")\n",
        "\n",
        "\n",
        "#print(word2vect.key_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZMf5Ms13MBc",
        "outputId": "d0b8c590-28e7-4657-e1b6-8682d45ba5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'computer' is in the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEUFHgpbhRH2"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 1.7** </span>\n",
        "**Implement the `plot2D_with_groups(word_list, model, k=10)` function to visualize groups of similar words in 2D space. The `word_list` parameter is a list of words, and for each word in the `word_list`, find its top-`k` most similar words (which forms a group) using the `find_most_similar` function. Use tSNE to project embedding vectors into 2D space and plot groups with different colors. You can use the colormaps from `matplotlib`, i.e., `cmap = plt.get_cmap('brg')`.**\n",
        "\n",
        "**The figure bellow is the output obtained by running `plot2D_with_groups` using the input `word_list=['an', 'introduction', 'to', 'deep', 'learning']`. Note that the words within the `word_list` are also visualized (in black) as shown in the figure.**\n",
        "\n",
        "<img src=\"./images/2Dtsne.png\" align=\"center\" width=600/>\n",
        "\n",
        "**As you can observe, words within each group tend to be closer to each other, while words from different groups are more distant.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[5 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "t3J2B0_YhRH2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(perplexity=5, n_components=2, init='pca', n_iter=5000)\n",
        "def plot2D_with_groups(word_list=None, k=10, model=None):\n",
        "  neighbour_words = []#contains similar words for each word in word_list\n",
        "  neighbour_vectors = []#contains vector form of similar words for each word in word_list\n",
        "\n",
        "\n",
        "  for word in word_list:\n",
        "    similar_words = find_most_similar(word, k, model,include_cur_word=True)#find top k similar words\n",
        "    neighbour_words.extend(similar_words)\n",
        "    neighbour_vectors.extend([model[w] for w in similar_words])#convert to vector form and save\n",
        "\n",
        "  #convert to 2d dimension\n",
        "  d2_embedding=tsne.fit_transform(np.array(neighbour_vectors))\n",
        "  #print(d2_embedding)\n",
        "  #print(len(d2_embedding))\n",
        "\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  cmap = plt.get_cmap('tab10')\n",
        "  num_groups = len(word_list)\n",
        "\n",
        "  for i,word in enumerate(word_list):\n",
        "    start = i*(k+1) #k neighbour + word itself\n",
        "    end = start +k +1\n",
        "    #print(start,end)\n",
        "\n",
        "    #plot word scatters for each group\n",
        "    plt.scatter(d2_embedding[start:end, 0], d2_embedding[start:end, 1],\n",
        "                    c=[cmap(i) for _ in range(k + 1)], label=word, alpha=0.6)\n",
        "\n",
        "    #plot annotation\n",
        "    plt.annotate(word, (d2_embedding[start, 0], d2_embedding[start, 1]),\n",
        "                     color='black', weight='bold')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.title(\"2D visualization of word groups\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "ub25OGpchRH2",
        "outputId": "07be1a9a-cb64-4b84-d432-015738a11d26"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "word_list = ['an', 'introduction', 'to', 'deep', 'learning']\n",
        "plot2D_with_groups(word_list=word_list, k=10, model=word2vect)\n",
        "#print(a,b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRyvwdflhRH2"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 2: Text CNN for sequence modeling and neural embedding </span>\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgMxgYY3hRH4"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 2.1**</span>\n",
        "\n",
        "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN that you need to construct.**\n",
        "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
        "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
        "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)).\n",
        "  - `data_manager`: the data manager to store information of the dataset.\n",
        "- The detail of the computational process is as follows:\n",
        "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size \\times maxlen \\times embed\\_size]$ as $h$.\n",
        "  - We feed $h$ to three *Conv1D* layers, each of which has $state\\_size$ filters, padding=same, activation= relu, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size \\times output\\_size \\times state\\_size]$.\n",
        "  - We then apply *GlobalMaxPool1D()* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
        "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$. Note that you need to specify the axis to concatenate.\n",
        "  - We finally build up one dense layer on the top of $h$ for classification.\n",
        "  \n",
        "  <div style=\"text-align: right\"><span style=\"color:red\">[8 marks]</span></div>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylzdm3BIhRH5"
      },
      "outputs": [],
      "source": [
        "class TextCNN:\n",
        "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
        "        self.data_manager = data_manager\n",
        "        self.embed_size = embed_size\n",
        "        self.state_size = state_size\n",
        "\n",
        "    def build(self):\n",
        "        x = tf.keras.layers.Input(shape=[None])\n",
        "        h = tf.keras.layers.Embedding(self.data_manager.vocab_size +1, self.embed_size, mask_zero=True)(x)\n",
        "        h1 = tf.keras.layers.Conv1D(self.state_size, kernel_size=3, padding='same', activation='relu')(h)#three conv1d layer with kernel_size = 3\n",
        "        h2 = tf.keras.layers.Conv1D(self.state_size, kernel_size=5, padding='same', activation='relu')(h)#three conv1d layer with kernel_size = 5\n",
        "        h3 = tf.keras.layers.Conv1D(self.state_size, kernel_size=7, padding='same', activation='relu')(h)#three conv1d layer with kernel_size = 7\n",
        "        h1 = tf.keras.layers.GlobalMaxPooling1D()(h1)#global pooling\n",
        "        h2 = tf.keras.layers.GlobalMaxPooling1D()(h2)#global pooling\n",
        "        h3 = tf.keras.layers.GlobalMaxPooling1D()(h3)#global pooling\n",
        "        h = h = tf.keras.layers.Concatenate(axis=1)([h1, h2, h3])\n",
        "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(h)\n",
        "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
        "\n",
        "    def compile_model(self, *args, **kwargs):\n",
        "        self.model.compile(*args, **kwargs)\n",
        "\n",
        "    def fit(self, *args, **kwargs):\n",
        "        return self.model.fit(*args, **kwargs)\n",
        "\n",
        "    def evaluate(self, *args, **kwargs):\n",
        "        self.model.evaluate(*args, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoq4hbvbhRH5"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 2.2**</span>\n",
        "**Here is the code to test TextCNN above. You can observe that TextCNN outperforms the traditional approach Word2Vect with Logistic Regression for this task. The reason is that TextCNN enables us to automatically learn the feature that fits to the task. This makes deep learning different from hand-crafted feature approaches. Complete the code to test the model. Note that when compiling the model, you can use the Adam optimizer.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WJ2dMtShRH5",
        "outputId": "be474479-e7a2-4b0a-b83d-2bbc4bd1138d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "50/50 [==============================] - 3s 42ms/step - loss: 1.5683 - accuracy: 0.4656 - val_loss: 1.2738 - val_accuracy: 0.7350\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.8029 - accuracy: 0.8725 - val_loss: 0.4197 - val_accuracy: 0.9200\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.2232 - accuracy: 0.9594 - val_loss: 0.1785 - val_accuracy: 0.9575\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 2s 37ms/step - loss: 0.0939 - accuracy: 0.9844 - val_loss: 0.1252 - val_accuracy: 0.9600\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 0.0503 - accuracy: 0.9944 - val_loss: 0.1048 - val_accuracy: 0.9675\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0293 - accuracy: 0.9987 - val_loss: 0.0920 - val_accuracy: 0.9625\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0177 - accuracy: 0.9994 - val_loss: 0.0867 - val_accuracy: 0.9650\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 2s 38ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9650\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9650\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 0.9650\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0798 - val_accuracy: 0.9650\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0794 - val_accuracy: 0.9650\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9675\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0817 - val_accuracy: 0.9650\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 0.9675\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9675\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0829 - val_accuracy: 0.9675\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0845 - val_accuracy: 0.9675\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 0.9675\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 9.3144e-04 - accuracy: 1.0000 - val_loss: 0.0853 - val_accuracy: 0.9675\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d21354cd6c0>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "text_cnn = TextCNN(data_manager=dm)\n",
        "text_cnn.build()\n",
        "# Insert your code here\n",
        "# You are required to compile the model and train the model on 20 epochs\n",
        "\n",
        "text_cnn.compile_model(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "\n",
        "text_cnn.fit(dm.train_numeral_data,dm.train_numeral_labels, epochs=20,validation_data=(dm.valid_numeral_data, dm.valid_numeral_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is observed that the validation accuracy is 0.9675, which is much higher than the validation accuracy in question 1,which is 0.8650"
      ],
      "metadata": {
        "id": "t2wLpLMOfM11"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOOg9ieWhRH5"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 3: RNN-based models for sequence modeling and neural embedding</span>\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 32 marks]<span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzc-Im55hRH5"
      },
      "source": [
        "### <span style=\"color:#0b486b\">3.1. RNNs with different cell types</span> ###\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 12 marks]<span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOc-abZQhRH6"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.1.1**</span>\n",
        "**In this part, you need to construct a vanilla RNN to learn from the dataset of interest. Basically, you are required to construct the class RNN with the following requirements:**\n",
        "- Attribute `data_manager (self.data_manager)`: specifies the data manager used to store data for the model.\n",
        "- Attribute `cell_type (self.cell_type)`: can take one of the three values, i.e., `simple_rnn`, `gru`, or `lstm` which specifies the memory cells formed a hidden layer.\n",
        "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes  of memory cells. For example, $state\\_sizes = [64, 128]$ means that you have two hidden layers in your network with hidden sizes of $64$ and $128$ respectively.\n",
        "\n",
        "**Note that when declaring an embedding layer for the network, you need to set *mask_zero=True* so that the padding zeros in the sentences will be masked and ignored. This helps to have variable length RNNs. For more detail, you can refer to this [link](https://www.tensorflow.org/guide/keras/masking_and_padding).**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[7 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WhdhamKMhRH6"
      },
      "outputs": [],
      "source": [
        "class BaseRNN:\n",
        "    def __init__(self, cell_type='gru', embed_size=128, state_sizes=[128, 64], data_manager=None):\n",
        "        self.cell_type = cell_type\n",
        "        self.state_sizes = state_sizes\n",
        "        self.embed_size = embed_size\n",
        "        self.data_manager = data_manager\n",
        "        self.vocab_size = self.data_manager.vocab_size + 1\n",
        "\n",
        "    # return the corresponding memory cell\n",
        "    @staticmethod\n",
        "    def get_layer(cell_type='gru', state_size=128, return_sequences=False, activation='tanh'):\n",
        "        if cell_type == 'gru':\n",
        "            return tf.keras.layers.GRU(state_size,return_sequences=return_sequences,activation=activation)#use gru\n",
        "        elif cell_type == 'lstm':\n",
        "            return tf.keras.layers.LSTM(state_size,return_sequences=return_sequences,activation=activation)#use lstm\n",
        "        else:\n",
        "            return tf.keras.layers.SimpleRNN(state_size,return_sequences=return_sequences,activation=activation)#use simple rnn\n",
        "\n",
        "    def build(self):\n",
        "        x = tf.keras.layers.Input(shape=[None])\n",
        "        h = tf.keras.layers.Embedding(self.vocab_size,self.embed_size,mask_zero=True)(x)#embedding layer\n",
        "        num_layers = len(self.state_sizes)#number of layers\n",
        "        for i in range(num_layers):\n",
        "            if i==len(self.state_sizes)-1:\n",
        "              return_sequences=False#if it is the last layer,set return_sequence to be False\n",
        "            else:\n",
        "              return_sequences=True#if it is not the last layer,set return_sequence to be True\n",
        "            h = BaseRNN.get_layer(cell_type=self.cell_type,state_size=self.state_sizes[i],return_sequences=return_sequences)(h)\n",
        "\n",
        "        if return_sequences:#add one global pooling for the last layer\n",
        "            h = tf.keras.layers.GlobalMaxPooling1D()(h)\n",
        "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
        "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
        "\n",
        "    def compile_model(self, *args, **kwargs):\n",
        "        self.model.compile(*args, **kwargs)\n",
        "\n",
        "    def fit(self, *args, **kwargs):\n",
        "        return self.model.fit(*args, **kwargs)\n",
        "\n",
        "    def evaluate(self, *args, **kwargs):\n",
        "        self.model.evaluate(*args, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmkv1EKUhRH6"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.1.2**</span>\n",
        "**Run with simple RNN ('simple_rnn') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[1 mark]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyGv54rAhRH6",
        "outputId": "485f4323-6145-44ef-a6a5-796d40fa6fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 7s 116ms/step - loss: 0.8925 - accuracy: 0.6950 - val_loss: 0.3078 - val_accuracy: 0.9125\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.1990 - accuracy: 0.9450 - val_loss: 0.2060 - val_accuracy: 0.9375\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 0.0874 - accuracy: 0.9744 - val_loss: 0.1827 - val_accuracy: 0.9425\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.0862 - accuracy: 0.9725 - val_loss: 0.1111 - val_accuracy: 0.9625\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.1099 - val_accuracy: 0.9650\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.0338 - accuracy: 0.9900 - val_loss: 0.0897 - val_accuracy: 0.9650\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 2s 80ms/step - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.0993 - val_accuracy: 0.9650\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 0.9675\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 9.7477e-04 - accuracy: 1.0000 - val_loss: 0.1776 - val_accuracy: 0.9700\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 2s 81ms/step - loss: 5.5493e-04 - accuracy: 1.0000 - val_loss: 0.1918 - val_accuracy: 0.9650\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.0760 - accuracy: 0.9837 - val_loss: 0.1014 - val_accuracy: 0.9725\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 2s 81ms/step - loss: 0.0156 - accuracy: 0.9944 - val_loss: 0.1180 - val_accuracy: 0.9575\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 0.0021 - accuracy: 0.9987 - val_loss: 0.1166 - val_accuracy: 0.9625\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 2s 79ms/step - loss: 5.1801e-04 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9700\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 2s 81ms/step - loss: 2.9398e-04 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 0.9725\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 2.0503e-04 - accuracy: 1.0000 - val_loss: 0.1320 - val_accuracy: 0.9675\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 1.5392e-04 - accuracy: 1.0000 - val_loss: 0.1535 - val_accuracy: 0.9650\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 2s 78ms/step - loss: 1.2698e-04 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9650\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 1.0914e-04 - accuracy: 1.0000 - val_loss: 0.1561 - val_accuracy: 0.9625\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 9.1994e-05 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9700\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d214a836f80>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "base_rnn = BaseRNN(cell_type='simple_rnn',embed_size=128, state_sizes =[64,128], data_manager=dm)#run with simple rnn and other reequired parameters\n",
        "base_rnn.build()\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "base_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "base_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observed validation accuracy is 0.97 which is higher than the CNN approach accuaracy"
      ],
      "metadata": {
        "id": "1D1FsmY7qjZk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Eb_QRxhRH6"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.1.3**</span>\n",
        "**Run with GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[1 mark]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aezLzO7ihRH6",
        "outputId": "4936e056-612a-4dd7-a710-ace56d6b89aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 15s 311ms/step - loss: 1.6827 - accuracy: 0.2394 - val_loss: 1.6046 - val_accuracy: 0.2825\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 6s 257ms/step - loss: 1.2741 - accuracy: 0.5056 - val_loss: 0.8565 - val_accuracy: 0.6725\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 6s 234ms/step - loss: 0.5285 - accuracy: 0.8313 - val_loss: 0.3076 - val_accuracy: 0.9250\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 6s 233ms/step - loss: 0.1702 - accuracy: 0.9500 - val_loss: 0.2026 - val_accuracy: 0.9300\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 6s 228ms/step - loss: 0.0888 - accuracy: 0.9744 - val_loss: 0.2001 - val_accuracy: 0.9325\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 7s 269ms/step - loss: 0.0433 - accuracy: 0.9887 - val_loss: 0.1660 - val_accuracy: 0.9425\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 6s 225ms/step - loss: 0.0253 - accuracy: 0.9937 - val_loss: 0.1616 - val_accuracy: 0.9500\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 6s 224ms/step - loss: 0.0104 - accuracy: 0.9987 - val_loss: 0.2066 - val_accuracy: 0.9450\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 6s 237ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.1757 - val_accuracy: 0.9550\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 6s 236ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.2294 - val_accuracy: 0.9450\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 7s 272ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2100 - val_accuracy: 0.9500\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 6s 223ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9525\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 7s 263ms/step - loss: 9.6760e-04 - accuracy: 1.0000 - val_loss: 0.1915 - val_accuracy: 0.9600\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 8s 314ms/step - loss: 5.9656e-04 - accuracy: 1.0000 - val_loss: 0.2002 - val_accuracy: 0.9575\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 6s 228ms/step - loss: 4.4347e-04 - accuracy: 1.0000 - val_loss: 0.2080 - val_accuracy: 0.9550\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 6s 231ms/step - loss: 3.3588e-04 - accuracy: 1.0000 - val_loss: 0.2121 - val_accuracy: 0.9550\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 6s 230ms/step - loss: 2.8696e-04 - accuracy: 1.0000 - val_loss: 0.2087 - val_accuracy: 0.9575\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 7s 265ms/step - loss: 2.3819e-04 - accuracy: 1.0000 - val_loss: 0.2092 - val_accuracy: 0.9600\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 6s 251ms/step - loss: 2.0890e-04 - accuracy: 1.0000 - val_loss: 0.2099 - val_accuracy: 0.9600\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 6s 244ms/step - loss: 1.8556e-04 - accuracy: 1.0000 - val_loss: 0.2115 - val_accuracy: 0.9600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d214df87c70>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "base_rnn = BaseRNN(cell_type='gru',embed_size=128, state_sizes =[64,128], data_manager=dm)#run with gru,and other required parameters\n",
        "base_rnn.build()\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "base_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "base_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observed validation accuracy is 0.96"
      ],
      "metadata": {
        "id": "7zF6sqpss6Pq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2lMGbX7hRH7"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.1.4**</span>\n",
        "**Run with LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[1 mark]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBok8x0ZhRH7",
        "outputId": "2b085765-8ecd-4425-d9af-94c20db9ab15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 17s 387ms/step - loss: 1.6766 - accuracy: 0.2750 - val_loss: 1.5666 - val_accuracy: 0.2575\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 7s 280ms/step - loss: 1.0719 - accuracy: 0.6556 - val_loss: 0.6007 - val_accuracy: 0.8875\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 8s 341ms/step - loss: 0.3798 - accuracy: 0.9087 - val_loss: 0.2573 - val_accuracy: 0.9275\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 0.1473 - accuracy: 0.9631 - val_loss: 0.2120 - val_accuracy: 0.9325\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 7s 277ms/step - loss: 0.0893 - accuracy: 0.9750 - val_loss: 0.1286 - val_accuracy: 0.9475\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 7s 262ms/step - loss: 0.0522 - accuracy: 0.9862 - val_loss: 0.1021 - val_accuracy: 0.9600\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 8s 303ms/step - loss: 0.0385 - accuracy: 0.9912 - val_loss: 0.0963 - val_accuracy: 0.9625\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 7s 282ms/step - loss: 0.0224 - accuracy: 0.9937 - val_loss: 0.1168 - val_accuracy: 0.9650\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 7s 275ms/step - loss: 0.0396 - accuracy: 0.9919 - val_loss: 0.1240 - val_accuracy: 0.9650\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 8s 304ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.1337 - val_accuracy: 0.9600\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 8s 310ms/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.1497 - val_accuracy: 0.9600\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 7s 274ms/step - loss: 0.0188 - accuracy: 0.9950 - val_loss: 0.1330 - val_accuracy: 0.9650\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 7s 277ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.1506 - val_accuracy: 0.9675\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 7s 277ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.1671 - val_accuracy: 0.9600\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 7s 265ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.2027 - val_accuracy: 0.9675\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 7s 287ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1926 - val_accuracy: 0.9625\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 7s 302ms/step - loss: 9.9761e-04 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9625\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 8s 320ms/step - loss: 7.6380e-04 - accuracy: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9625\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 7s 277ms/step - loss: 5.6397e-04 - accuracy: 1.0000 - val_loss: 0.2037 - val_accuracy: 0.9625\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 8s 310ms/step - loss: 4.5014e-04 - accuracy: 1.0000 - val_loss: 0.2047 - val_accuracy: 0.9625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d2149f61ea0>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "base_rnn = BaseRNN(cell_type='lstm',embed_size=128, state_sizes =[64,128], data_manager=dm)#run with lstm and other required parameters\n",
        "base_rnn.build()\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "base_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "base_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observed validation accuracy is 0.9625"
      ],
      "metadata": {
        "id": "R-4hDr3Ftzyr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2JoM4cPhRH7"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.1.5**</span>\n",
        "**Write code to conduct experiments to compare the accuracies of RNNs with the three different cell types using 2 different lists of `state_sizes` (while the other hyperparameters are fixed). Specifically, for each `state_sizes = [...]`, you should report the accuracies of RNNs with simple RNN ('simple_rnn') cell, GRU ('gru') cell, and LSTM ('lstm') cell. Give your comments on the results.**\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYMS4RkWhRH7"
      },
      "outputs": [],
      "source": [
        "# Insert your code here\n",
        "def experiment(cell_type, state_sizes, epochs=20):#self defined function. input cell_type, state_sizes, return final validation accuracy\n",
        "\n",
        "    rnn_model = BaseRNN(cell_type=cell_type, embed_size=128, state_sizes=state_sizes, data_manager=dm)\n",
        "    rnn_model.build()\n",
        "\n",
        "\n",
        "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "    rnn_model.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    history = rnn_model.fit(dm.tf_train_set.batch(64), epochs=epochs, validation_data=dm.tf_valid_set.batch(64))\n",
        "\n",
        "\n",
        "    final_val_accuracy = history.history['val_accuracy'][-1]\n",
        "    return final_val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cell_types = ['simple_rnn', 'gru', 'lstm']#set up parameters for cell_types\n",
        "state_sizes_list = [[64, 128], [128, 256]]#set up parameters for state_sizes\n",
        "\n",
        "\n",
        "results = {}\n",
        "for state_sizes in state_sizes_list:\n",
        "    results[str(state_sizes)] = {}\n",
        "    for cell_type in cell_types:\n",
        "        val_accuracy = experiment(cell_type, state_sizes)#get accuracy for each parameter combination\n",
        "        results[str(state_sizes)][cell_type] = val_accuracy\n",
        "        print(f\"Validation accuracy for {cell_type} with state sizes {state_sizes}: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1uqhERAu09t",
        "outputId": "05bb17d3-7fa3-42f2-8c9e-4c505986764b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 4s 100ms/step - loss: 0.8660 - accuracy: 0.7050 - val_loss: 0.2734 - val_accuracy: 0.9275\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 2s 79ms/step - loss: 0.1987 - accuracy: 0.9469 - val_loss: 0.1721 - val_accuracy: 0.9450\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 2s 82ms/step - loss: 0.1024 - accuracy: 0.9694 - val_loss: 0.3464 - val_accuracy: 0.8700\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 0.0608 - accuracy: 0.9794 - val_loss: 0.1347 - val_accuracy: 0.9550\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 0.1369 - val_accuracy: 0.9575\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.0462 - accuracy: 0.9875 - val_loss: 0.1095 - val_accuracy: 0.9575\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.1152 - val_accuracy: 0.9650\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 2s 81ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.1828 - val_accuracy: 0.9425\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0170 - accuracy: 0.9956 - val_loss: 0.1193 - val_accuracy: 0.9575\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1376 - val_accuracy: 0.9550\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 8.4223e-04 - accuracy: 1.0000 - val_loss: 0.1410 - val_accuracy: 0.9600\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 3.6628e-04 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9575\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 2.2755e-04 - accuracy: 1.0000 - val_loss: 0.1614 - val_accuracy: 0.9575\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 1.6561e-04 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9600\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 2s 82ms/step - loss: 1.4865e-04 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9625\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 2s 83ms/step - loss: 1.6806e-04 - accuracy: 1.0000 - val_loss: 0.1736 - val_accuracy: 0.9625\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0096 - accuracy: 0.9950 - val_loss: 0.1992 - val_accuracy: 0.9650\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 2s 92ms/step - loss: 9.8766e-04 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9650\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 1.1230e-04 - accuracy: 1.0000 - val_loss: 0.1691 - val_accuracy: 0.9625\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 8.6487e-05 - accuracy: 1.0000 - val_loss: 0.1757 - val_accuracy: 0.9625\n",
            "Validation accuracy for simple_rnn with state sizes [64, 128]: 0.9625\n",
            "Epoch 1/20\n",
            "25/25 [==============================] - 16s 352ms/step - loss: 1.6778 - accuracy: 0.2637 - val_loss: 1.6124 - val_accuracy: 0.2375\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 6s 226ms/step - loss: 1.2955 - accuracy: 0.4762 - val_loss: 0.8224 - val_accuracy: 0.7300\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 6s 227ms/step - loss: 0.5211 - accuracy: 0.8369 - val_loss: 0.3456 - val_accuracy: 0.8875\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 6s 236ms/step - loss: 0.1667 - accuracy: 0.9531 - val_loss: 0.2803 - val_accuracy: 0.8850\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 7s 262ms/step - loss: 0.0771 - accuracy: 0.9787 - val_loss: 0.2865 - val_accuracy: 0.9175\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 6s 257ms/step - loss: 0.0538 - accuracy: 0.9869 - val_loss: 0.2066 - val_accuracy: 0.9425\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 6s 229ms/step - loss: 0.0423 - accuracy: 0.9875 - val_loss: 0.2388 - val_accuracy: 0.9325\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 7s 271ms/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 0.2459 - val_accuracy: 0.9325\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 6s 239ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.1979 - val_accuracy: 0.9425\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 6s 252ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.2472 - val_accuracy: 0.9425\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 8s 304ms/step - loss: 0.0140 - accuracy: 0.9956 - val_loss: 0.2015 - val_accuracy: 0.9475\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 6s 229ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2041 - val_accuracy: 0.9500\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 6s 232ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9500\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 7s 267ms/step - loss: 9.4732e-04 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9500\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 6s 240ms/step - loss: 6.3283e-04 - accuracy: 1.0000 - val_loss: 0.2584 - val_accuracy: 0.9525\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 6s 237ms/step - loss: 4.6236e-04 - accuracy: 1.0000 - val_loss: 0.2585 - val_accuracy: 0.9500\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 7s 271ms/step - loss: 3.5787e-04 - accuracy: 1.0000 - val_loss: 0.2590 - val_accuracy: 0.9500\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 6s 232ms/step - loss: 3.0362e-04 - accuracy: 1.0000 - val_loss: 0.2565 - val_accuracy: 0.9525\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 6s 226ms/step - loss: 2.5999e-04 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9550\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 7s 282ms/step - loss: 2.2173e-04 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.9525\n",
            "Validation accuracy for gru with state sizes [64, 128]: 0.9525\n",
            "Epoch 1/20\n",
            "25/25 [==============================] - 16s 354ms/step - loss: 1.6800 - accuracy: 0.2537 - val_loss: 1.5245 - val_accuracy: 0.2925\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 7s 262ms/step - loss: 1.0214 - accuracy: 0.6850 - val_loss: 0.6303 - val_accuracy: 0.8600\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 8s 305ms/step - loss: 0.3783 - accuracy: 0.9206 - val_loss: 0.3003 - val_accuracy: 0.9175\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 8s 307ms/step - loss: 0.1803 - accuracy: 0.9519 - val_loss: 0.2087 - val_accuracy: 0.9225\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 8s 313ms/step - loss: 0.1098 - accuracy: 0.9700 - val_loss: 0.1947 - val_accuracy: 0.9300\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 7s 278ms/step - loss: 0.0576 - accuracy: 0.9850 - val_loss: 0.1388 - val_accuracy: 0.9525\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 8s 309ms/step - loss: 0.0368 - accuracy: 0.9900 - val_loss: 0.1371 - val_accuracy: 0.9525\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 8s 304ms/step - loss: 0.0203 - accuracy: 0.9956 - val_loss: 0.1461 - val_accuracy: 0.9500\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 7s 275ms/step - loss: 0.0281 - accuracy: 0.9931 - val_loss: 0.1510 - val_accuracy: 0.9525\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 7s 279ms/step - loss: 0.0099 - accuracy: 0.9975 - val_loss: 0.1514 - val_accuracy: 0.9475\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 8s 302ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.1618 - val_accuracy: 0.9525\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 7s 276ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.1691 - val_accuracy: 0.9525\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 7s 275ms/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 7s 281ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.2381 - val_accuracy: 0.9450\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 7s 268ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.2422 - val_accuracy: 0.9450\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 8s 323ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9475\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 8s 313ms/step - loss: 0.0082 - accuracy: 0.9987 - val_loss: 0.2088 - val_accuracy: 0.9525\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 8s 308ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2182 - val_accuracy: 0.9550\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 7s 279ms/step - loss: 7.8275e-04 - accuracy: 1.0000 - val_loss: 0.2323 - val_accuracy: 0.9525\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 6.2388e-04 - accuracy: 1.0000 - val_loss: 0.2654 - val_accuracy: 0.9500\n",
            "Validation accuracy for lstm with state sizes [64, 128]: 0.9500\n",
            "Epoch 1/20\n",
            "25/25 [==============================] - 8s 237ms/step - loss: 1.0844 - accuracy: 0.6062 - val_loss: 0.4530 - val_accuracy: 0.8525\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 5s 192ms/step - loss: 0.2407 - accuracy: 0.9381 - val_loss: 0.2050 - val_accuracy: 0.9250\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 6s 227ms/step - loss: 0.1739 - accuracy: 0.9519 - val_loss: 0.1564 - val_accuracy: 0.9475\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 5s 218ms/step - loss: 0.0803 - accuracy: 0.9750 - val_loss: 0.2894 - val_accuracy: 0.9325\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 5s 195ms/step - loss: 0.0812 - accuracy: 0.9756 - val_loss: 0.2075 - val_accuracy: 0.9525\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 4s 180ms/step - loss: 0.1184 - accuracy: 0.9725 - val_loss: 0.0982 - val_accuracy: 0.9700\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 6s 238ms/step - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.0899 - val_accuracy: 0.9725\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 5s 202ms/step - loss: 0.1293 - accuracy: 0.9644 - val_loss: 0.0896 - val_accuracy: 0.9725\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 6s 241ms/step - loss: 0.0232 - accuracy: 0.9937 - val_loss: 0.1016 - val_accuracy: 0.9700\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 6s 230ms/step - loss: 0.0736 - accuracy: 0.9812 - val_loss: 0.0856 - val_accuracy: 0.9625\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 5s 206ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0983 - val_accuracy: 0.9625\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 6s 229ms/step - loss: 0.0569 - accuracy: 0.9781 - val_loss: 0.0930 - val_accuracy: 0.9700\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 5s 192ms/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.0732 - val_accuracy: 0.9775\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 5s 217ms/step - loss: 6.2631e-04 - accuracy: 1.0000 - val_loss: 0.0772 - val_accuracy: 0.9800\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 5s 204ms/step - loss: 3.3375e-04 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9775\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 6s 249ms/step - loss: 1.9568e-04 - accuracy: 1.0000 - val_loss: 0.0943 - val_accuracy: 0.9750\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 5s 196ms/step - loss: 1.3168e-04 - accuracy: 1.0000 - val_loss: 0.1052 - val_accuracy: 0.9725\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 6s 236ms/step - loss: 9.6754e-05 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9725\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 5s 208ms/step - loss: 7.2330e-05 - accuracy: 1.0000 - val_loss: 0.1131 - val_accuracy: 0.9725\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 5s 207ms/step - loss: 5.8802e-05 - accuracy: 1.0000 - val_loss: 0.1129 - val_accuracy: 0.9725\n",
            "Validation accuracy for simple_rnn with state sizes [128, 256]: 0.9725\n",
            "Epoch 1/20\n",
            "25/25 [==============================] - 22s 565ms/step - loss: 1.6700 - accuracy: 0.2581 - val_loss: 1.6021 - val_accuracy: 0.2850\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 13s 504ms/step - loss: 1.2641 - accuracy: 0.4638 - val_loss: 0.8122 - val_accuracy: 0.6625\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 12s 499ms/step - loss: 0.5515 - accuracy: 0.8012 - val_loss: 0.2683 - val_accuracy: 0.9125\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 12s 498ms/step - loss: 0.1251 - accuracy: 0.9638 - val_loss: 0.1570 - val_accuracy: 0.9475\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 13s 509ms/step - loss: 0.1008 - accuracy: 0.9681 - val_loss: 0.1297 - val_accuracy: 0.9550\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0235 - accuracy: 0.9925 - val_loss: 0.1257 - val_accuracy: 0.9575\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 12s 500ms/step - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.1372 - val_accuracy: 0.9600\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0514 - accuracy: 0.9900 - val_loss: 0.1317 - val_accuracy: 0.9575\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.9575\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 14s 545ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1664 - val_accuracy: 0.9525\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 12s 486ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.9600\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 13s 505ms/step - loss: 6.1578e-04 - accuracy: 1.0000 - val_loss: 0.2001 - val_accuracy: 0.9600\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 13s 509ms/step - loss: 4.0714e-04 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9600\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 12s 502ms/step - loss: 3.0968e-04 - accuracy: 1.0000 - val_loss: 0.2097 - val_accuracy: 0.9625\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 12s 500ms/step - loss: 2.4189e-04 - accuracy: 1.0000 - val_loss: 0.2115 - val_accuracy: 0.9625\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 13s 532ms/step - loss: 1.9246e-04 - accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9600\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 1.6060e-04 - accuracy: 1.0000 - val_loss: 0.2157 - val_accuracy: 0.9600\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 13s 511ms/step - loss: 1.3818e-04 - accuracy: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.9575\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 13s 508ms/step - loss: 1.2079e-04 - accuracy: 1.0000 - val_loss: 0.2209 - val_accuracy: 0.9550\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 13s 517ms/step - loss: 1.0724e-04 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9575\n",
            "Validation accuracy for gru with state sizes [128, 256]: 0.9575\n",
            "Epoch 1/20\n",
            "25/25 [==============================] - 30s 963ms/step - loss: 1.6469 - accuracy: 0.3019 - val_loss: 1.4698 - val_accuracy: 0.3375\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 21s 827ms/step - loss: 0.9838 - accuracy: 0.6338 - val_loss: 0.6302 - val_accuracy: 0.8500\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 21s 836ms/step - loss: 0.3846 - accuracy: 0.9056 - val_loss: 0.2440 - val_accuracy: 0.9300\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 22s 872ms/step - loss: 0.1499 - accuracy: 0.9600 - val_loss: 0.1735 - val_accuracy: 0.9375\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 21s 841ms/step - loss: 0.0775 - accuracy: 0.9787 - val_loss: 0.1714 - val_accuracy: 0.9425\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 21s 849ms/step - loss: 0.1204 - accuracy: 0.9769 - val_loss: 0.1166 - val_accuracy: 0.9575\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 21s 855ms/step - loss: 0.0242 - accuracy: 0.9944 - val_loss: 0.1122 - val_accuracy: 0.9625\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 22s 871ms/step - loss: 0.0139 - accuracy: 0.9975 - val_loss: 0.1158 - val_accuracy: 0.9625\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 20s 817ms/step - loss: 0.0291 - accuracy: 0.9931 - val_loss: 0.0934 - val_accuracy: 0.9700\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 21s 844ms/step - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.0986 - val_accuracy: 0.9700\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 21s 854ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.1083 - val_accuracy: 0.9750\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 20s 822ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.1030 - val_accuracy: 0.9775\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 21s 838ms/step - loss: 0.0048 - accuracy: 0.9969 - val_loss: 0.1073 - val_accuracy: 0.9725\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 22s 874ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1139 - val_accuracy: 0.9750\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 21s 823ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9775\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 21s 836ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1259 - val_accuracy: 0.9725\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 21s 859ms/step - loss: 8.6796e-04 - accuracy: 1.0000 - val_loss: 0.1321 - val_accuracy: 0.9700\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 22s 864ms/step - loss: 6.0226e-04 - accuracy: 1.0000 - val_loss: 0.1437 - val_accuracy: 0.9700\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 21s 848ms/step - loss: 5.3865e-04 - accuracy: 1.0000 - val_loss: 0.1474 - val_accuracy: 0.9725\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 21s 849ms/step - loss: 4.6921e-04 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9725\n",
            "Validation accuracy for lstm with state sizes [128, 256]: 0.9725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XayKToAqhRH8"
      },
      "source": [
        "\\# Give your comments on the results here (maximum 150 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "results:\n",
        "Validation accuracy for simple_rnn with state sizes [64, 128]: 0.9625\n",
        "Validation accuracy for simple_rnn with state sizes [128, 256]: 0.9725\n",
        "\n",
        "Validation accuracy for gru with state sizes [64, 128]: 0.9525\n",
        "Validation accuracy for gru with state sizes [128, 256]: 0.9575\n",
        "\n",
        "Validation accuracy for lstm with state sizes [64, 128]: 0.9500\n",
        "Validation accuracy for lstm with state sizes [128, 256]: 0.9725\n",
        "\n",
        "It is observed that increasing the state size can increase the validation accuracy.\n",
        "And among the three cell_type, the simple_rnn works the best\n"
      ],
      "metadata": {
        "id": "QFho5__f1DMC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VitO6ABhRH8"
      },
      "source": [
        "### <span style=\"color:#0b486b\">3.2. RNNs with fine-tuning embedding matrix</span> ###\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 8 marks]<span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0co0Lz6hRH8"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.2.1**</span>\n",
        "\n",
        "**In what follows, you are required to extend the class BaseRNN in Part 3.1 to achieve a class RNN in which the embedding matrix can be initialized using a pretrained Word2Vect.**\n",
        "\n",
        "**Below are the descriptions of the attributes of the class *RNN*:**\n",
        "- `run_mode (self.run_mode)` has three values (scratch, init-only, and init-fine-tune).\n",
        "  - `scratch` means training the embedding matrix from scratch.\n",
        "  - `init-only` means only initializing the embedding matrix with a pretrained Word2Vect but **not further doing** fine-tuning that matrix.\n",
        "  - `init-fine-tune` means both initializing the embedding matrix with a pretrained Word2Vect and **further doing** fine-tuning that matrix.\n",
        "- `cell_type (self.cell_type)` has three values (simple-rnn, gru, and lstm) which specify the memory cell used in the network.\n",
        "- `embed_model (self.embed_model)` specifes the pretrained Word2Vect model used.\n",
        "-  `embed_size (self.embed_size)` specifes the embedding size. Note that when run_mode is either 'init-only' or 'init-fine-tune', this embedding size is extracted from embed_model for dimension compatability.\n",
        "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes  of memory cells. For example, $state\\_sizes = [64, 128]$ means that you have two hidden layers in your network with hidden sizes of $64$ and $128$ respectively.\n",
        "\n",
        "**Complete the code of the class *RNN*.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[6 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wv72n819hRH8"
      },
      "outputs": [],
      "source": [
        "class RNN(BaseRNN):\n",
        "    def __init__(self, run_mode='scratch', embed_model='glove-wiki-gigaword-100', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.run_mode = run_mode\n",
        "        self.embed_model = embed_model\n",
        "        if self.run_mode != 'scratch':\n",
        "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
        "        self.word2idx = dm.word2idx\n",
        "        self.word2vect = None\n",
        "        self.embed_matrix = np.zeros(shape=[self.vocab_size, self.embed_size])\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        if self.run_mode != 'scratch':#if run mode is not scratch\n",
        "          self.word2vect = api.load(self.embed_model)#initialize pretrained model\n",
        "          for word, idx in self.word2idx.items():\n",
        "            try:\n",
        "              self.embed_matrix[idx]=self.word2vect[word]#assign value for the embeding matrix\n",
        "            except KeyError:\n",
        "              pass\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        x = tf.keras.layers.Input(shape=[None])\n",
        "\n",
        "        em_init = 'uniform'# default for embeding initializer\n",
        "        em_trainable = True# if embeding is trainable\n",
        "\n",
        "        if self.run_mode != 'scratch':\n",
        "          self.build_embedding_matrix()\n",
        "          em_init = tf.keras.initializers.Constant(self.embed_matrix)\n",
        "          if self.run_mode == 'init-fine-tune':#if init-fine-tune, then embeding is trainable\n",
        "            em_trainable=True\n",
        "          else:\n",
        "            em_trainable=False#for init-only\n",
        "\n",
        "        h = tf.keras.layers.Embedding(self.vocab_size,self.embed_size,embeddings_initializer = em_init,trainable=em_trainable,mask_zero=True)(x)#embeding layer\n",
        "\n",
        "        num_layers = len(self.state_sizes)#number of layers\n",
        "        for i in range(num_layers):\n",
        "          if i< len(self.state_sizes) -1:\n",
        "            return_sequences = True#if not the last layer, return sequence is true\n",
        "          else:\n",
        "            return_sequences=False#if the last layer, return sequence is false\n",
        "          h = self.get_layer(cell_type=self.cell_type, state_size=self.state_sizes[i],return_sequences=return_sequences)(h)\n",
        "\n",
        "        if return_sequences:\n",
        "          h = tf.keras.layers.GlobalMaxPooling1D()(h)#global pooling\n",
        "\n",
        "        h=tf.keras.layers.Dense(self.data_manager.num_classes,activation='softmax')(h)#feed to the dense layer\n",
        "        self.model = tf.keras.Model(inputs=x,outputs=h)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compile_model(self, *args, **kwargs):\n",
        "        self.model.compile(*args, **kwargs)\n",
        "\n",
        "    def fit(self, *args, **kwargs):\n",
        "        return self.model.fit(*args, **kwargs)\n",
        "\n",
        "    def evaluate(self, *args, **kwargs):\n",
        "        self.model.evaluate(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKkChOOlhRH8"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.2.2**</span>\n",
        "\n",
        "**Write code to conduct experiments to compare three running modes for the embedding matrix. Note that you should stick with fixed values for other attributes and only vary *run_mode*. Give your comments on the results.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "cell_type = 'simple_rnn'#consist with the best parameters found previously\n",
        "embed_size = 128\n",
        "state_sizes = [128, 256]\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "modes = ['scratch', 'init-only', 'init-fine-tune']\n",
        "results = {}\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    return tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "num_classes = 6\n",
        "train_labels_hot = one_hot_encode(dm.train_numeral_labels, num_classes)\n",
        "valid_labels_hot = one_hot_encode(dm.valid_numeral_labels, num_classes)\n",
        "\n",
        "for mode in modes:\n",
        "    print(f\"Training RNN with run_mode: {mode}\")\n",
        "\n",
        "\n",
        "    rnn = RNN(run_mode=mode, cell_type=cell_type, embed_size=embed_size, state_sizes=state_sizes, data_manager=dm)\n",
        "    rnn.build()\n",
        "    rnn.compile_model(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = rnn.fit(dm.train_numeral_data, train_labels_hot, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "    # somehow, save_model methods have some error so rebuild the model and load the weight. The benefit is\n",
        "    # that if the code corupts, I still have the model weight and do not need to rerun the whole code.\n",
        "    rnn.model.save_weights(f\"rnn_model_weights_{mode}.h5\")\n",
        "    rnn2 = RNN(run_mode=mode, cell_type=cell_type, embed_size=embed_size, state_sizes=state_sizes, data_manager=dm)\n",
        "    rnn2.build()\n",
        "    rnn2.compile_model(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    rnn2.model.load_weights(f\"rnn_model_weights_{mode}.h5\")\n",
        "\n",
        "    # for evaluation\n",
        "    evaluation = rnn2.model.evaluate(dm.valid_numeral_data, valid_labels_hot)\n",
        "    results[mode] = {\n",
        "        'history': history.history,\n",
        "        'evaluation': evaluation\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNPajhpfkmPa",
        "outputId": "667fb23d-a0ba-4123-99bc-9d9bcc71167f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN with run_mode: scratch\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 9s 298ms/step - loss: 1.0148 - accuracy: 0.6148 - val_loss: 0.2599 - val_accuracy: 0.9156\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.1870 - accuracy: 0.9430 - val_loss: 0.2862 - val_accuracy: 0.9062\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 6s 316ms/step - loss: 0.1119 - accuracy: 0.9695 - val_loss: 0.1208 - val_accuracy: 0.9594\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 5s 254ms/step - loss: 0.0585 - accuracy: 0.9820 - val_loss: 0.1121 - val_accuracy: 0.9594\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 7s 349ms/step - loss: 0.0481 - accuracy: 0.9875 - val_loss: 0.1032 - val_accuracy: 0.9656\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 5s 265ms/step - loss: 0.0565 - accuracy: 0.9883 - val_loss: 0.1232 - val_accuracy: 0.9594\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 7s 341ms/step - loss: 0.0983 - accuracy: 0.9727 - val_loss: 0.2458 - val_accuracy: 0.9438\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 5s 265ms/step - loss: 0.0651 - accuracy: 0.9820 - val_loss: 0.1471 - val_accuracy: 0.9625\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 7s 340ms/step - loss: 0.0795 - accuracy: 0.9883 - val_loss: 0.1297 - val_accuracy: 0.9656\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.0490 - accuracy: 0.9914 - val_loss: 0.1836 - val_accuracy: 0.9594\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 6s 292ms/step - loss: 0.1514 - accuracy: 0.9641 - val_loss: 0.5892 - val_accuracy: 0.8625\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 6s 271ms/step - loss: 0.2221 - accuracy: 0.9344 - val_loss: 0.1408 - val_accuracy: 0.9563\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 5s 251ms/step - loss: 0.0355 - accuracy: 0.9914 - val_loss: 0.1426 - val_accuracy: 0.9531\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 7s 337ms/step - loss: 0.0209 - accuracy: 0.9945 - val_loss: 0.1694 - val_accuracy: 0.9563\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 5s 250ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.1181 - val_accuracy: 0.9625\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 7s 340ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.1200 - val_accuracy: 0.9688\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0914 - val_accuracy: 0.9719\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 6s 329ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0879 - val_accuracy: 0.9781\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 5s 262ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0909 - val_accuracy: 0.9750\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 6s 304ms/step - loss: 9.9206e-04 - accuracy: 1.0000 - val_loss: 0.0916 - val_accuracy: 0.9750\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.1454 - accuracy: 0.9625\n",
            "Training RNN with run_mode: init-only\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 9s 307ms/step - loss: 1.3780 - accuracy: 0.4187 - val_loss: 0.9110 - val_accuracy: 0.6531\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 5s 257ms/step - loss: 0.6005 - accuracy: 0.7648 - val_loss: 0.4754 - val_accuracy: 0.8344\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 6s 316ms/step - loss: 0.3958 - accuracy: 0.8453 - val_loss: 0.3969 - val_accuracy: 0.8656\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 5s 258ms/step - loss: 0.2649 - accuracy: 0.9031 - val_loss: 0.2198 - val_accuracy: 0.9406\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 6s 316ms/step - loss: 0.1568 - accuracy: 0.9539 - val_loss: 0.1849 - val_accuracy: 0.9375\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 5s 239ms/step - loss: 0.1121 - accuracy: 0.9641 - val_loss: 0.2050 - val_accuracy: 0.9281\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 6s 313ms/step - loss: 0.0511 - accuracy: 0.9906 - val_loss: 0.1499 - val_accuracy: 0.9563\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 5s 254ms/step - loss: 0.0352 - accuracy: 0.9906 - val_loss: 0.1588 - val_accuracy: 0.9500\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 5s 253ms/step - loss: 0.0258 - accuracy: 0.9953 - val_loss: 0.1270 - val_accuracy: 0.9594\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 6s 325ms/step - loss: 0.0174 - accuracy: 0.9969 - val_loss: 0.1255 - val_accuracy: 0.9594\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 5s 253ms/step - loss: 0.0231 - accuracy: 0.9922 - val_loss: 0.1586 - val_accuracy: 0.9563\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 6s 328ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.1397 - val_accuracy: 0.9469\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 5s 254ms/step - loss: 0.0252 - accuracy: 0.9945 - val_loss: 0.1705 - val_accuracy: 0.9375\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 6s 312ms/step - loss: 0.0583 - accuracy: 0.9828 - val_loss: 0.1827 - val_accuracy: 0.9563\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 5s 253ms/step - loss: 0.0422 - accuracy: 0.9859 - val_loss: 0.1917 - val_accuracy: 0.9406\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 5s 252ms/step - loss: 0.0181 - accuracy: 0.9984 - val_loss: 0.1653 - val_accuracy: 0.9563\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 6s 310ms/step - loss: 0.0105 - accuracy: 0.9977 - val_loss: 0.1682 - val_accuracy: 0.9531\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 5s 254ms/step - loss: 0.0483 - accuracy: 0.9883 - val_loss: 0.2354 - val_accuracy: 0.9344\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 6s 330ms/step - loss: 0.0375 - accuracy: 0.9883 - val_loss: 0.1951 - val_accuracy: 0.9531\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 5s 253ms/step - loss: 0.0286 - accuracy: 0.9930 - val_loss: 0.2050 - val_accuracy: 0.9375\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.2234 - accuracy: 0.9400\n",
            "Training RNN with run_mode: init-fine-tune\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 8s 281ms/step - loss: 1.3880 - accuracy: 0.4305 - val_loss: 0.8932 - val_accuracy: 0.6438\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 7s 336ms/step - loss: 0.6291 - accuracy: 0.7875 - val_loss: 0.4918 - val_accuracy: 0.8062\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 5s 242ms/step - loss: 0.2759 - accuracy: 0.9164 - val_loss: 0.3206 - val_accuracy: 0.9000\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 6s 287ms/step - loss: 0.1397 - accuracy: 0.9617 - val_loss: 0.1967 - val_accuracy: 0.9344\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.0875 - accuracy: 0.9781 - val_loss: 0.1631 - val_accuracy: 0.9594\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 5s 256ms/step - loss: 0.1100 - accuracy: 0.9641 - val_loss: 0.1674 - val_accuracy: 0.9500\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 6s 318ms/step - loss: 0.0554 - accuracy: 0.9844 - val_loss: 0.2046 - val_accuracy: 0.9406\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 5s 259ms/step - loss: 0.0340 - accuracy: 0.9922 - val_loss: 0.2147 - val_accuracy: 0.9438\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 6s 326ms/step - loss: 0.0798 - accuracy: 0.9781 - val_loss: 0.2058 - val_accuracy: 0.9281\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 5s 259ms/step - loss: 0.0517 - accuracy: 0.9844 - val_loss: 0.1840 - val_accuracy: 0.9500\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 6s 323ms/step - loss: 0.0276 - accuracy: 0.9930 - val_loss: 0.1932 - val_accuracy: 0.9531\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 5s 262ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.1247 - val_accuracy: 0.9719\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 5s 260ms/step - loss: 0.0108 - accuracy: 0.9984 - val_loss: 0.1151 - val_accuracy: 0.9781\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 6s 314ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.1309 - val_accuracy: 0.9688\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 5s 249ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 0.9750\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 6s 321ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1223 - val_accuracy: 0.9719\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 5s 260ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9750\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 7s 337ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1231 - val_accuracy: 0.9750\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 5s 260ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1254 - val_accuracy: 0.9750\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 6s 328ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1255 - val_accuracy: 0.9750\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.1345 - accuracy: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ln_jO46unWI",
        "outputId": "b2aad3e2-0ee2-481d-9c08-61e956adf719"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'scratch': {'history': {'loss': [1.0147818326950073, 0.18696120381355286, 0.1119476705789566, 0.058542393147945404, 0.04806692153215408, 0.05648744851350784, 0.0983356311917305, 0.06510436534881592, 0.07952382415533066, 0.04898275062441826, 0.15142610669136047, 0.22210855782032013, 0.035458680242300034, 0.02086576260626316, 0.011455336585640907, 0.008313080295920372, 0.0019136365735903382, 0.0014107184251770377, 0.0011570334900170565, 0.000992060056887567], 'accuracy': [0.6148437261581421, 0.9429687261581421, 0.969531238079071, 0.9820312261581421, 0.987500011920929, 0.98828125, 0.97265625, 0.9820312261581421, 0.98828125, 0.991406261920929, 0.964062511920929, 0.934374988079071, 0.991406261920929, 0.9945312738418579, 0.996874988079071, 0.999218761920929, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.25987666845321655, 0.28616827726364136, 0.1208275705575943, 0.1121082678437233, 0.10322685539722443, 0.12324869632720947, 0.2458362579345703, 0.1471322476863861, 0.12969358265399933, 0.18360015749931335, 0.5891588926315308, 0.14083069562911987, 0.14262782037258148, 0.16936542093753815, 0.11805291473865509, 0.11998377740383148, 0.09137481451034546, 0.08785807341337204, 0.09090776741504669, 0.0915885865688324], 'val_accuracy': [0.9156249761581421, 0.90625, 0.9593750238418579, 0.9593750238418579, 0.965624988079071, 0.9593750238418579, 0.9437500238418579, 0.9624999761581421, 0.965624988079071, 0.9593750238418579, 0.862500011920929, 0.956250011920929, 0.953125, 0.956250011920929, 0.9624999761581421, 0.96875, 0.971875011920929, 0.9781249761581421, 0.9750000238418579, 0.9750000238418579]}, 'evaluation': [0.14541679620742798, 0.9624999761581421]}, 'init-only': {'history': {'loss': [1.378031611442566, 0.6005252003669739, 0.3957828879356384, 0.2649334669113159, 0.15680375695228577, 0.11205576360225677, 0.05113847181200981, 0.03523546829819679, 0.025796297937631607, 0.017364639788866043, 0.023085875436663628, 0.011585275642573833, 0.02519567869603634, 0.05833910033106804, 0.04221635311841965, 0.018092984333634377, 0.010476822964847088, 0.04828808084130287, 0.03754611313343048, 0.028581922873854637], 'accuracy': [0.41874998807907104, 0.764843761920929, 0.8453124761581421, 0.903124988079071, 0.953906238079071, 0.964062511920929, 0.9906250238418579, 0.9906250238418579, 0.995312511920929, 0.996874988079071, 0.9921875, 0.9984375238418579, 0.9945312738418579, 0.9828125238418579, 0.9859374761581421, 0.9984375238418579, 0.9976562261581421, 0.98828125, 0.98828125, 0.992968738079071], 'val_loss': [0.9109800457954407, 0.47542649507522583, 0.3969259262084961, 0.21978063881397247, 0.18486659228801727, 0.20500412583351135, 0.14991287887096405, 0.15880735218524933, 0.12699055671691895, 0.12550443410873413, 0.15858951210975647, 0.13966801762580872, 0.17050790786743164, 0.18269982933998108, 0.19168075919151306, 0.16530415415763855, 0.16820670664310455, 0.23544320464134216, 0.1951121687889099, 0.20501351356506348], 'val_accuracy': [0.653124988079071, 0.8343750238418579, 0.8656250238418579, 0.940625011920929, 0.9375, 0.9281250238418579, 0.956250011920929, 0.949999988079071, 0.9593750238418579, 0.9593750238418579, 0.956250011920929, 0.9468749761581421, 0.9375, 0.956250011920929, 0.940625011920929, 0.956250011920929, 0.953125, 0.934374988079071, 0.953125, 0.9375]}, 'evaluation': [0.22343288362026215, 0.9399999976158142]}, 'init-fine-tune': {'history': {'loss': [1.3880066871643066, 0.6290796995162964, 0.2758703827857971, 0.13971421122550964, 0.08754970878362656, 0.10995575040578842, 0.05536366254091263, 0.034006521105766296, 0.07980380952358246, 0.05170164629817009, 0.027636587619781494, 0.009691347368061543, 0.010839168913662434, 0.003931525629013777, 0.0022255158983170986, 0.0017001578817144036, 0.0014721158659085631, 0.0012980846222490072, 0.0011575674870982766, 0.0010432227281853557], 'accuracy': [0.43046873807907104, 0.7875000238418579, 0.9164062738418579, 0.961718738079071, 0.9781249761581421, 0.964062511920929, 0.984375, 0.9921875, 0.9781249761581421, 0.984375, 0.992968738079071, 1.0, 0.9984375238418579, 0.999218761920929, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.893156886100769, 0.491759717464447, 0.3205575942993164, 0.19665764272212982, 0.16309663653373718, 0.16739115118980408, 0.20455272495746613, 0.21465890109539032, 0.20580795407295227, 0.18404357135295868, 0.1931629478931427, 0.1247473955154419, 0.11509434133768082, 0.13090606033802032, 0.12080289423465729, 0.12227122485637665, 0.12241928279399872, 0.12308317422866821, 0.1254146844148636, 0.1254989206790924], 'val_accuracy': [0.643750011920929, 0.8062499761581421, 0.8999999761581421, 0.934374988079071, 0.9593750238418579, 0.949999988079071, 0.940625011920929, 0.9437500238418579, 0.9281250238418579, 0.949999988079071, 0.953125, 0.971875011920929, 0.9781249761581421, 0.96875, 0.9750000238418579, 0.971875011920929, 0.9750000238418579, 0.9750000238418579, 0.9750000238418579, 0.9750000238418579]}, 'evaluation': [0.1344713568687439, 0.9624999761581421]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "302rNJUXhRH9"
      },
      "source": [
        "\\# Give your comments on the results here (maximum 150 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result:\n",
        "as shown in the above evalution results:\n",
        "for scratch the accuracy is 0.9625\n",
        "for init-only the accuracy is 0.94\n",
        "for init-fine-tune the accuary is 0.9625\n",
        "\n",
        "the other parameter chosen are:\n",
        "cell_type = 'simple_rnn'\n",
        "embed_size = 128  \n",
        "state_sizes = [128, 256]\n",
        "epochs = 20  \n",
        "batch_size = 64  \n",
        "\n",
        "which is found in the q3.1.5\n",
        "\n",
        "I was expecting the accuracy for init-fine-tune to be higher than scratch but it turns out the accuracy for scratch and init-fine-tune are almost the same"
      ],
      "metadata": {
        "id": "3rv7kVQ2z93M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q46NUpFkhRH9"
      },
      "source": [
        "### <span style=\"color:#0b486b\">3.3. RNNs with Attention for Text and Sequence Classification</span> ###\n",
        "\n",
        "**In what follows, you are required to implement a RNN with the attention mechanism for text and sequence classification. This attention mechanism is applied at the last hidden layer of our RNN. Specifically, let $\\textbf{h}_1^L, \\textbf{h}_2^L,...,\\textbf{h}_{T-1}^L, \\textbf{h}_T^L$ be the hidden states at the last hidden layer $L$ where $T$ is the sequence length. We compute the context vector $\\textbf{c}$ as $\\textbf{c}=\\sum_{i=1}^{T}\\textbf{a}_{i}\\textbf{h}_{i}^L$ where $\\textbf{a}_1,...,\\textbf{a}_T$ are the alignment weights (i.e., $\\textbf{a}_i\\geq 0$ and $\\sum_{i=1}^{T}\\textbf{a}_{i}=1$).**\n",
        "\n",
        "**The alignment weights are computed as follows:**\n",
        "- $\\textbf{a}=[\\textbf{a}_{i}]_{i=1}^{T}=\\text{softmax}(\\textbf{s})$ where $\\textbf{s}= [\\textbf{s}_{i}]_{i=1}^{T}$ consists of the alignment scores.\n",
        "- The alignment scores are computed as $\\textbf{s}=\\text{tanh}(\\textbf{h}^LU)V$ where $\\textbf{h}^L=\\left[\\begin{array}{c}\n",
        "\\textbf{h}_{1}^L\\\\\n",
        "\\textbf{h}_{2}^L\\\\\n",
        "...\\\\\n",
        "\\textbf{h}_{T-1}^L\\\\\n",
        "\\textbf{h}_{T}^L\n",
        "\\end{array}\\right]\\in\\mathbb{R}^{T\\times state\\_size_{L}}$, $U\\in\\mathbb{R}^{state\\_size_{L}\\times output\\_length}$, $V\\in\\mathbb{R}^{output\\_length\\times1}$, and $output\\_length$ is a hyperparameter. Note that if we consider a mini-batch, the shape of $\\textbf{h}^L$ is $(batch\\_size, T, state\\_size_L)$ where $state\\_size_L$ is the hidden size of the last hidden layer. The figure on the right below illustrates the process of calculating a score $\\textbf{s}_i$ for an individual hidden state $\\textbf{h}_i^L$. Weight matrices $U$ and $V$ are shared across the hidden states $\\textbf{h}_1^L,\\textbf{h}_2^L,\\dots,\\textbf{h}_T^L$.\n",
        "\n",
        "**After having the context vector $\\textbf{c}$, we concatenate with the last hidden state $\\textbf{h}_T^L$. On top of this concatenation, we conduct the output layer with the softmax activation.**\n",
        "\n",
        "<img src=\"./images/attentionRNN.png\" align=\"center\" width=700/>\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 12 marks]<span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoqDtFa6hRH-"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.3.1**</span>\n",
        "\n",
        "**We declare the  layer `MyAttention` as a class inherited from `tf.keras.layers.Layer` to realize our attention mechanism. You are required to provide the code for this class. Note that in the `def call(self, all_states, last_state)` method, `all_states` is the collection of all hidden states and `last_state` is the last hidden state.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[4 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw0ADDRQhRH-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MyAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_length=50):\n",
        "        super().__init__()\n",
        "        self.output_length = output_length\n",
        "\n",
        "    def build(self, input_shape):#initialize U V as required in the question\n",
        "        self.U = self.add_weight(shape=(input_shape[-1], self.output_length), initializer=\"random_normal\", trainable=True)\n",
        "        self.V = self.add_weight(shape=(self.output_length, 1), initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "    def call(self, all_states, last_state):\n",
        "        s = tf.tanh(tf.matmul(all_states, self.U)) #s = tanh(hL*U)\n",
        "        scores = tf.matmul(s, self.V)# scores which is s, score = s*V as required in the question\n",
        "\n",
        "        alignment_weights = tf.nn.softmax(scores, axis=1)#which is a in the question\n",
        "        context_vector = tf.reduce_sum(alignment_weights * all_states, axis=1)#which is c in the question\n",
        "        return tf.concat([context_vector, last_state], axis=-1)#concanate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0vzBidkhRH-"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.3.2**</span>\n",
        "\n",
        "**You are required to extend the class `RNN` in Question `3.2.1` to achieve the class `AttentionRNN` in which the attention mechanism mentioned above is applied at the last hidden layer.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[6 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionRNN(RNN):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_layer(cell_type='gru', state_size=128, return_sequences=False, activation='tanh'):#the code for chosing gru/lstm/simple-rnn\n",
        "\n",
        "        if cell_type == 'gru':# for 'gru'\n",
        "            return tf.keras.layers.GRU(state_size, return_sequences=return_sequences, activation=activation)\n",
        "        elif cell_type == 'lstm':# for 'lstm'\n",
        "            return tf.keras.layers.LSTM(state_size, return_sequences=return_sequences, activation=activation)\n",
        "        elif cell_type == 'simple_rnn':#for 'simple-rnn'\n",
        "            return tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences, activation=activation)\n",
        "        else:#for others in case\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "    def compile_model(self, *args, **kwargs):\n",
        "        self.model.compile(*args, **kwargs)\n",
        "\n",
        "    def fit(self, *args, **kwargs):\n",
        "        return self.model.fit(*args, **kwargs)\n",
        "\n",
        "    def evaluate(self, *args, **kwargs):\n",
        "        return self.model.evaluate(*args, **kwargs)\n",
        "\n",
        "    def build(self):\n",
        "        x = tf.keras.layers.Input(shape=[None])\n",
        "\n",
        "        em_init = 'uniform' # default for embedding initializer\n",
        "        em_trainable = True# if embeding is trainable\n",
        "\n",
        "        if self.run_mode != 'scratch':\n",
        "            self.build_embedding_matrix()\n",
        "            em_init = tf.keras.initializers.Constant(self.embed_matrix)\n",
        "            if self.run_mode == 'init-fine-tune':\n",
        "                em_trainable = True#for init-fine-tune\n",
        "            else:\n",
        "                em_trainable = False#for init-only\n",
        "\n",
        "        h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, embeddings_initializer=em_init, trainable=em_trainable, mask_zero=True)(x)\n",
        "\n",
        "        num_layers = len(self.state_sizes)\n",
        "        for i in range(num_layers):\n",
        "            if i < len(self.state_sizes) - 1:#if not the last layer\n",
        "                return_sequences = True\n",
        "            else:\n",
        "                return_sequences = True  #change to true because one more attention layer\n",
        "            h = self.get_layer(cell_type=self.cell_type, state_size=self.state_sizes[i], return_sequences=return_sequences)(h)\n",
        "\n",
        "        # attention layer\n",
        "        all_states = h\n",
        "        last_state = tf.keras.layers.Lambda(lambda x: x[:, -1, :])(h)#get the last state\n",
        "        context_vector = MyAttention()(all_states, last_state)\n",
        "\n",
        "        # concatenate context vector with the last state\n",
        "        combined_vector = tf.keras.layers.Concatenate(axis=-1)([context_vector, last_state])\n",
        "\n",
        "        # feed the combined vector to the Dense layer\n",
        "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(combined_vector)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "7ciRr2eADOwW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIX8z7YQhRH_"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 3.3.3**</span>\n",
        "\n",
        "**Choose a common setting for standard RNN and RNN with attention and conduct experiments to compare them. The setting here means `run_mode`, `cell_type` and list of `state_sizes`. Give your comments on the results.**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E2Cu8tvhRIA"
      },
      "source": [
        "\\# Give your comments on the results here (maximum 150 words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#some self-defined valuables, functions i need for the following experiment\n",
        "num_classes = 6\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    return tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "train_labels_hot = one_hot_encode(dm.train_numeral_labels, num_classes)\n",
        "valid_labels_hot = one_hot_encode(dm.valid_numeral_labels, num_classes)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xbXXCzR5YZy7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#experiment to see comparasion\n",
        "modes = ['init-only', 'init-fine-tune']\n",
        "cell_types = ['simple_rnn', 'gru', 'lstm']\n",
        "\n",
        "for mode in modes:\n",
        "  for cell_type in cell_types:\n",
        "\n",
        "\n",
        "    print(f\"rnn attention with {mode} and {cell_type}\")\n",
        "    attention_rnn = AttentionRNN(run_mode=mode, cell_type=cell_type, state_sizes=[128, 256], data_manager=dm)\n",
        "    attention_rnn.build()\n",
        "    attention_rnn.compile_model(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history_attention_rnn = attention_rnn.fit(dm.train_numeral_data, train_labels_hot, epochs=20, batch_size=64, validation_split=0.2)\n",
        "    evaluation_attention_rnn = attention_rnn.evaluate(dm.valid_numeral_data, valid_labels_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtE89jRJkgyY",
        "outputId": "6e72560c-aa0d-4182-c3dc-ddb56d331340"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn attention with init-only and simple_rnn\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 12s 381ms/step - loss: 1.3019 - accuracy: 0.4711 - val_loss: 0.7506 - val_accuracy: 0.7312\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 6s 282ms/step - loss: 0.4267 - accuracy: 0.8500 - val_loss: 0.2475 - val_accuracy: 0.9187\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 7s 368ms/step - loss: 0.1713 - accuracy: 0.9391 - val_loss: 0.1776 - val_accuracy: 0.9406\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 6s 283ms/step - loss: 0.1228 - accuracy: 0.9609 - val_loss: 0.1350 - val_accuracy: 0.9500\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 7s 371ms/step - loss: 0.1107 - accuracy: 0.9664 - val_loss: 0.2754 - val_accuracy: 0.9156\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 6s 295ms/step - loss: 0.1177 - accuracy: 0.9648 - val_loss: 0.1311 - val_accuracy: 0.9563\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 7s 366ms/step - loss: 0.0446 - accuracy: 0.9883 - val_loss: 0.1676 - val_accuracy: 0.9563\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 6s 296ms/step - loss: 0.0290 - accuracy: 0.9922 - val_loss: 0.1240 - val_accuracy: 0.9656\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 8s 420ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.1510 - val_accuracy: 0.9563\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 6s 301ms/step - loss: 0.0092 - accuracy: 0.9992 - val_loss: 0.1493 - val_accuracy: 0.9563\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 7s 335ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.1334 - val_accuracy: 0.9688\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 6s 327ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.1475 - val_accuracy: 0.9625\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 7s 366ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.1583 - val_accuracy: 0.9563\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 7s 355ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.1739 - val_accuracy: 0.9500\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 6s 286ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1480 - val_accuracy: 0.9656\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 7s 377ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.1514 - val_accuracy: 0.9563\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 6s 294ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1631 - val_accuracy: 0.9656\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 7s 369ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1646 - val_accuracy: 0.9656\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 7s 355ms/step - loss: 8.6956e-04 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9656\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 7s 371ms/step - loss: 7.3529e-04 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9688\n",
            "13/13 [==============================] - 1s 72ms/step - loss: 0.1629 - accuracy: 0.9575\n",
            "rnn attention with init-only and gru\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 23s 836ms/step - loss: 1.4775 - accuracy: 0.4000 - val_loss: 1.1628 - val_accuracy: 0.5437\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 14s 718ms/step - loss: 0.7796 - accuracy: 0.7211 - val_loss: 0.4504 - val_accuracy: 0.8813\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 14s 724ms/step - loss: 0.2762 - accuracy: 0.9094 - val_loss: 0.2609 - val_accuracy: 0.9344\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 15s 756ms/step - loss: 0.1339 - accuracy: 0.9578 - val_loss: 0.1520 - val_accuracy: 0.9531\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 16s 784ms/step - loss: 0.0789 - accuracy: 0.9758 - val_loss: 0.1179 - val_accuracy: 0.9719\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 15s 732ms/step - loss: 0.0623 - accuracy: 0.9773 - val_loss: 0.1063 - val_accuracy: 0.9656\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 14s 717ms/step - loss: 0.0468 - accuracy: 0.9820 - val_loss: 0.1159 - val_accuracy: 0.9625\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 15s 737ms/step - loss: 0.0359 - accuracy: 0.9875 - val_loss: 0.1155 - val_accuracy: 0.9719\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 15s 751ms/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 0.1110 - val_accuracy: 0.9719\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 15s 766ms/step - loss: 0.0166 - accuracy: 0.9969 - val_loss: 0.1277 - val_accuracy: 0.9688\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 15s 745ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 0.1385 - val_accuracy: 0.9688\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 15s 750ms/step - loss: 0.0139 - accuracy: 0.9945 - val_loss: 0.1408 - val_accuracy: 0.9781\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 14s 719ms/step - loss: 0.0489 - accuracy: 0.9875 - val_loss: 0.2827 - val_accuracy: 0.9500\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 14s 731ms/step - loss: 0.0559 - accuracy: 0.9820 - val_loss: 0.0880 - val_accuracy: 0.9781\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 14s 731ms/step - loss: 0.0169 - accuracy: 0.9937 - val_loss: 0.1012 - val_accuracy: 0.9719\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 15s 736ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 0.9781\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 14s 731ms/step - loss: 0.0074 - accuracy: 0.9984 - val_loss: 0.1119 - val_accuracy: 0.9750\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 14s 730ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1168 - val_accuracy: 0.9750\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 14s 725ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.1247 - val_accuracy: 0.9750\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 15s 735ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.1267 - val_accuracy: 0.9781\n",
            "13/13 [==============================] - 4s 123ms/step - loss: 0.0986 - accuracy: 0.9700\n",
            "rnn attention with init-only and lstm\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 29s 1s/step - loss: 1.2321 - accuracy: 0.5234 - val_loss: 0.6347 - val_accuracy: 0.8000\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.3833 - accuracy: 0.8719 - val_loss: 0.2622 - val_accuracy: 0.9187\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 20s 986ms/step - loss: 0.1766 - accuracy: 0.9508 - val_loss: 0.1577 - val_accuracy: 0.9531\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 20s 993ms/step - loss: 0.1194 - accuracy: 0.9625 - val_loss: 0.1583 - val_accuracy: 0.9563\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 19s 982ms/step - loss: 0.0871 - accuracy: 0.9734 - val_loss: 0.1209 - val_accuracy: 0.9563\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 20s 994ms/step - loss: 0.0795 - accuracy: 0.9742 - val_loss: 0.2059 - val_accuracy: 0.9281\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 18s 927ms/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 0.1687 - val_accuracy: 0.9469\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 19s 980ms/step - loss: 0.0651 - accuracy: 0.9781 - val_loss: 0.1769 - val_accuracy: 0.9438\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 20s 989ms/step - loss: 0.0515 - accuracy: 0.9859 - val_loss: 0.0985 - val_accuracy: 0.9781\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0618 - accuracy: 0.9797 - val_loss: 0.1849 - val_accuracy: 0.9500\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 18s 915ms/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.1195 - val_accuracy: 0.9656\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0263 - accuracy: 0.9922 - val_loss: 0.0887 - val_accuracy: 0.9750\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 20s 986ms/step - loss: 0.0210 - accuracy: 0.9937 - val_loss: 0.0963 - val_accuracy: 0.9812\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0325 - accuracy: 0.9883 - val_loss: 0.1330 - val_accuracy: 0.9625\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 20s 995ms/step - loss: 0.0294 - accuracy: 0.9914 - val_loss: 0.0927 - val_accuracy: 0.9812\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 19s 930ms/step - loss: 0.0225 - accuracy: 0.9898 - val_loss: 0.1141 - val_accuracy: 0.9656\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 18s 908ms/step - loss: 0.0135 - accuracy: 0.9945 - val_loss: 0.1455 - val_accuracy: 0.9688\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 20s 988ms/step - loss: 0.0130 - accuracy: 0.9937 - val_loss: 0.1075 - val_accuracy: 0.9844\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 19s 981ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.1386 - val_accuracy: 0.9781\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.1462 - val_accuracy: 0.9719\n",
            "13/13 [==============================] - 5s 172ms/step - loss: 0.0925 - accuracy: 0.9725\n",
            "rnn attention with init-fine-tune and simple_rnn\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 12s 415ms/step - loss: 1.4765 - accuracy: 0.3906 - val_loss: 0.9003 - val_accuracy: 0.6844\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 6s 305ms/step - loss: 0.4901 - accuracy: 0.8383 - val_loss: 0.4571 - val_accuracy: 0.8469\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 8s 389ms/step - loss: 0.1862 - accuracy: 0.9438 - val_loss: 0.3375 - val_accuracy: 0.8938\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 6s 305ms/step - loss: 0.1049 - accuracy: 0.9742 - val_loss: 0.2544 - val_accuracy: 0.9156\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 7s 378ms/step - loss: 0.1036 - accuracy: 0.9688 - val_loss: 0.2821 - val_accuracy: 0.9125\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 6s 314ms/step - loss: 0.0677 - accuracy: 0.9828 - val_loss: 0.2026 - val_accuracy: 0.9187\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 8s 387ms/step - loss: 0.0349 - accuracy: 0.9891 - val_loss: 0.1822 - val_accuracy: 0.9250\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 6s 311ms/step - loss: 0.0230 - accuracy: 0.9953 - val_loss: 0.1177 - val_accuracy: 0.9688\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 7s 373ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9688\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 7s 346ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9625\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 7s 334ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9750\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 8s 410ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9688\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 6s 311ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 0.9719\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 8s 391ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9719\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 6s 307ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9719\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 8s 391ms/step - loss: 8.8858e-04 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9688\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 6s 307ms/step - loss: 7.9948e-04 - accuracy: 1.0000 - val_loss: 0.1329 - val_accuracy: 0.9688\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 7s 378ms/step - loss: 7.2051e-04 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9688\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 6s 305ms/step - loss: 6.5666e-04 - accuracy: 1.0000 - val_loss: 0.1342 - val_accuracy: 0.9688\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 8s 381ms/step - loss: 5.9704e-04 - accuracy: 1.0000 - val_loss: 0.1339 - val_accuracy: 0.9688\n",
            "13/13 [==============================] - 1s 59ms/step - loss: 0.1809 - accuracy: 0.9600\n",
            "rnn attention with init-fine-tune and gru\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 26s 1s/step - loss: 1.4482 - accuracy: 0.4141 - val_loss: 1.0428 - val_accuracy: 0.5844\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 15s 770ms/step - loss: 0.7022 - accuracy: 0.7516 - val_loss: 0.3664 - val_accuracy: 0.8906\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 15s 771ms/step - loss: 0.1945 - accuracy: 0.9469 - val_loss: 0.1846 - val_accuracy: 0.9594\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 15s 770ms/step - loss: 0.0949 - accuracy: 0.9680 - val_loss: 0.1660 - val_accuracy: 0.9469\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 16s 833ms/step - loss: 0.0600 - accuracy: 0.9805 - val_loss: 0.1191 - val_accuracy: 0.9781\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 15s 771ms/step - loss: 0.0237 - accuracy: 0.9937 - val_loss: 0.0920 - val_accuracy: 0.9812\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 15s 754ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.1237 - val_accuracy: 0.9719\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 15s 765ms/step - loss: 0.0112 - accuracy: 0.9953 - val_loss: 0.1407 - val_accuracy: 0.9656\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 15s 758ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.1296 - val_accuracy: 0.9719\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 17s 843ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1266 - val_accuracy: 0.9812\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 15s 768ms/step - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.2225 - val_accuracy: 0.9750\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 15s 771ms/step - loss: 0.0054 - accuracy: 0.9992 - val_loss: 0.1389 - val_accuracy: 0.9719\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 15s 770ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.1431 - val_accuracy: 0.9812\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 15s 767ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 0.9844\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 17s 838ms/step - loss: 7.9674e-04 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 0.9875\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 15s 755ms/step - loss: 3.9582e-04 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9844\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 15s 766ms/step - loss: 3.0032e-04 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9875\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 15s 766ms/step - loss: 2.4936e-04 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9875\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 15s 757ms/step - loss: 2.1936e-04 - accuracy: 1.0000 - val_loss: 0.1211 - val_accuracy: 0.9875\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 16s 827ms/step - loss: 1.9365e-04 - accuracy: 1.0000 - val_loss: 0.1223 - val_accuracy: 0.9875\n",
            "13/13 [==============================] - 6s 161ms/step - loss: 0.1629 - accuracy: 0.9675\n",
            "rnn attention with init-fine-tune and lstm\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 32s 1s/step - loss: 1.2568 - accuracy: 0.5336 - val_loss: 0.6462 - val_accuracy: 0.7812\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 19s 949ms/step - loss: 0.3957 - accuracy: 0.8695 - val_loss: 0.2427 - val_accuracy: 0.9219\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 20s 995ms/step - loss: 0.1779 - accuracy: 0.9445 - val_loss: 0.1837 - val_accuracy: 0.9438\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.1154 - accuracy: 0.9664 - val_loss: 0.1562 - val_accuracy: 0.9281\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 20s 952ms/step - loss: 0.0944 - accuracy: 0.9734 - val_loss: 0.1614 - val_accuracy: 0.9563\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.1267 - val_accuracy: 0.9594\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0514 - accuracy: 0.9836 - val_loss: 0.1630 - val_accuracy: 0.9438\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0709 - accuracy: 0.9805 - val_loss: 0.1134 - val_accuracy: 0.9625\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0439 - accuracy: 0.9867 - val_loss: 0.1216 - val_accuracy: 0.9563\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0412 - accuracy: 0.9859 - val_loss: 0.0922 - val_accuracy: 0.9781\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 19s 955ms/step - loss: 0.0180 - accuracy: 0.9953 - val_loss: 0.0892 - val_accuracy: 0.9781\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0941 - val_accuracy: 0.9781\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.1092 - val_accuracy: 0.9781\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 19s 949ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.1091 - val_accuracy: 0.9750\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0043 - accuracy: 0.9984 - val_loss: 0.1168 - val_accuracy: 0.9781\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.1165 - val_accuracy: 0.9750\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.1267 - val_accuracy: 0.9781\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.1308 - val_accuracy: 0.9781\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1373 - val_accuracy: 0.9781\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 19s 940ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.9781\n",
            "13/13 [==============================] - 6s 172ms/step - loss: 0.0997 - accuracy: 0.9725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is observed that the validation accuracy for attention with init-only and simple-rnn is 0.9575\n",
        "with init-only and gru is 0.97\n",
        "with init-only and lstm is 0.9725\n",
        "with fine-tune and simple-rnn is 0.96\n",
        "with fine-tune and gru is 0.9675\n",
        "with fine-tune and lstm is 0.9725\n",
        "\n",
        "\n",
        "compared with standard rnn:\n",
        "for scratch the accuracy is 0.9625 for init-only the accuracy is 0.94 for init-fine-tune the accuary is 0.9625 when choosing cell_type = 'simple_rnn'\n",
        "\n",
        "the accuracy improves noticably.\n",
        "\n",
        "Interestingly, for standard rnn, simple-rnn works the best. in the attention version, lstm works the best"
      ],
      "metadata": {
        "id": "6SyzAx04s6IX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPPWvBmFhRIA"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 4: Transformer-based models for sequence modeling and neural embedding and the overall ranking</span>\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 23 marks]<span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOxAx5BUhRIB"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 4.1**</span>\n",
        "\n",
        "**Implement the multi-head attention module of the Transformer for the text classification problem. The provided code is from [this source](https://keras.io/examples/nlp/text_classification_with_transformer/). In this part, we only use the output of the Transformer encoder for the classification task. For further information on the Transformer model, refer to [this paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).**\n",
        "\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[11 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPtFuREYhRIB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c2xU7efhRIB"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    # Insert your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-mKM6KuhRIB"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = # Insert your code here to call 'MultiHeadAttention' class\n",
        "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ByLVNxUhRIB"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y0E-JPwhRIC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "vocab_size = dm.vocab_size + 1\n",
        "maxlen = dm.maxlen\n",
        "\n",
        "def create_tfm_model(embed_dim=64, num_heads=8, ff_dim=32, dropout_rate=0.1):\n",
        "    \"\"\"\n",
        "    Creates a Transformer model using the given hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "        embed_dim (int): The embedding dimension for each token.\n",
        "        num_heads (int): The number of attention heads in the multi-head attention layer.\n",
        "        ff_dim (int): The hidden layer size in the feed forward network inside the transformer block.\n",
        "        dropout_rate (float): The dropout rate for regularization.\n",
        "    Returns:\n",
        "        keras.models.Sequential: A Transformer model.\n",
        "    \"\"\"\n",
        "    tfm = Sequential()\n",
        "    tfm.add(Input(shape=(maxlen,)))\n",
        "    tfm.add(TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim))\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
        "    tfm.add(transformer_block)\n",
        "    tfm.add(GlobalAveragePooling1D())\n",
        "    tfm.add(Dropout(dropout_rate))\n",
        "    tfm.add(Dense(units=dm.num_classes, activation='softmax'))\n",
        "    tfm.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return tfm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkxuFYFrhRIC"
      },
      "outputs": [],
      "source": [
        "model = create_tfm_model()\n",
        "model.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmmOBpqthRIC"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 4.2**</span>\n",
        "**Write code to conduct experiments to evaluate the impact of hyperparameters `embed_dim`, `num_heads`, `ff_dim`, and `dropout_rate` of the Transformer model on the accuracy. Report (i) your findings from the experiments, (ii) the accuracy of your best Transformer model (i.e., the one with the highest accuracy on the validation set), and (iii) the values of the mentioned hyperparameters of that best model.  \n",
        "Note that the necessary condition to get the full mark for this question is that the accuracy of your best Transformer model should be at least 90%.**\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[3 marks]</span></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFkuoi4phRIC"
      },
      "outputs": [],
      "source": [
        "# Insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shtWkwW0hRIC"
      },
      "source": [
        "\\# Give your answer here.\n",
        "\n",
        "(i) Your findings from the experiments (maximum 200 words)\n",
        "\n",
        "(ii) The accuracy of your best Transformer model on the validation set\n",
        "\n",
        "(iii) The values of the mentioned hyperparameters of your best Transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6neqi8IChRIC"
      },
      "source": [
        "#### <span style=\"color:red\">**Question 4.3**</span>\n",
        "**For any models defined in the previous questions (of all parts), you are free to fine-tune hyperparameters, e.g., `optimizer`, `learning_rate`, `state_sizes`, such that you get a best model, i.e., the one with the highest accuracy on the validation set. You will need to report (i) what is your best model,  (ii) its accuracy on the validation set, and (iii) the values of its hyperparameters. Note that you must report your best model's accuracy with rounding to 4 decimal places, i.e., 0.xxxx. You will also need to upload your best model (or provide us with the link to download your best model). The assessment will be based on your best model's accuracy, with up to 9 marks available, specifically:**\n",
        "* The best accuracy $\\ge$ 0.98: 9 marks\n",
        "* 0.98 $>$ The best accuracy $\\ge$ 0.92: 6 marks\n",
        "* 0.92 $>$ The best accuracy $\\ge$ 0.85: 3 marks\n",
        "* The best accuracy $<$ 0.85: 0 mark\n",
        "<div style=\"text-align: right\"><span style=\"color:red\">[9 marks]</span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj5tlhk_hRID"
      },
      "source": [
        "\\# Give your answer here.\n",
        "\n",
        "(i) What is your best model?\n",
        "\n",
        "(ii) The accuracy of your best model on the validation set\n",
        "\n",
        "(iii) The values of the hyperparameters of your best model\n",
        "\n",
        "(iv) The link to download your best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noy1YOpbhRID"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: center\"> <span style=\"color:green\">GOOD LUCK WITH YOUR ASSIGNMENT 2!</span> </div>\n",
        "<div style=\"text-align: center\"> <span style=\"color:black\">END OF ASSIGNMENT</span> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) the model i found so far is the attention version rnn with \"init-fine-tune\" and \"lstm\"\n",
        "\n",
        "(ii)the model accuracy is 0.9675.\n",
        "0.9750(the second time i run it)\n",
        "\n",
        "(iii) best parameter :\n",
        "embed_size = 128  \n",
        "state_sizes = [128, 256]\n",
        "epochs = 20  \n",
        "batch_size = 64  \n",
        "(iv) the model is included in the zip called \"best_model_A2.keras\"\n",
        "\n",
        "\n",
        "\n",
        "Note: accuracy can also be seen in the following result.\n"
      ],
      "metadata": {
        "id": "acOKMna35R7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_rnn = AttentionRNN(run_mode='init-fine-tune', cell_type='lstm', state_sizes=[128, 256], data_manager=dm)\n",
        "attention_rnn.build()\n",
        "attention_rnn.compile_model(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_attention_rnn = attention_rnn.fit(dm.train_numeral_data, train_labels_hot, epochs=20, batch_size=64, validation_split=0.2)\n",
        "evaluation_attention_rnn = attention_rnn.evaluate(dm.valid_numeral_data, valid_labels_hot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDTBlFFj0Mrt",
        "outputId": "33c3fbb8-2297-4e90-e9dc-690104bc923b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 35s 1s/step - loss: 1.2533 - accuracy: 0.5227 - val_loss: 0.7188 - val_accuracy: 0.7719\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 30s 2s/step - loss: 0.3751 - accuracy: 0.8883 - val_loss: 0.3073 - val_accuracy: 0.9219\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.1793 - accuracy: 0.9453 - val_loss: 0.2260 - val_accuracy: 0.9438\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.0906 - accuracy: 0.9758 - val_loss: 0.1703 - val_accuracy: 0.9563\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0739 - accuracy: 0.9789 - val_loss: 0.1066 - val_accuracy: 0.9688\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 19s 952ms/step - loss: 0.0800 - accuracy: 0.9766 - val_loss: 0.1317 - val_accuracy: 0.9563\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0527 - accuracy: 0.9836 - val_loss: 0.0944 - val_accuracy: 0.9656\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0308 - accuracy: 0.9914 - val_loss: 0.1275 - val_accuracy: 0.9781\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 27s 1s/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 0.0848 - val_accuracy: 0.9812\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 35s 2s/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.0954 - val_accuracy: 0.9781\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 20s 989ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 0.0953 - val_accuracy: 0.9781\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 30s 1s/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0970 - val_accuracy: 0.9781\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.1118 - val_accuracy: 0.9781\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0063 - accuracy: 0.9992 - val_loss: 0.1280 - val_accuracy: 0.9750\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 24s 1s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9719\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.1095 - val_accuracy: 0.9750\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1164 - val_accuracy: 0.9812\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1184 - val_accuracy: 0.9750\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 23s 1s/step - loss: 7.8041e-04 - accuracy: 1.0000 - val_loss: 0.1226 - val_accuracy: 0.9750\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 28s 1s/step - loss: 6.1942e-04 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9781\n",
            "13/13 [==============================] - 7s 166ms/step - loss: 0.0966 - accuracy: 0.9750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"best_model_A2.keras\"\n",
        "\n",
        "\n",
        "import os\n",
        "if os.path.exists(model_save_path):\n",
        "    os.remove(model_save_path)\n",
        "\n",
        "attention_rnn.model.save(model_save_path)\n"
      ],
      "metadata": {
        "id": "k-ZuqSBU3qll"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html A2_Section2_Solution.ipynb"
      ],
      "metadata": {
        "id": "A_lnthhp_8LY",
        "outputId": "6e462612-1335-4fee-b829-2fd18937e086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook A2_Section2_Solution.ipynb to html\n",
            "[NbConvertApp] Writing 1037699 bytes to A2_Section2_Solution.html\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}