{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Coding Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section consists of coding questions. The total mark for this section is **85**. \n",
    "The task we are considering in this section is the text classification task. In this section, some code has been provided to help you get started. For each cell marked with **# Insert your code here**, these are placeholders where you **must** supply your own codes when instructed.\n",
    "This section consists of 5 parts:\n",
    "* Part 0: Downloading and preprocessing data. This part has been completed. No marks are allocated for this part.\n",
    "* Part 1: Coding assessment on using Word2Vect to transform texts to vectors (20 marks).\n",
    "* Part 2: Coding assessment on Text CNN for sequence modeling and neural embedding (10 marks).\n",
    "* Part 3: Coding assessment on RNNs for sequence modeling and neural embedding (32 marks).\n",
    "* Part 4: Coding assessment on Transformer for sequence modeling and neural embedding and the overall ranking (23 marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Set random seeds</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing tensorflow and numpy and setting random seeds for TF and numpy. You can use any seeds you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(6789)\n",
    "np.random.seed(6789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 0: Download and preprocess the data</span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use for this assignment is a question classification dataset for which the training set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
    "- abbreviation (ABBR), \n",
    "- entity (ENTY), \n",
    "- description (DESC), \n",
    "- human (HUM), \n",
    "- location (LOC) and \n",
    "- numeric (NUM).\n",
    "\n",
    "In this assignment, we will utilize a subset of this dataset, containing $2,000$ questions for training and validation. We will use 80% of those 2000 questions for trainning and the rest for validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data is a crucial initial step in any machine learning or deep learning project. The *TextDataManager* class simplifies the process by providing functionalities to download and preprocess data specifically designed for the subsequent questions in this assignment. It is highly recommended to gain a comprehensive understanding of the class's functionality by **carefully reading** the content provided in the *TextDataManager.py* file before proceeding to answer the questions in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextDataManager import SimpleTextDataset as DataManager\n",
    "\n",
    "print('Loading data...')\n",
    "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "\n",
    "dm = DataManager(maxlen=100)\n",
    "dm.read_data(\"data/\", [\"train_2000.label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_split(train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dm.train_str_questions)\n",
    "print(dm.train_numeral_data.shape)  \n",
    "print(dm.valid_numeral_data.shape)  \n",
    "print(dm.train_numeral_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a data manager, named *dm* containing the training and validiation sets in both text and numeric forms. Your task is to play around and read this code to figure out the meanings of some important attributes that will be used in the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Using Word2Vect to transform texts to vectors </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 20 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will be assessed on how to use a pretrained Word2Vect model for realizing a machine learning task. Basically, you will use this pretrained Word2Vect to transform the questions in the above dataset stored in the *data manager object dm* to numeric form for training a classifier using Logistic Regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.1**</span> \n",
    "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
    "\n",
    "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vect = # Insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        vector = # Insert your code here\n",
    "    except: # word not in the vocabulary\n",
    "        vector = # Insert your code here\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.2**</span> \n",
    "\n",
    "**Write the code for the function `get_sentence_vector(sentence, important_score=None, model= None)`. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
    "- $important\\_weight = softmax(important\\_score)$.\n",
    "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[T]\\times v[T]$ where $T$ is the length of the sentence and $v[i]$ is the vector representation of the $i-th$  word in this sentence.\n",
    "\n",
    "**Note that if `important_score=None` is set by default, your function should return the average of all representation vectors corresponding to set `important_score=[1,1,...,1]`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, important_score=None, model=None):\n",
    "    # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.3**</span> \n",
    "\n",
    "**Write code to transform questions in *dm.train_str_questions* and *dm.valid_str_questions* to feature vectors. Note that after running the following cells, you must have $X\\_train$ and $X\\_valid$ which are two numpy arrays of the feature vectors and $y\\_train$ and $y\\_valid$ which are two arrays of numeric labels (Hint: *dm.train_numeral_labels* and *dm.valid_numeral_labels*). You can add more lines to the following cells if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 marks]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transform training set to feature vectors...\")\n",
    "X_train = # Insert your code here\n",
    "y_train = # Insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transform validation set to feature vectors...\")\n",
    "X_valid = # Insert your code here\n",
    "y_valid = # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.4**</span> \n",
    "\n",
    "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in scikit-learn to scale both training and validation sets to the range $(-1,1)$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X_train)\n",
    "X_train = # Insert your code here\n",
    "scaler.fit(X_valid)\n",
    "X_valid = # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.5**</span>\n",
    "**Train a Logistic Regression model on the training set and then evaluate on the validation set.**\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for validation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embedding visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you know, the embedding matrix is a collection of embedding vectors, each is for one word. In this part, you will base on the cosine similarity of the embedding vectors for the words to find the top-k most relevant words for a given word.**\n",
    "\n",
    "**Good embeddings should have words close in meaning near each other by some similarity metrics. The similarity metric we'll use is the `consine` similarity, which is defined for two vector $\\mathbf{u}$ and $\\mathbf{v}$ as $\\cos(\\mathbf{u}, \\mathbf{v})=\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\left\\Vert{\\mathbf{u}}\\right\\Vert\\left\\Vert{\\mathbf{v}}\\right\\Vert}$ where $\\cdot$ means dot product and $\\left\\Vert\\cdot\\right\\Vert$ means the $L^2$ norm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u,v):\n",
    "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.6** </span> \n",
    "\n",
    "**Write code for the `function find_most_similar(word=None, k=5, model=None)` which returns a list of the top-`k` most similar words (in descending order) for a given word. This similarity is based on the cosine similarity of embedding vectors obtained from the pretrained model `glove-wiki-gigaword-100` (see Question 1.1). The parameter `include_cur_word` specifies whether the given word should be included in the returned output or not. This means that if `include_cur_word=True`, the function will return `k+1` words.** \n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[3 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word=None, k=5, model=None, include_cur_word=False):\n",
    "    try:\n",
    "        # Insert your code here\n",
    "    except: # Word not in the vocabulary\n",
    "        print(\"Word is not in the dictionary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example of the above function. You can check your implementation using the provided `result`. As you can observe, the `result` makes sense which demonstrates that the embedding matrix is meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['computer','computers','software','technology','pc','hardware','internet','desktop','electronic','systems','computing']\n",
    "output = find_most_similar(word='computer', k=10, model=word2vect, include_cur_word=True)[0]\n",
    "if output == result:\n",
    "    print(\"Your implementation is correct.\")\n",
    "else:\n",
    "    print(\"Your implementation is not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.7** </span> \n",
    "**Implement the `plot2D_with_groups(word_list, model, k=10)` function to visualize groups of similar words in 2D space. The `word_list` parameter is a list of words, and for each word in the `word_list`, find its top-`k` most similar words (which forms a group) using the `find_most_similar` function. Use tSNE to project embedding vectors into 2D space and plot groups with different colors. You can use the colormaps from `matplotlib`, i.e., `cmap = plt.get_cmap('brg')`.**\n",
    "\n",
    "**The figure bellow is the output obtained by running `plot2D_with_groups` using the input `word_list=['an', 'introduction', 'to', 'deep', 'learning']`. Note that the words within the `word_list` are also visualized (in black) as shown in the figure.**\n",
    "\n",
    "<img src=\"./images/2Dtsne.png\" align=\"center\" width=600/>\n",
    "\n",
    "**As you can observe, words within each group tend to be closer to each other, while words from different groups are more distant.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity=5, n_components=2, init='pca', n_iter=5000)\n",
    "def plot2D_with_groups(word_list=None, k=10, model=None):\n",
    "   # Insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['an', 'introduction', 'to', 'deep', 'learning']\n",
    "plot2D_with_groups(word_list=word_list, k=10, model=word2vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Text CNN for sequence modeling and neural embedding </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.1**</span> \n",
    "\n",
    "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN that you need to construct.**\n",
    "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
    "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
    "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)).\n",
    "  - `data_manager`: the data manager to store information of the dataset.\n",
    "- The detail of the computational process is as follows:\n",
    "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size \\times maxlen \\times embed\\_size]$ as $h$.\n",
    "  - We feed $h$ to three *Conv1D* layers, each of which has $state\\_size$ filters, padding=same, activation= relu, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size \\times output\\_size \\times state\\_size]$.\n",
    "  - We then apply *GlobalMaxPool1D()* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
    "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$. Note that you need to specify the axis to concatenate.\n",
    "  - We finally build up one dense layer on the top of $h$ for classification.\n",
    "  \n",
    "  <div style=\"text-align: right\"><span style=\"color:red\">[8 marks]</span></div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
    "        self.data_manager = data_manager\n",
    "        self.embed_size = embed_size\n",
    "        self.state_size = state_size\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.data_manager.vocab_size +1, self.embed_size, mask_zero=True)(x)\n",
    "        h1 = # Insert your code here\n",
    "        h2 = # Insert your code here\n",
    "        h3 = # Insert your code here\n",
    "        h1 = # Insert your code here\n",
    "        h2 = # Insert your code here\n",
    "        h3 = # Insert your code here\n",
    "        h = # Insert your code here\n",
    "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h) \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.2**</span> \n",
    "**Here is the code to test TextCNN above. You can observe that TextCNN outperforms the traditional approach Word2Vect with Logistic Regression for this task. The reason is that TextCNN enables us to automatically learn the feature that fits to the task. This makes deep learning different from hand-crafted feature approaches. Complete the code to test the model. Note that when compiling the model, you can use the Adam optimizer.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cnn = TextCNN(data_manager=dm)\n",
    "text_cnn.build()\n",
    "# Insert your code here \n",
    "# You are required to compile the model and train the model on 20 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: RNN-based models for sequence modeling and neural embedding</span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 32 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">3.1. RNNs with different cell types</span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 12 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.1.1**</span> \n",
    "**In this part, you need to construct a vanilla RNN to learn from the dataset of interest. Basically, you are required to construct the class RNN with the following requirements:**\n",
    "- Attribute `data_manager (self.data_manager)`: specifies the data manager used to store data for the model.\n",
    "- Attribute `cell_type (self.cell_type)`: can take one of the three values, i.e., `simple_rnn`, `gru`, or `lstm` which specifies the memory cells formed a hidden layer.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes  of memory cells. For example, $state\\_sizes = [64, 128]$ means that you have two hidden layers in your network with hidden sizes of $64$ and $128$ respectively.\n",
    "\n",
    "**Note that when declaring an embedding layer for the network, you need to set *mask_zero=True* so that the padding zeros in the sentences will be masked and ignored. This helps to have variable length RNNs. For more detail, you can refer to this [link](https://www.tensorflow.org/guide/keras/masking_and_padding).**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[7 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRNN:\n",
    "    def __init__(self, cell_type='gru', embed_size=128, state_sizes=[128, 64], data_manager=None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size + 1 \n",
    "        \n",
    "    # return the corresponding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type='gru', state_size=128, return_sequences=False, activation='tanh'):\n",
    "        if cell_type == 'gru':\n",
    "            return # Insert your code here\n",
    "        elif cell_type == 'lstm':\n",
    "            return # Insert your code here\n",
    "        else:\n",
    "            return # Insert your code here\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = # Insert your code here\n",
    "        num_layers = # Insert your code here\n",
    "        for i in range(num_layers):\n",
    "            h =  # Insert your code here, you can insert more lines if necessary\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "   \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.1.2**</span> \n",
    "**Run with simple RNN ('simple_rnn') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 mark]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rnn = # Insert your code here\n",
    "base_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "base_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "base_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.1.3**</span> \n",
    "**Run with GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 mark]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rnn = # Insert your code here\n",
    "base_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "base_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "base_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.1.4**</span> \n",
    "**Run with LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 mark]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rnn = # Insert your code here\n",
    "base_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "base_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "base_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.1.5**</span> \n",
    "**Write code to conduct experiments to compare the accuracies of RNNs with the three different cell types using 2 different lists of `state_sizes` (while the other hyperparameters are fixed). Specifically, for each `state_sizes = [...]`, you should report the accuracies of RNNs with simple RNN ('simple_rnn') cell, GRU ('gru') cell, and LSTM ('lstm') cell. Give your comments on the results.**\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Give your comments on the results here (maximum 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">3.2. RNNs with fine-tuning embedding matrix</span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 8 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.2.1**</span> \n",
    "\n",
    "**In what follows, you are required to extend the class BaseRNN in Part 3.1 to achieve a class RNN in which the embedding matrix can be initialized using a pretrained Word2Vect.**\n",
    "\n",
    "**Below are the descriptions of the attributes of the class *RNN*:**\n",
    "- `run_mode (self.run_mode)` has three values (scratch, init-only, and init-fine-tune).\n",
    "  - `scratch` means training the embedding matrix from scratch.\n",
    "  - `init-only` means only initializing the embedding matrix with a pretrained Word2Vect but **not further doing** fine-tuning that matrix.\n",
    "  - `init-fine-tune` means both initializing the embedding matrix with a pretrained Word2Vect and **further doing** fine-tuning that matrix.\n",
    "- `cell_type (self.cell_type)` has three values (simple-rnn, gru, and lstm) which specify the memory cell used in the network.\n",
    "- `embed_model (self.embed_model)` specifes the pretrained Word2Vect model used.\n",
    "-  `embed_size (self.embed_size)` specifes the embedding size. Note that when run_mode is either 'init-only' or 'init-fine-tune', this embedding size is extracted from embed_model for dimension compatability.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes  of memory cells. For example, $state\\_sizes = [64, 128]$ means that you have two hidden layers in your network with hidden sizes of $64$ and $128$ respectively.\n",
    "\n",
    "**Complete the code of the class *RNN*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[6 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(BaseRNN):\n",
    "    def __init__(self, run_mode='scratch', embed_model='glove-wiki-gigaword-100', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.run_mode = run_mode\n",
    "        self.embed_model = embed_model\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.word2idx = dm.word2idx\n",
    "        self.word2vect = None\n",
    "        self.embed_matrix = np.zeros(shape=[self.vocab_size, self.embed_size])\n",
    "\n",
    "    def build_embedding_matrix(self):\n",
    "        # Insert your code here\n",
    "\n",
    "    def build(self):\n",
    "        # Insert your code here to override the 'build' method of BaseRNN\n",
    "\n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.2.2**</span> \n",
    "\n",
    "**Write code to conduct experiments to compare three running modes for the embedding matrix. Note that you should stick with fixed values for other attributes and only vary *run_mode*. Give your comments on the results.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Give your comments on the results here (maximum 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">3.3. RNNs with Attention for Text and Sequence Classification</span> ###\n",
    "\n",
    "**In what follows, you are required to implement a RNN with the attention mechanism for text and sequence classification. This attention mechanism is applied at the last hidden layer of our RNN. Specifically, let $\\textbf{h}_1^L, \\textbf{h}_2^L,...,\\textbf{h}_{T-1}^L, \\textbf{h}_T^L$ be the hidden states at the last hidden layer $L$ where $T$ is the sequence length. We compute the context vector $\\textbf{c}$ as $\\textbf{c}=\\sum_{i=1}^{T}\\textbf{a}_{i}\\textbf{h}_{i}^L$ where $\\textbf{a}_1,...,\\textbf{a}_T$ are the alignment weights (i.e., $\\textbf{a}_i\\geq 0$ and $\\sum_{i=1}^{T}\\textbf{a}_{i}=1$).**\n",
    "\n",
    "**The alignment weights are computed as follows:**\n",
    "- $\\textbf{a}=[\\textbf{a}_{i}]_{i=1}^{T}=\\text{softmax}(\\textbf{s})$ where $\\textbf{s}= [\\textbf{s}_{i}]_{i=1}^{T}$ consists of the alignment scores.\n",
    "- The alignment scores are computed as $\\textbf{s}=\\text{tanh}(\\textbf{h}^LU)V$ where $\\textbf{h}^L=\\left[\\begin{array}{c}\n",
    "\\textbf{h}_{1}^L\\\\\n",
    "\\textbf{h}_{2}^L\\\\\n",
    "...\\\\\n",
    "\\textbf{h}_{T-1}^L\\\\\n",
    "\\textbf{h}_{T}^L\n",
    "\\end{array}\\right]\\in\\mathbb{R}^{T\\times state\\_size_{L}}$, $U\\in\\mathbb{R}^{state\\_size_{L}\\times output\\_length}$, $V\\in\\mathbb{R}^{output\\_length\\times1}$, and $output\\_length$ is a hyperparameter. Note that if we consider a mini-batch, the shape of $\\textbf{h}^L$ is $(batch\\_size, T, state\\_size_L)$ where $state\\_size_L$ is the hidden size of the last hidden layer. The figure on the right below illustrates the process of calculating a score $\\textbf{s}_i$ for an individual hidden state $\\textbf{h}_i^L$. Weight matrices $U$ and $V$ are shared across the hidden states $\\textbf{h}_1^L,\\textbf{h}_2^L,\\dots,\\textbf{h}_T^L$.\n",
    "\n",
    "**After having the context vector $\\textbf{c}$, we concatenate with the last hidden state $\\textbf{h}_T^L$. On top of this concatenation, we conduct the output layer with the softmax activation.**\n",
    "\n",
    "<img src=\"./images/attentionRNN.png\" align=\"center\" width=700/>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 12 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.3.1**</span>\n",
    "\n",
    "**We declare the  layer `MyAttention` as a class inherited from `tf.keras.layers.Layer` to realize our attention mechanism. You are required to provide the code for this class. Note that in the `def call(self, all_states, last_state)` method, `all_states` is the collection of all hidden states and `last_state` is the last hidden state.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_length=50):\n",
    "        super().__init__()\n",
    "        # Insert your code here\n",
    "    \n",
    "    # all_states is the collection of all hidden states and last_state is the last hidden state\n",
    "    def call(self, all_states, last_state):\n",
    "        # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.3.2**</span> \n",
    "\n",
    "**You are required to extend the class `RNN` in Question `3.2.1` to achieve the class `AttentionRNN` in which the attention mechanism mentioned above is applied at the last hidden layer.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[6 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNN(RNN):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type='gru', hidden_size=128, return_sequences=False, activation='tanh'):\n",
    "        # Insert your code here to override the method get_layer() in BaseRNN class.\n",
    "\n",
    "    def build(self):\n",
    "        # Insert your code here to override the method build() in BaseRNN class.\n",
    "\n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.3.3**</span> \n",
    "\n",
    "**Choose a common setting for standard RNN and RNN with attention and conduct experiments to compare them. The setting here means `run_mode`, `cell_type` and list of `state_sizes`. Give your comments on the results.** \n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Give your comments on the results here (maximum 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: Transformer-based models for sequence modeling and neural embedding and the overall ranking</span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 23 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.1**</span> \n",
    "\n",
    "**Implement the multi-head attention module of the Transformer for the text classification problem. The provided code is from [this source](https://keras.io/examples/nlp/text_classification_with_transformer/). In this part, we only use the output of the Transformer encoder for the classification task. For further information on the Transformer model, refer to [this paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[11 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    # Insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = # Insert your code here to call 'MultiHeadAttention' class\n",
    "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "vocab_size = dm.vocab_size + 1\n",
    "maxlen = dm.maxlen\n",
    "\n",
    "def create_tfm_model(embed_dim=64, num_heads=8, ff_dim=32, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Creates a Transformer model using the given hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "        embed_dim (int): The embedding dimension for each token.\n",
    "        num_heads (int): The number of attention heads in the multi-head attention layer.\n",
    "        ff_dim (int): The hidden layer size in the feed forward network inside the transformer block.\n",
    "        dropout_rate (float): The dropout rate for regularization.\n",
    "    Returns:\n",
    "        keras.models.Sequential: A Transformer model.\n",
    "    \"\"\"\n",
    "    tfm = Sequential()\n",
    "    tfm.add(Input(shape=(maxlen,)))\n",
    "    tfm.add(TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim))\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    tfm.add(transformer_block)\n",
    "    tfm.add(GlobalAveragePooling1D())\n",
    "    tfm.add(Dropout(dropout_rate))\n",
    "    tfm.add(Dense(units=dm.num_classes, activation='softmax'))\n",
    "    tfm.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_tfm_model()\n",
    "model.fit(dm.tf_train_set.batch(64), epochs=20, validation_data=dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.2**</span> \n",
    "**Write code to conduct experiments to evaluate the impact of hyperparameters `embed_dim`, `num_heads`, `ff_dim`, and `dropout_rate` of the Transformer model on the accuracy. Report (i) your findings from the experiments, (ii) the accuracy of your best Transformer model (i.e., the one with the highest accuracy on the validation set), and (iii) the values of the mentioned hyperparameters of that best model.  \n",
    "Note that the necessary condition to get the full mark for this question is that the accuracy of your best Transformer model should be at least 90%.** \n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[3 marks]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Give your answer here. \n",
    "\n",
    "(i) Your findings from the experiments (maximum 200 words)\n",
    "\n",
    "(ii) The accuracy of your best Transformer model on the validation set\n",
    "\n",
    "(iii) The values of the mentioned hyperparameters of your best Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.3**</span> \n",
    "**For any models defined in the previous questions (of all parts), you are free to fine-tune hyperparameters, e.g., `optimizer`, `learning_rate`, `state_sizes`, such that you get a best model, i.e., the one with the highest accuracy on the validation set. You will need to report (i) what is your best model,  (ii) its accuracy on the validation set, and (iii) the values of its hyperparameters. Note that you must report your best model's accuracy with rounding to 4 decimal places, i.e., 0.xxxx. You will also need to upload your best model (or provide us with the link to download your best model). The assessment will be based on your best model's accuracy, with up to 9 marks available, specifically:**\n",
    "* The best accuracy $\\ge$ 0.98: 9 marks\n",
    "* 0.98 $>$ The best accuracy $\\ge$ 0.92: 6 marks\n",
    "* 0.92 $>$ The best accuracy $\\ge$ 0.85: 3 marks\n",
    "* The best accuracy $<$ 0.85: 0 mark\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[9 marks]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Give your answer here. \n",
    "\n",
    "(i) What is your best model?\n",
    "\n",
    "(ii) The accuracy of your best model on the validation set\n",
    "\n",
    "(iii) The values of the hyperparameters of your best model\n",
    "\n",
    "(iv) The link to download your best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<div style=\"text-align: center\"> <span style=\"color:green\">GOOD LUCK WITH YOUR ASSIGNMENT 2!</span> </div>\n",
    "<div style=\"text-align: center\"> <span style=\"color:black\">END OF ASSIGNMENT</span> </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
