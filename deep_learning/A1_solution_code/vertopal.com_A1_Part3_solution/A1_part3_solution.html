<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>9ac88920e9814ec9ab18faf6d7506316</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section
id="part-3-convolutional-neural-networks-and-image-classification"
class="cell markdown" id="7yp9gdRi--w5">
<h2><span style="color:#0b486b">Part 3: Convolutional Neural Networks
and Image Classification</span></h2>
<div style="text-align: right"><span style="color:red">[Total marks for this part: 40 points]</span></div>
</section>
<div class="cell markdown" id="r_JpTYOU--w-" data-tags="[]">
<p>This part of the asssignment is designed to assess your knowledge and
coding skill with Tensorflow as well as hands-on experience with
training Convolutional Neural Network (CNN).</p>
<p>The dataset we use for this part is the <a
href="https://cs.stanford.edu/~acoates/stl10/">STL10 dataset</a> which
consists of <span class="math inline">5,â€†000</span> training images of
airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck; each of
which has 500 images. You can download the dataset at <a
href="https://drive.google.com/file/d/1bEwEx72lLrjY_Idj_FgV22atIdjtCV66/view?usp=sharing">download
here</a> and then decompress to the folder <code>datasets\Animals</code>
in your assignment folder.**</p>
<p>Your task is to build a CNN model using <em>TF 2.x</em> to classify
these animals. You're provided with the module
<span style="color:red">models.py</span>, which you can find in the
assignment folder, with some of the following classes:</p>
<ol>
<li><code>DatasetManager</code>: Support with loading and spliting the
dataset into the train-val-test sets. It also supports generating next
batches for training.</li>
<li><code>BaseImageClassifier</code>: A base class image classfication,
which is basically a CNN model.</li>
</ol>
<p><em>Note</em>: You may need to install the package
<code>imutils</code> if you have not installed yet</p>
</div>
<div class="cell markdown" id="khjbMFui--w_">
<p>Firstly, we need to run the following cells to load required
packages.</p>
</div>
<div class="cell code" id="M_CREWH4--w_">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code></pre></div>
</div>
<div class="cell code" id="DgIxW-jo--xC">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">&#39;ggplot&#39;</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> A1_S2_2023 <span class="im">import</span> DatasetManager, BaseImageClassifier</span></code></pre></div>
</div>
<div class="cell markdown" id="ikJy9fhO--xD">
<p>Note that the class <code>DatasetManager</code> has attributes
related to <em>the training, validation, and testing sets</em>. You can
use them in training your developped models in the sequel.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:905,&quot;referenced_widgets&quot;:[&quot;7b1070ac1dbd495cae3f27b52a956161&quot;,&quot;95dcdcad1d594b7e8e5f165005c710be&quot;,&quot;98fa42f841ba4ac79da94674a0c86269&quot;,&quot;d13db60f222849faad893f098cebef0b&quot;,&quot;677b125b73b241229746b1ca60adc789&quot;,&quot;e678a29376d940bf911464f7414fc709&quot;,&quot;dfe722c2dd08429dba89757f274364d5&quot;,&quot;187dd8e3474c4f97b4d46f44b18d5d51&quot;,&quot;e0ff45f96a884081b606f833e9f96828&quot;,&quot;835c9f7bedf149e687eb4071237f2b53&quot;,&quot;51e66b8034b149bf9f3501517617231a&quot;,&quot;fc037ffe02874e51b138e3201a397b0f&quot;,&quot;758026432a4f4733b0a75689f1fbc0f3&quot;,&quot;c5ad6044a19447b3b4ed210c4b45c6d8&quot;,&quot;8a4270b4f5b64e3ba0f2814709211c58&quot;,&quot;f42d82bcae4747158a557397ba65c150&quot;,&quot;010fc05bdea64ffc8d011dcf5c63aa14&quot;,&quot;ec5141ad02c64b5c84a47f95abf6cb40&quot;,&quot;ccd340e9c4ea4171b7a7bfa8226cdeb5&quot;,&quot;109dbf97004b4139be9226f39f743c40&quot;,&quot;d71b4a6bbf5742f0b8432c22c5ba722f&quot;,&quot;8e4399c5a71343c594872664f2109f32&quot;,&quot;992b3c985dfa47039423763c8b75c59f&quot;,&quot;27c1054134ce45b6ae0dd6c5dbf8c881&quot;,&quot;a0ce48138eac42f9b31cf5cd46450bba&quot;,&quot;53cdd653e5b74e34b4bb001d9eae4451&quot;,&quot;572429d13add43d7bc6d8197399f78dc&quot;,&quot;495cb1311830444d9b8e5103b5ee46c4&quot;,&quot;9a387394fedf497290fcc544c5cf8396&quot;,&quot;5aa12c1683454817afaa220c13787c30&quot;,&quot;cd4f93b67b8d43eab11220db72ad23da&quot;,&quot;e3c2fa0e00ee44758bb232c74b2b37da&quot;,&quot;3729eda6eb92475683c6814f8cb4cdbc&quot;,&quot;041705454dbb40e09d0f4de6dc9fb07d&quot;,&quot;c0b16a5bc9624f10920798aa86471160&quot;,&quot;01210ca4382b42b1abc634c79f16f78a&quot;,&quot;2e39fff5ed7c497683bc537033c013b4&quot;,&quot;587baae326634353ab681344f2ecca39&quot;,&quot;2e3613c8b40947efab5de3f761ff7502&quot;,&quot;352932ff052a433a894c50e8c3f2d210&quot;,&quot;d1b2d0ab487747f9b06237fd7cbc945f&quot;,&quot;205147b9cba143f7b303ec4e9e015100&quot;,&quot;cef22f72f044406ea8aef53a58876cfe&quot;,&quot;b5e50e1d2af6413b8256c54547b626a1&quot;,&quot;063727f1bd5d41478430b0513cff47d6&quot;,&quot;c751fa6753ac4e5cbb157d151b0b4e06&quot;,&quot;8858039d26b64fdb9ba2cfe30d38a240&quot;,&quot;dc74da48773d4fd0bc3a442c0d03ec39&quot;,&quot;db97a980bf31477e99e79602b564669b&quot;,&quot;d44fa987874548e08a3f2a4e62d90a81&quot;,&quot;c31b509d73a045da8651788296fefd0d&quot;,&quot;b5c9225052c9470fa0302e3b998136d3&quot;,&quot;1918928c80f94896a594ecd52088ff47&quot;,&quot;fdd5e8adae4746f7b8ffc83fb248c949&quot;,&quot;a5d4a6291d2c447fbb6b734209ffa791&quot;,&quot;6b2f5a9b652949ce80ad40a13008b4c4&quot;,&quot;362d483b3b184484a3072f82dfaaf0d7&quot;,&quot;77183e757e424830bc3d913abdcedbcb&quot;,&quot;827fe409d2924b6787257595249ab070&quot;,&quot;15db247449b74d9a871094f1df5e4680&quot;,&quot;3b7fd803b63148acadbebee613802461&quot;,&quot;3838e514efce4c24b44d6dc346bfa0fe&quot;,&quot;a6faa9b54ea54bccb1b8c1550a2d267e&quot;,&quot;e4d2da5a14694f60a52da15057cabf7c&quot;,&quot;3cb962f864f2458184b5cca7391d87ef&quot;,&quot;7e959d73450f4c27ac556b2ca1bc9e7a&quot;,&quot;ae4bfa52838c4b35a2944c5af0110ef6&quot;,&quot;9041ba296b294acd89519bad61c8c2e6&quot;,&quot;182c471c8fea446d80f50461cc55976c&quot;,&quot;9d9df90a01d4410e85d9833ade374220&quot;,&quot;3398101f4dab422dbd99e817e850b59e&quot;,&quot;e980533c2264488794de220c2236bd99&quot;,&quot;23ca5d19ef3c4b3584bbc85475f258c0&quot;,&quot;e5c4dfc134504d998914dc39173d74c0&quot;,&quot;7c15276307a64ee9b2181926f995b594&quot;,&quot;e5f40c24ed4b46559f1882710127135b&quot;,&quot;a6c90b2631b04b219fde58977f68dac8&quot;,&quot;4ed77441a0e64a1dab5df1bca61dd843&quot;,&quot;01015e6cc9d349ebafabe255e354fa2e&quot;,&quot;683e3e50f9f342cc99eff1db759686ed&quot;,&quot;7112887c0acd4edeb6ca3dc85eb15c23&quot;,&quot;f8aabe0234404462a04f14e01c46c254&quot;,&quot;87f1d8639154493a863b7ed95b5b13ee&quot;,&quot;2f4a197a087641448f0f2e8132df1ea8&quot;,&quot;d332418fb5be4f5fa5023b4c885a1f7b&quot;,&quot;d987dab9cca342599b9517b579a4a7dd&quot;,&quot;c2a9726a3e4141ca9bfe74e853f8ff25&quot;,&quot;61c57be3dbe24e16a9dad23b9404c611&quot;,&quot;f17ba80df5bc47bab2d8171f59a66e47&quot;,&quot;eed86f2860bd46729087403475022b13&quot;,&quot;ea9b6133f1024fbcb647c59638765929&quot;,&quot;431f14326e46477a95c01d935f2c11e0&quot;,&quot;61564cf33c3145db98d9e3228f242c11&quot;,&quot;88968698c0f04b44ae1152417c6c26db&quot;,&quot;5cca5cc791d34bc6b9ec7fbe69d6e48b&quot;,&quot;4409f1d9b89b4b729e9e28cc00cdf652&quot;,&quot;d9b424f7f93a4bbcb063a97da5280f0c&quot;,&quot;d8e7e3afc968452e899fcf74e6ca0c01&quot;,&quot;f774dcb78dbe4204b30a4c7a45be3bce&quot;,&quot;8c4c7d3ddd754b819f255bc17f180e98&quot;,&quot;6914c6fc83924dd890d820d528d902d4&quot;,&quot;33cf81be07cc4daf8585f94013f04350&quot;,&quot;d7f89bc37c04466f898563c1593a444f&quot;,&quot;c0534d17d42e4073be684b96614512d9&quot;,&quot;cd37b5f6b7b5412ab7fb775a5abacd68&quot;,&quot;8e623db2bcee49a5bb7911dd6d932b9c&quot;,&quot;c39778409ea54cd481dca788f2dda5c4&quot;,&quot;e33f5bf15d36491d93c57fb935d2ee8f&quot;,&quot;2ae14eb5b0a940c7b2f762fde4609a5e&quot;,&quot;b3004fd01d764f89b3391de9335c8045&quot;]}"
id="dg_15iGa--xD" data-outputId="f541852d-a9c9-4444-88ec-6535d0231f26">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">&#39;stl10&#39;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose path to store dataset</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> <span class="st">&#39;</span><span class="sc">{}</span><span class="st">/tensorflow_datasets&#39;</span>.<span class="bu">format</span>(os.path.expanduser(<span class="st">&#39;~&#39;</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>data_manager <span class="op">=</span> DatasetManager(dataset_name, data_dir)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>data_manager.load_dataset()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>data_manager.preprocess_dataset()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>data_manager.show_examples()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Downloading and preparing dataset 2.46 GiB (download: 2.46 GiB, generated: 1.86 GiB, total: 4.32 GiB) to /root/tensorflow_datasets/stl10/1.0.0...
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb5"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;7b1070ac1dbd495cae3f27b52a956161&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb6"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;fc037ffe02874e51b138e3201a397b0f&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb7"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;992b3c985dfa47039423763c8b75c59f&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb8"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;041705454dbb40e09d0f4de6dc9fb07d&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb9"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;063727f1bd5d41478430b0513cff47d6&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb10"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;6b2f5a9b652949ce80ad40a13008b4c4&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb11"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;ae4bfa52838c4b35a2944c5af0110ef6&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb12"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;4ed77441a0e64a1dab5df1bca61dd843&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb13"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;f17ba80df5bc47bab2d8171f59a66e47&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb14"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;8c4c7d3ddd754b819f255bc17f180e98&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>Dataset stl10 downloaded and prepared to /root/tensorflow_datasets/stl10/1.0.0. Subsequent calls will reuse this data.
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_262e2868f6474e89852d78a2fd9920d2/e42841fd39f76f130d301606cff1d795d50ffa76.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="NtA7z5Hh--xE" data-outputId="6565d6c8-b1f2-4dec-84bb-46d45c9dfbc7">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose a random example</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>num_examples <span class="op">=</span> tf.data.experimental.cardinality(data_manager.ds_train).numpy()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>random_index <span class="op">=</span> random.randint(<span class="dv">0</span>, num_examples <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>example <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(data_manager.ds_train.skip(random_index).take(<span class="dv">1</span>)))[<span class="dv">0</span>]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape and value of the image</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Image shape:&quot;</span>, example.shape)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Image value range:&quot;</span>, example.numpy().<span class="bu">min</span>(), <span class="st">&quot;to&quot;</span>, example.numpy().<span class="bu">max</span>())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Image shape: (32, 32, 3)
Image value range: 0.09803922 to 0.9647059
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="P-POoUTC--xE" data-outputId="33b45c22-4386-46ea-dcb5-cf82f9bcb23c">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the number of examples in each dataset</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tf.data.experimental.cardinality(data_manager.ds_train))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tf.data.experimental.cardinality(data_manager.ds_val))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tf.data.experimental.cardinality(data_manager.ds_test))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>tf.Tensor(4500, shape=(), dtype=int64)
tf.Tensor(500, shape=(), dtype=int64)
tf.Tensor(8000, shape=(), dtype=int64)
</code></pre>
</div>
</div>
<div class="cell markdown" id="1JxAU_Pp--xF">
<p>We now use <strong>BaseImageClassifier</strong> built in the
<strong>A1_S2_2023.py</strong> file which serves as a basic baseline to
start the investigation. Follow the following steps to realize how to
run a model and know the built-in methods associated with.</p>
</div>
<div class="cell code" id="x6o5m_KC--xF">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>network1 <span class="op">=</span> BaseImageClassifier(name<span class="op">=</span><span class="st">&#39;network1&#39;</span>,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>                       num_classes<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>                       optimizer<span class="op">=</span><span class="st">&#39;sgd&#39;</span>,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                       batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                       num_epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>                       learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span></code></pre></div>
</div>
<div class="cell markdown" id="KqhgJJot--xG">
<p>We first initialize a default model from the DefaultModel class.
Basically, we can define the relevant parameters of training a model
including <code>num_classes</code>, <code>optimizer</code>,
<code>learning_rate</code>, <code>batch_size</code>, and
<code>num_epochs</code>.</p>
</div>
<div class="cell markdown" id="-0kE0Txw--xH">
<p>The method <code>build_cnn()</code> assists us in building your
convolutional neural network. You can view the code (in the
<strong>A1_S2_2023.py</strong> file) of the model behind a default model
to realize how simple it is. Additionally, the method
<code>summary()</code> shows the architecture of a model.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="YrxKyOwZ--xI" data-outputId="3f622a42-5873-4527-ca9b-ff8774bf13c0">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>network1.build_cnn()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>network1.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 32, 32, 32)        896       
                                                                 
 conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      
                                                                 
 average_pooling2d (AverageP  (None, 16, 16, 32)       0         
 ooling2D)                                                       
                                                                 
 conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 average_pooling2d_1 (Averag  (None, 8, 8, 64)         0         
 ePooling2D)                                                     
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 10)                40970     
                                                                 
=================================================================
Total params: 106,538
Trainable params: 106,538
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="kNm7k7_E--xJ" data-outputId="c43d318e-bdf4-4a25-96b8-21db96854a85">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>x_train_batch <span class="op">=</span> network1.optimize_data_pipeline(data_manager.ds_train, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>x_val_batch <span class="op">=</span> network1.optimize_data_pipeline(data_manager.ds_val, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>network1.fit(x_train_batch, x_val_batch, num_epochs<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/20
141/141 [==============================] - 16s 31ms/step - loss: 2.2981 - accuracy: 0.1222 - val_loss: 2.2889 - val_accuracy: 0.1500
Epoch 2/20
141/141 [==============================] - 1s 5ms/step - loss: 2.2765 - accuracy: 0.1427 - val_loss: 2.2474 - val_accuracy: 0.1940
Epoch 3/20
141/141 [==============================] - 1s 5ms/step - loss: 2.1806 - accuracy: 0.1969 - val_loss: 2.1057 - val_accuracy: 0.2500
Epoch 4/20
141/141 [==============================] - 1s 5ms/step - loss: 2.0548 - accuracy: 0.2629 - val_loss: 1.9850 - val_accuracy: 0.2960
Epoch 5/20
141/141 [==============================] - 1s 4ms/step - loss: 1.9539 - accuracy: 0.3029 - val_loss: 1.9003 - val_accuracy: 0.3360
Epoch 6/20
141/141 [==============================] - 1s 4ms/step - loss: 1.8941 - accuracy: 0.3187 - val_loss: 1.8511 - val_accuracy: 0.3460
Epoch 7/20
141/141 [==============================] - 1s 5ms/step - loss: 1.8360 - accuracy: 0.3376 - val_loss: 1.8105 - val_accuracy: 0.3560
Epoch 8/20
141/141 [==============================] - 1s 5ms/step - loss: 1.7824 - accuracy: 0.3649 - val_loss: 1.7775 - val_accuracy: 0.3620
Epoch 9/20
141/141 [==============================] - 1s 5ms/step - loss: 1.7403 - accuracy: 0.3756 - val_loss: 1.7436 - val_accuracy: 0.3720
Epoch 10/20
141/141 [==============================] - 1s 4ms/step - loss: 1.7052 - accuracy: 0.3900 - val_loss: 1.7134 - val_accuracy: 0.3800
Epoch 11/20
141/141 [==============================] - 1s 6ms/step - loss: 1.6717 - accuracy: 0.4027 - val_loss: 1.6897 - val_accuracy: 0.3780
Epoch 12/20
141/141 [==============================] - 1s 6ms/step - loss: 1.6402 - accuracy: 0.4124 - val_loss: 1.6700 - val_accuracy: 0.3880
Epoch 13/20
141/141 [==============================] - 1s 5ms/step - loss: 1.6102 - accuracy: 0.4251 - val_loss: 1.6527 - val_accuracy: 0.3940
Epoch 14/20
141/141 [==============================] - 1s 5ms/step - loss: 1.5811 - accuracy: 0.4329 - val_loss: 1.6379 - val_accuracy: 0.4040
Epoch 15/20
141/141 [==============================] - 1s 4ms/step - loss: 1.5540 - accuracy: 0.4376 - val_loss: 1.6254 - val_accuracy: 0.4260
Epoch 16/20
141/141 [==============================] - 1s 5ms/step - loss: 1.5261 - accuracy: 0.4478 - val_loss: 1.6139 - val_accuracy: 0.4340
Epoch 17/20
141/141 [==============================] - 1s 4ms/step - loss: 1.4978 - accuracy: 0.4564 - val_loss: 1.6024 - val_accuracy: 0.4440
Epoch 18/20
141/141 [==============================] - 1s 5ms/step - loss: 1.4693 - accuracy: 0.4700 - val_loss: 1.5910 - val_accuracy: 0.4540
Epoch 19/20
141/141 [==============================] - 1s 4ms/step - loss: 1.4404 - accuracy: 0.4800 - val_loss: 1.5783 - val_accuracy: 0.4600
Epoch 20/20
141/141 [==============================] - 1s 5ms/step - loss: 1.4111 - accuracy: 0.4931 - val_loss: 1.5668 - val_accuracy: 0.4560
</code></pre>
</div>
</div>
<div class="cell markdown" id="-5ubGopx--xK">
<p>To train a model regarding to the datasets stored in
<code>data_manager</code>, you can invoke the method <code>fit()</code>
for which you can specify the batch size and number of epochs for your
training.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="bDSIFZZk--xL" data-outputId="257ec845-23cb-49f4-d0a8-90616c8397fe">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>x_test_batch <span class="op">=</span> network1.optimize_data_pipeline(data_manager.ds_test, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>network1.compute_accuracy(x_test_batch)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>250/250 [==============================] - 4s 17ms/step - loss: 1.5981 - accuracy: 0.4289
loss: 1.5980982780456543
accuracy: 0.42887499928474426
</code></pre>
</div>
</div>
<div class="cell markdown" id="ey1Eoyzt--xL">
<p>Below shows how you can inspect the training progress.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:453}"
id="4I8FMOkG--xM" data-outputId="05dcdd4e-e0b8-4083-9b98-2cf7378feaa5">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>network1.plot_progress()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_262e2868f6474e89852d78a2fd9920d2/d1012e2a706bc68f695c5b5904ea48d57ad6a032.png" /></p>
</div>
</div>
<div class="cell markdown" id="xFMTObjF--xM">
<p>You can use the method <code>predict()</code> to predict labels for
data examples in a test set.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="wAZsJHsX--xN" data-outputId="c3defd60-e900-43fe-f56d-07b3b55ca03a">
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>sample_dataset <span class="op">=</span> data_manager.ds_test.take(num_samples)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>network1.predict(sample_dataset.batch(num_samples), data_manager.ds_info)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>1/1 [==============================] - 0s 298ms/step
Sample 1: Predicted label - ship
Sample 2: Predicted label - monkey
Sample 3: Predicted label - cat
Sample 4: Predicted label - bird
Sample 5: Predicted label - ship
Sample 6: Predicted label - dog
Sample 7: Predicted label - deer
Sample 8: Predicted label - cat
Sample 9: Predicted label - airplane
Sample 10: Predicted label - deer
Sample 11: Predicted label - deer
Sample 12: Predicted label - ship
Sample 13: Predicted label - horse
Sample 14: Predicted label - ship
Sample 15: Predicted label - deer
Sample 16: Predicted label - cat
Sample 17: Predicted label - deer
Sample 18: Predicted label - airplane
Sample 19: Predicted label - truck
Sample 20: Predicted label - ship
Sample 21: Predicted label - dog
Sample 22: Predicted label - horse
Sample 23: Predicted label - ship
Sample 24: Predicted label - truck
Sample 25: Predicted label - airplane
</code></pre>
</div>
</div>
<div class="cell markdown" id="mJ694x_X--xN">
<p>Finally, the method <code>plot_prediction()</code> visualizes the
predictions for a test set in which several images are chosen to show
the predictions.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:522}"
id="Ouwgbvne--xO" data-outputId="fc8ebee4-bee2-4e7b-ba4e-76c3e4bdfe51"
data-tags="[]">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>sample_dataset <span class="op">=</span> data_manager.ds_test.take(num_samples)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>network1.plot_predictions(sample_dataset, data_manager.ds_info, num_samples<span class="op">=</span>num_samples, grid_shape<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">5</span>))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>1/1 [==============================] - 0s 79ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_262e2868f6474e89852d78a2fd9920d2/7a987902c591a89e9b83b8b7c84c4fc24f84a676.png" /></p>
</div>
</div>
<section id="question-31-observe-the-learning-curve"
class="cell markdown" id="1g0l0ZLg--xO">
<h3><span style="color:#0b486b">Question 3.1: Observe the learning
curve</span></h3>
<p>After running the above cells to train the default model and observe
the learning curve. Report your observation (i.e. did the model learn
well? if not, what is the problem? What would you do to improve it?).
Write your answer below.</p>
<div style="text-align: right"> <span style="color:red">[4 points]</span> </div>
</section>
<div class="cell markdown" id="_nDqv_rqKb4P">
<p>The model behaves not too goood. The accuracy for test dataset is
only 0.422.</p>
<p>What could be done to immprove: increase number of epoches, add skip
connection, add dropout</p>
</div>
<div class="cell markdown" id="om9BYwk0--xP">
<p><strong>For questions 3.2 to 3.9</strong>, you'll need to write your
own model in a way that makes it easy for you to experiment with
different architectures and parameters. The goal is to be able to pass
the parameters to initialize a new instance of <code>YourModel</code> to
build different network architectures with different parameters. Below
are descriptions of some parameters which you can find in function
<code>__init__()</code> for the class
<code>BaseImageClassifier</code>:</p>
<ol>
<li><p><code>num_blocks</code>: an integer specifying the number of
blocks in our network. Each block has the pattern
<code>[conv, batch norm, activation, conv, batch norm, activation, mean pool, dropout]</code>.
All convolutional layers have filter size <span
class="math inline">(3,3)</span>, strides <span
class="math inline">(1,1)</span> and 'SAME' padding, and all mean pool
layers have strides <span class="math inline">(2,2)</span> and 'SAME'
padding. The network will consists of a few blocks before applying a
linear layer to output the logits for the softmax layer.</p></li>
<li><p><code>feature_maps</code>: the number of feature maps in the
first block of the network. The number of feature_maps will double in
each of the following block. To make it convenient for you, we already
calculated the number of feature maps for each block for you in line
<span class="math inline">106</span></p></li>
<li><p><code>drop_rate</code>: the keep probability for dropout. Setting
<code>drop_rate</code> to <span class="math inline">0.0</span> means not
using dropout.</p></li>
<li><p><code>batch_norm</code>: the batch normalization function is used
or not. Setting <code>batch_norm</code> to <code>None</code> means not
using batch normalization.</p></li>
<li><p>The <code>skip connection</code> is added to the output of the
second <code>batch norm</code>. Additionally, your class has a boolean
property (i.e., instance variable) named <code>use_skip</code>. If
<code>use_skip=True</code>, the skip connectnion is enable. Otherwise,
if <code>use_skip=False</code>, the skip connectnion is
disable.</p></li>
</ol>
<p>Below is the architecture of one block:</p>
<p><img src="Figures/OneBlock.png" width="350" align="center"/></p>
<p>Below is the architecture of the entire deep net with
<code>two blocks</code>:</p>
<p><img src="Figures/NetworkArchitecture.png" width="1200" align="center"/></p>
<p>Here we assume that the first block has
<code>feature_maps = feature_maps[0] = 32</code>. Note that the initial
number of feature maps of the first block is declared in the instance
variable <code>feature_maps</code> and is multiplied by <span
class="math inline">2</span> in each follpwing block.</p>
</div>
<div class="cell code" id="j-2Nv08B--xP">
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers, models</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">3181</span>)</span></code></pre></div>
</div>
<section id="question-32-define-your-cnn" class="cell markdown"
id="34UnCoYh--xQ">
<h3><span style="color:#0b486b">Question 3.2: Define your
CNN</span></h3>
<p>Write the code of the <code>YourModel</code> class here. Note that
this class will be inherited from the <code>BaseImageClassifier</code>
class. You'll only need to re-write the code for the
<code>build_cnn</code> method in the <code>YourModel</code> class from
the cell below.</p>
<div style="text-align: right"> <span style="color:red">[4 points]</span> </div>
</section>
<div class="cell code" id="iu55zTZk--xQ">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, Flatten, Dropout</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> YourModel(BaseImageClassifier):</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>                 name<span class="op">=</span><span class="st">&#39;network1&#39;</span>,</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>                 width<span class="op">=</span><span class="dv">32</span>, height<span class="op">=</span><span class="dv">32</span>, depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>                 num_blocks<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>                 feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>                 num_classes<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>                 drop_rate<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>                 batch_norm<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>                 is_augmentation<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>                 activation_func<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>,</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>                 use_skip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>                 batch_size<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>                 num_epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="fl">0.0001</span>,</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>                 verbose<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(YourModel, <span class="va">self</span>).<span class="fu">__init__</span>(name, width, height, depth, num_blocks, feature_maps, num_classes, drop_rate, batch_norm, is_augmentation,</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>                                        activation_func, use_skip, optimizer, batch_size, num_epochs, learning_rate, verbose)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> custom_block(<span class="va">self</span>, x, filters):</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># skip connection</span></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>        shortcut <span class="op">=</span> x</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First Conv layer</span></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span>filters, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second Conv layer</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span>filters, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the skip connection to the output</span></span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_skip:</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> tf.keras.layers.add([shortcut, x])</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mean Pooling layer</span></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.AveragePooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout</span></span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(rate<span class="op">=</span><span class="va">self</span>.drop_rate)(x)</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_cnn(<span class="va">self</span>):</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>      input_tensor <span class="op">=</span> layers.Input(shape<span class="op">=</span>(<span class="va">self</span>.height, <span class="va">self</span>.width, <span class="va">self</span>.depth))</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> input_tensor</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> current_feature_maps <span class="kw">in</span> <span class="va">self</span>.feature_maps:</span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial convolution</span></span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(current_feature_maps, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, activation<span class="op">=</span><span class="va">None</span>)(x)</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second convolution with skip connection</span></span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>        shortcut <span class="op">=</span> x</span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(current_feature_maps, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, activation<span class="op">=</span><span class="va">None</span>)(x)</span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_skip:</span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.Add()([shortcut, x])</span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mean Pooling and Dropout</span></span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.AveragePooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.drop_rate <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb33-77"><a href="#cb33-77" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.Dropout(<span class="va">self</span>.drop_rate)(x)</span>
<span id="cb33-78"><a href="#cb33-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-79"><a href="#cb33-79" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> layers.Flatten()(x)</span>
<span id="cb33-80"><a href="#cb33-80" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> layers.Dense(<span class="va">self</span>.num_classes, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)(x)</span>
<span id="cb33-81"><a href="#cb33-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-82"><a href="#cb33-82" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.model <span class="op">=</span> models.Model(inputs<span class="op">=</span>input_tensor, outputs<span class="op">=</span>x)</span>
<span id="cb33-83"><a href="#cb33-83" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="va">self</span>.optimizer, loss<span class="op">=</span><span class="st">&#39;sparse_categorical_crossentropy&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb33-84"><a href="#cb33-84" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
</div>
<section id="question-33-experiment-with-skip-connection"
class="cell markdown" id="wS3aL4zF--xQ">
<h3><span style="color:#0b486b">Question 3.3: Experiment with skip
connection</span></h3>
<p>Once writing your own model, you need to compare two cases: (i)
<em>using the skip connection</em> and (ii) <em>not using the skip
connection</em>. You should set the instance variable
<code>use_skip</code> to either <code>True</code> or <code>False</code>.
For your runs, report which case is better and if you confront
overfitting in training.</p>
<div style="text-align: right"> <span style="color:red">[6 points]</span> </div>
</section>
<div class="cell markdown" id="Zb8Va8Eo--xR">
<p><em># WRITE YOUR ANSWER AND OBSERVATION HERE</em></p>
<p>.....</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="d-NWK14w--xS" data-outputId="18638d07-5a6c-4216-93c3-b7ff1a74b2d0">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers, models</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>my_network_skip <span class="op">=</span> YourModel(name<span class="op">=</span><span class="st">&#39;network1&#39;</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>                     feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>                     num_classes<span class="op">=</span>data_manager.n_classes,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>                     num_blocks<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>                     drop_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>                     batch_norm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>                     use_skip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>                     optimizer<span class="op">=</span><span class="st">&#39;sgd&#39;</span>,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>                     learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>my_network_skip.build_cnn()</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>my_network_skip.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               
                                                                                                  
 conv2d_4 (Conv2D)              (None, 32, 32, 32)   896         [&#39;input_1[0][0]&#39;]                
                                                                                                  
 batch_normalization (BatchNorm  (None, 32, 32, 32)  128         [&#39;conv2d_4[0][0]&#39;]               
 alization)                                                                                       
                                                                                                  
 activation (Activation)        (None, 32, 32, 32)   0           [&#39;batch_normalization[0][0]&#39;]    
                                                                                                  
 conv2d_5 (Conv2D)              (None, 32, 32, 32)   9248        [&#39;activation[0][0]&#39;]             
                                                                                                  
 batch_normalization_1 (BatchNo  (None, 32, 32, 32)  128         [&#39;conv2d_5[0][0]&#39;]               
 rmalization)                                                                                     
                                                                                                  
 add (Add)                      (None, 32, 32, 32)   0           [&#39;activation[0][0]&#39;,             
                                                                  &#39;batch_normalization_1[0][0]&#39;]  
                                                                                                  
 activation_1 (Activation)      (None, 32, 32, 32)   0           [&#39;add[0][0]&#39;]                    
                                                                                                  
 average_pooling2d_2 (AveragePo  (None, 16, 16, 32)  0           [&#39;activation_1[0][0]&#39;]           
 oling2D)                                                                                         
                                                                                                  
 conv2d_6 (Conv2D)              (None, 16, 16, 64)   18496       [&#39;average_pooling2d_2[0][0]&#39;]    
                                                                                                  
 batch_normalization_2 (BatchNo  (None, 16, 16, 64)  256         [&#39;conv2d_6[0][0]&#39;]               
 rmalization)                                                                                     
                                                                                                  
 activation_2 (Activation)      (None, 16, 16, 64)   0           [&#39;batch_normalization_2[0][0]&#39;]  
                                                                                                  
 conv2d_7 (Conv2D)              (None, 16, 16, 64)   36928       [&#39;activation_2[0][0]&#39;]           
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 16, 16, 64)  256         [&#39;conv2d_7[0][0]&#39;]               
 rmalization)                                                                                     
                                                                                                  
 add_1 (Add)                    (None, 16, 16, 64)   0           [&#39;activation_2[0][0]&#39;,           
                                                                  &#39;batch_normalization_3[0][0]&#39;]  
                                                                                                  
 activation_3 (Activation)      (None, 16, 16, 64)   0           [&#39;add_1[0][0]&#39;]                  
                                                                                                  
 average_pooling2d_3 (AveragePo  (None, 8, 8, 64)    0           [&#39;activation_3[0][0]&#39;]           
 oling2D)                                                                                         
                                                                                                  
 conv2d_8 (Conv2D)              (None, 8, 8, 128)    73856       [&#39;average_pooling2d_3[0][0]&#39;]    
                                                                                                  
 batch_normalization_4 (BatchNo  (None, 8, 8, 128)   512         [&#39;conv2d_8[0][0]&#39;]               
 rmalization)                                                                                     
                                                                                                  
 activation_4 (Activation)      (None, 8, 8, 128)    0           [&#39;batch_normalization_4[0][0]&#39;]  
                                                                                                  
 conv2d_9 (Conv2D)              (None, 8, 8, 128)    147584      [&#39;activation_4[0][0]&#39;]           
                                                                                                  
 batch_normalization_5 (BatchNo  (None, 8, 8, 128)   512         [&#39;conv2d_9[0][0]&#39;]               
 rmalization)                                                                                     
                                                                                                  
 add_2 (Add)                    (None, 8, 8, 128)    0           [&#39;activation_4[0][0]&#39;,           
                                                                  &#39;batch_normalization_5[0][0]&#39;]  
                                                                                                  
 activation_5 (Activation)      (None, 8, 8, 128)    0           [&#39;add_2[0][0]&#39;]                  
                                                                                                  
 average_pooling2d_4 (AveragePo  (None, 4, 4, 128)   0           [&#39;activation_5[0][0]&#39;]           
 oling2D)                                                                                         
                                                                                                  
 flatten_1 (Flatten)            (None, 2048)         0           [&#39;average_pooling2d_4[0][0]&#39;]    
                                                                                                  
 dense_1 (Dense)                (None, 10)           20490       [&#39;flatten_1[0][0]&#39;]              
                                                                                                  
==================================================================================================
Total params: 309,290
Trainable params: 308,394
Non-trainable params: 896
__________________________________________________________________________________________________
None
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="_AkCd9gy--xT" data-outputId="c1c66c2c-9e99-4a07-85c9-90fe5f00c003">
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>my_network_skip.fit(x_train_batch, x_val_batch, num_epochs<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/20
141/141 [==============================] - 3s 9ms/step - loss: 1.8172 - accuracy: 0.3380 - val_loss: 2.8307 - val_accuracy: 0.1060
Epoch 2/20
141/141 [==============================] - 1s 9ms/step - loss: 1.3395 - accuracy: 0.5089 - val_loss: 3.2411 - val_accuracy: 0.1680
Epoch 3/20
141/141 [==============================] - 1s 7ms/step - loss: 1.1227 - accuracy: 0.5958 - val_loss: 2.2395 - val_accuracy: 0.2900
Epoch 4/20
141/141 [==============================] - 1s 7ms/step - loss: 0.9691 - accuracy: 0.6582 - val_loss: 1.5108 - val_accuracy: 0.4720
Epoch 5/20
141/141 [==============================] - 1s 7ms/step - loss: 0.8469 - accuracy: 0.7169 - val_loss: 1.3289 - val_accuracy: 0.5520
Epoch 6/20
141/141 [==============================] - 1s 7ms/step - loss: 0.7444 - accuracy: 0.7638 - val_loss: 1.3557 - val_accuracy: 0.5360
Epoch 7/20
141/141 [==============================] - 1s 7ms/step - loss: 0.6531 - accuracy: 0.8058 - val_loss: 1.3614 - val_accuracy: 0.5520
Epoch 8/20
141/141 [==============================] - 1s 7ms/step - loss: 0.5696 - accuracy: 0.8438 - val_loss: 1.4007 - val_accuracy: 0.5420
Epoch 9/20
141/141 [==============================] - 1s 7ms/step - loss: 0.4970 - accuracy: 0.8738 - val_loss: 1.4979 - val_accuracy: 0.5180
Epoch 10/20
141/141 [==============================] - 1s 6ms/step - loss: 0.4394 - accuracy: 0.8938 - val_loss: 1.6191 - val_accuracy: 0.5200
Epoch 11/20
141/141 [==============================] - 1s 8ms/step - loss: 0.3891 - accuracy: 0.9113 - val_loss: 2.0143 - val_accuracy: 0.4480
Epoch 12/20
141/141 [==============================] - 1s 10ms/step - loss: 0.3448 - accuracy: 0.9344 - val_loss: 2.4209 - val_accuracy: 0.4020
Epoch 13/20
141/141 [==============================] - 1s 7ms/step - loss: 0.3029 - accuracy: 0.9460 - val_loss: 2.4210 - val_accuracy: 0.4380
Epoch 14/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2607 - accuracy: 0.9649 - val_loss: 2.1427 - val_accuracy: 0.4680
Epoch 15/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2190 - accuracy: 0.9764 - val_loss: 1.7753 - val_accuracy: 0.4940
Epoch 16/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1865 - accuracy: 0.9871 - val_loss: 1.5216 - val_accuracy: 0.5520
Epoch 17/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1598 - accuracy: 0.9933 - val_loss: 1.4127 - val_accuracy: 0.5760
Epoch 18/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1373 - accuracy: 0.9964 - val_loss: 1.4383 - val_accuracy: 0.5740
Epoch 19/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1188 - accuracy: 0.9980 - val_loss: 1.4569 - val_accuracy: 0.5700
Epoch 20/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1036 - accuracy: 0.9989 - val_loss: 1.4557 - val_accuracy: 0.5740
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="bPwzN2eC--xV" data-outputId="9ddb97b7-78ef-4c9d-8014-73d748a0530e">
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>my_network_no_skip <span class="op">=</span> YourModel(name<span class="op">=</span><span class="st">&#39;network1&#39;</span>,</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                     feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>                     num_classes<span class="op">=</span>data_manager.n_classes,</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>                     num_blocks<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>                     drop_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>                     batch_norm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>                     use_skip<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>                     optimizer<span class="op">=</span><span class="st">&#39;sgd&#39;</span>,</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>                     learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>my_network_no_skip.build_cnn()</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>my_network_no_skip.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;model_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
 conv2d_10 (Conv2D)          (None, 32, 32, 32)        896       
                                                                 
 batch_normalization_6 (Batc  (None, 32, 32, 32)       128       
 hNormalization)                                                 
                                                                 
 activation_6 (Activation)   (None, 32, 32, 32)        0         
                                                                 
 conv2d_11 (Conv2D)          (None, 32, 32, 32)        9248      
                                                                 
 batch_normalization_7 (Batc  (None, 32, 32, 32)       128       
 hNormalization)                                                 
                                                                 
 activation_7 (Activation)   (None, 32, 32, 32)        0         
                                                                 
 average_pooling2d_5 (Averag  (None, 16, 16, 32)       0         
 ePooling2D)                                                     
                                                                 
 conv2d_12 (Conv2D)          (None, 16, 16, 64)        18496     
                                                                 
 batch_normalization_8 (Batc  (None, 16, 16, 64)       256       
 hNormalization)                                                 
                                                                 
 activation_8 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 conv2d_13 (Conv2D)          (None, 16, 16, 64)        36928     
                                                                 
 batch_normalization_9 (Batc  (None, 16, 16, 64)       256       
 hNormalization)                                                 
                                                                 
 activation_9 (Activation)   (None, 16, 16, 64)        0         
                                                                 
 average_pooling2d_6 (Averag  (None, 8, 8, 64)         0         
 ePooling2D)                                                     
                                                                 
 conv2d_14 (Conv2D)          (None, 8, 8, 128)         73856     
                                                                 
 batch_normalization_10 (Bat  (None, 8, 8, 128)        512       
 chNormalization)                                                
                                                                 
 activation_10 (Activation)  (None, 8, 8, 128)         0         
                                                                 
 conv2d_15 (Conv2D)          (None, 8, 8, 128)         147584    
                                                                 
 batch_normalization_11 (Bat  (None, 8, 8, 128)        512       
 chNormalization)                                                
                                                                 
 activation_11 (Activation)  (None, 8, 8, 128)         0         
                                                                 
 average_pooling2d_7 (Averag  (None, 4, 4, 128)        0         
 ePooling2D)                                                     
                                                                 
 flatten_2 (Flatten)         (None, 2048)              0         
                                                                 
 dense_2 (Dense)             (None, 10)                20490     
                                                                 
=================================================================
Total params: 309,290
Trainable params: 308,394
Non-trainable params: 896
_________________________________________________________________
None
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="q3I1C0MP--xW" data-outputId="0395e0b0-bf21-4546-e413-91f9de383606">
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>my_network_no_skip.fit(x_train_batch, x_val_batch, num_epochs<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/20
141/141 [==============================] - 3s 9ms/step - loss: 1.7145 - accuracy: 0.3591 - val_loss: 2.5450 - val_accuracy: 0.1120
Epoch 2/20
141/141 [==============================] - 1s 6ms/step - loss: 1.3194 - accuracy: 0.5147 - val_loss: 2.6301 - val_accuracy: 0.1600
Epoch 3/20
141/141 [==============================] - 1s 7ms/step - loss: 1.1367 - accuracy: 0.5996 - val_loss: 1.9719 - val_accuracy: 0.3020
Epoch 4/20
141/141 [==============================] - 1s 6ms/step - loss: 0.9927 - accuracy: 0.6609 - val_loss: 1.5810 - val_accuracy: 0.4420
Epoch 5/20
141/141 [==============================] - 1s 6ms/step - loss: 0.8759 - accuracy: 0.7138 - val_loss: 1.5433 - val_accuracy: 0.4580
Epoch 6/20
141/141 [==============================] - 1s 7ms/step - loss: 0.7781 - accuracy: 0.7636 - val_loss: 1.9402 - val_accuracy: 0.4120
Epoch 7/20
141/141 [==============================] - 1s 8ms/step - loss: 0.6990 - accuracy: 0.7980 - val_loss: 2.0749 - val_accuracy: 0.4020
Epoch 8/20
141/141 [==============================] - 1s 9ms/step - loss: 0.6278 - accuracy: 0.8309 - val_loss: 2.0775 - val_accuracy: 0.4140
Epoch 9/20
141/141 [==============================] - 1s 7ms/step - loss: 0.5581 - accuracy: 0.8598 - val_loss: 2.0063 - val_accuracy: 0.4400
Epoch 10/20
141/141 [==============================] - 1s 7ms/step - loss: 0.4959 - accuracy: 0.8862 - val_loss: 2.0783 - val_accuracy: 0.4420
Epoch 11/20
141/141 [==============================] - 1s 6ms/step - loss: 0.4391 - accuracy: 0.9091 - val_loss: 2.1168 - val_accuracy: 0.4320
Epoch 12/20
141/141 [==============================] - 1s 7ms/step - loss: 0.3874 - accuracy: 0.9307 - val_loss: 1.9595 - val_accuracy: 0.4360
Epoch 13/20
141/141 [==============================] - 1s 7ms/step - loss: 0.3393 - accuracy: 0.9467 - val_loss: 1.7357 - val_accuracy: 0.4820
Epoch 14/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2954 - accuracy: 0.9613 - val_loss: 1.6131 - val_accuracy: 0.5120
Epoch 15/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2531 - accuracy: 0.9744 - val_loss: 1.6467 - val_accuracy: 0.5040
Epoch 16/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2167 - accuracy: 0.9844 - val_loss: 1.5854 - val_accuracy: 0.5160
Epoch 17/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1854 - accuracy: 0.9918 - val_loss: 1.4717 - val_accuracy: 0.5200
Epoch 18/20
141/141 [==============================] - 1s 9ms/step - loss: 0.1580 - accuracy: 0.9951 - val_loss: 1.3932 - val_accuracy: 0.5460
Epoch 19/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1354 - accuracy: 0.9976 - val_loss: 1.3732 - val_accuracy: 0.5460
Epoch 20/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1174 - accuracy: 0.9984 - val_loss: 1.3709 - val_accuracy: 0.5680
</code></pre>
</div>
</div>
<div class="cell markdown" id="MTI7Ksn-Wm8h">
<p>The new model works apparently much much better. The originnal
accuracy was only around 0.422</p>
<p>with skip, the accuracy becomes 0.612 impressive without skip, the
accuracy becomes 0.564</p>
<p>It improves, but seems not much. And apparently, there is overfitting
issue as accuracy is much higher than the val_accuracy</p>
</div>
<section id="question-34-tune-hyperparameters-with-grid-search"
class="cell markdown" id="8iZ8Nhwb--xa">
<h3><span style="color:#0b486b">Question 3.4: Tune hyperparameters with
grid search</span></h3>
<p>Now, let us tune the <span
class="math inline"><em>n</em><em>u</em><em>m</em>_<em>b</em><em>l</em><em>o</em><em>c</em><em>k</em><em>s</em>â€„âˆˆâ€„{2,â€†3,â€†4}</span>,
<span
class="math inline"><em>u</em><em>s</em><em>e</em>_<em>s</em><em>k</em><em>i</em><em>p</em>â€„âˆˆâ€„{<em>T</em><em>r</em><em>u</em><em>e</em>,â€†<em>F</em><em>a</em><em>l</em><em>s</em><em>e</em>}</span>,
and <span
class="math inline"><em>l</em><em>e</em><em>a</em><em>r</em><em>n</em><em>i</em><em>n</em><em>g</em>_<em>r</em><em>a</em><em>t</em><em>e</em>â€„âˆˆâ€„{0.001,â€†0.0001}</span>.
Write your code for this tuning and report the result of the best model
on the testing set. Note that you need to show your code for tuning and
evaluating on the test set to earn the full marks. During tuning, you
can set the instance variable <code>verbose</code> of your model to
<code>False</code> for not showing the training details of each
epoch.</p>
<div style="text-align: right"> <span style="color:red">[4 points]</span> </div>
</section>
<div class="cell markdown" id="vegL5VXd--xb">
<p><em># REPORT THE BEST PARAMETERS AND THE TESTING ACCURACY
HERE</em></p>
<p>.....</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ynitF-bk--xb" data-outputId="9eb3c279-f159-4f5b-e0ad-3ec1d357f927"
data-tags="[]">
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You can add more cells if necessary</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>best_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> {}</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co">#set up different parameters for number_blocks, skip, and learning rate</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> blocks <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]:</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> skip <span class="kw">in</span> [<span class="va">True</span>, <span class="va">False</span>]:</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> lr <span class="kw">in</span> [<span class="fl">0.001</span>, <span class="fl">0.0001</span>]:</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Training model with num_blocks=</span><span class="sc">{</span>blocks<span class="sc">}</span><span class="ss">, use_skip=</span><span class="sc">{</span>skip<span class="sc">}</span><span class="ss">, and learning_rate=</span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>            model_3_4<span class="op">=</span>YourModel(name<span class="op">=</span><span class="st">&#39;q3_4&#39;</span>,</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>                     feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>                     num_classes<span class="op">=</span>data_manager.n_classes,</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>                     num_blocks<span class="op">=</span>blocks,</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>                     drop_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>                     batch_norm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>                     use_skip<span class="op">=</span>skip,</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>                     optimizer<span class="op">=</span><span class="st">&#39;sgd&#39;</span>,</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>                     learning_rate<span class="op">=</span>lr)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>            model_3_4.build_cnn()</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>            model_3_4.fit(x_train_batch, x_val_batch,num_epochs<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>            val_accuracy <span class="op">=</span> <span class="bu">max</span>(model_3_4.history.history[<span class="st">&#39;val_accuracy&#39;</span>])</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> val_accuracy <span class="op">&gt;</span> best_accuracy:</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>                best_accuracy <span class="op">=</span> val_accuracy</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>                best_model <span class="op">=</span> model_3_4</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>                best_params <span class="op">=</span> {</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;num_blocks&quot;</span>: blocks,</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;use_skip&quot;</span>: skip,</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;learning_rate&quot;</span>: lr</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Best validation accuracy is </span><span class="sc">{</span>best_accuracy<span class="sc">}</span><span class="ss"> with params: </span><span class="sc">{</span>best_params<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Training model with num_blocks=2, use_skip=True, and learning_rate=0.001
Epoch 1/20
141/141 [==============================] - 3s 8ms/step - loss: 1.9165 - accuracy: 0.3236 - val_loss: 2.7488 - val_accuracy: 0.0920
Epoch 2/20
141/141 [==============================] - 1s 6ms/step - loss: 1.3802 - accuracy: 0.4924 - val_loss: 2.7620 - val_accuracy: 0.1980
Epoch 3/20
141/141 [==============================] - 1s 5ms/step - loss: 1.1641 - accuracy: 0.5789 - val_loss: 2.1526 - val_accuracy: 0.3280
Epoch 4/20
141/141 [==============================] - 1s 5ms/step - loss: 1.0179 - accuracy: 0.6398 - val_loss: 1.7824 - val_accuracy: 0.4180
Epoch 5/20
141/141 [==============================] - 1s 5ms/step - loss: 0.9008 - accuracy: 0.6876 - val_loss: 1.6467 - val_accuracy: 0.4600
Epoch 6/20
141/141 [==============================] - 1s 5ms/step - loss: 0.8078 - accuracy: 0.7273 - val_loss: 1.7132 - val_accuracy: 0.4660
Epoch 7/20
141/141 [==============================] - 1s 5ms/step - loss: 0.7297 - accuracy: 0.7631 - val_loss: 1.9207 - val_accuracy: 0.4160
Epoch 8/20
141/141 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.7907 - val_loss: 1.9553 - val_accuracy: 0.4280
Epoch 9/20
141/141 [==============================] - 1s 5ms/step - loss: 0.5889 - accuracy: 0.8233 - val_loss: 2.1519 - val_accuracy: 0.4160
Epoch 10/20
141/141 [==============================] - 1s 6ms/step - loss: 0.5281 - accuracy: 0.8460 - val_loss: 1.8633 - val_accuracy: 0.4640
Epoch 11/20
141/141 [==============================] - 1s 7ms/step - loss: 0.4728 - accuracy: 0.8691 - val_loss: 1.7808 - val_accuracy: 0.4920
Epoch 12/20
141/141 [==============================] - 1s 7ms/step - loss: 0.4238 - accuracy: 0.8920 - val_loss: 1.9491 - val_accuracy: 0.4660
Epoch 13/20
141/141 [==============================] - 1s 5ms/step - loss: 0.3834 - accuracy: 0.9131 - val_loss: 2.0269 - val_accuracy: 0.4840
Epoch 14/20
141/141 [==============================] - 1s 5ms/step - loss: 0.3549 - accuracy: 0.9233 - val_loss: 1.8934 - val_accuracy: 0.4880
Epoch 15/20
141/141 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.9322 - val_loss: 1.7375 - val_accuracy: 0.5140
Epoch 16/20
141/141 [==============================] - 1s 6ms/step - loss: 0.3065 - accuracy: 0.9407 - val_loss: 1.6873 - val_accuracy: 0.5260
Epoch 17/20
141/141 [==============================] - 1s 5ms/step - loss: 0.2790 - accuracy: 0.9480 - val_loss: 1.7562 - val_accuracy: 0.5060
Epoch 18/20
141/141 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.9569 - val_loss: 1.7542 - val_accuracy: 0.5100
Epoch 19/20
141/141 [==============================] - 1s 5ms/step - loss: 0.2329 - accuracy: 0.9649 - val_loss: 1.6554 - val_accuracy: 0.5280
Epoch 20/20
141/141 [==============================] - 1s 6ms/step - loss: 0.2136 - accuracy: 0.9729 - val_loss: 1.6399 - val_accuracy: 0.5220
Training model with num_blocks=2, use_skip=True, and learning_rate=0.0001
Epoch 1/20
141/141 [==============================] - 2s 8ms/step - loss: 2.1670 - accuracy: 0.2229 - val_loss: 2.3032 - val_accuracy: 0.1140
Epoch 2/20
141/141 [==============================] - 1s 6ms/step - loss: 1.8024 - accuracy: 0.3469 - val_loss: 2.3262 - val_accuracy: 0.1840
Epoch 3/20
141/141 [==============================] - 1s 6ms/step - loss: 1.6595 - accuracy: 0.3978 - val_loss: 1.9562 - val_accuracy: 0.3120
Epoch 4/20
141/141 [==============================] - 1s 5ms/step - loss: 1.5599 - accuracy: 0.4360 - val_loss: 1.6757 - val_accuracy: 0.3920
Epoch 5/20
141/141 [==============================] - 1s 5ms/step - loss: 1.4812 - accuracy: 0.4640 - val_loss: 1.5819 - val_accuracy: 0.4080
Epoch 6/20
141/141 [==============================] - 1s 5ms/step - loss: 1.4160 - accuracy: 0.4893 - val_loss: 1.5388 - val_accuracy: 0.4320
Epoch 7/20
141/141 [==============================] - 1s 5ms/step - loss: 1.3604 - accuracy: 0.5122 - val_loss: 1.5037 - val_accuracy: 0.4620
Epoch 8/20
141/141 [==============================] - 1s 5ms/step - loss: 1.3117 - accuracy: 0.5329 - val_loss: 1.4731 - val_accuracy: 0.4700
Epoch 9/20
141/141 [==============================] - 1s 5ms/step - loss: 1.2681 - accuracy: 0.5471 - val_loss: 1.4462 - val_accuracy: 0.4820
Epoch 10/20
141/141 [==============================] - 1s 5ms/step - loss: 1.2287 - accuracy: 0.5616 - val_loss: 1.4231 - val_accuracy: 0.4880
Epoch 11/20
141/141 [==============================] - 1s 6ms/step - loss: 1.1928 - accuracy: 0.5796 - val_loss: 1.4003 - val_accuracy: 0.4920
Epoch 12/20
141/141 [==============================] - 1s 8ms/step - loss: 1.1597 - accuracy: 0.5949 - val_loss: 1.3824 - val_accuracy: 0.5120
Epoch 13/20
141/141 [==============================] - 1s 6ms/step - loss: 1.1290 - accuracy: 0.6102 - val_loss: 1.3665 - val_accuracy: 0.5240
Epoch 14/20
141/141 [==============================] - 1s 6ms/step - loss: 1.1004 - accuracy: 0.6220 - val_loss: 1.3530 - val_accuracy: 0.5240
Epoch 15/20
141/141 [==============================] - 1s 5ms/step - loss: 1.0735 - accuracy: 0.6313 - val_loss: 1.3416 - val_accuracy: 0.5260
Epoch 16/20
141/141 [==============================] - 1s 5ms/step - loss: 1.0480 - accuracy: 0.6391 - val_loss: 1.3307 - val_accuracy: 0.5260
Epoch 17/20
141/141 [==============================] - 1s 6ms/step - loss: 1.0237 - accuracy: 0.6489 - val_loss: 1.3217 - val_accuracy: 0.5340
Epoch 18/20
141/141 [==============================] - 1s 6ms/step - loss: 1.0007 - accuracy: 0.6618 - val_loss: 1.3138 - val_accuracy: 0.5400
Epoch 19/20
141/141 [==============================] - 1s 5ms/step - loss: 0.9787 - accuracy: 0.6702 - val_loss: 1.3069 - val_accuracy: 0.5400
Epoch 20/20
141/141 [==============================] - 1s 5ms/step - loss: 0.9578 - accuracy: 0.6791 - val_loss: 1.2993 - val_accuracy: 0.5440
Training model with num_blocks=2, use_skip=False, and learning_rate=0.001
Epoch 1/20
141/141 [==============================] - 2s 7ms/step - loss: 1.7959 - accuracy: 0.3511 - val_loss: 2.4202 - val_accuracy: 0.1260
Epoch 2/20
141/141 [==============================] - 1s 8ms/step - loss: 1.3637 - accuracy: 0.5091 - val_loss: 2.2538 - val_accuracy: 0.2160
Epoch 3/20
141/141 [==============================] - 1s 6ms/step - loss: 1.1944 - accuracy: 0.5742 - val_loss: 1.6436 - val_accuracy: 0.3920
Epoch 4/20
141/141 [==============================] - 1s 5ms/step - loss: 1.0677 - accuracy: 0.6256 - val_loss: 1.4147 - val_accuracy: 0.4900
Epoch 5/20
141/141 [==============================] - 1s 6ms/step - loss: 0.9632 - accuracy: 0.6787 - val_loss: 1.3391 - val_accuracy: 0.5260
Epoch 6/20
141/141 [==============================] - 1s 5ms/step - loss: 0.8723 - accuracy: 0.7136 - val_loss: 1.3621 - val_accuracy: 0.5080
Epoch 7/20
141/141 [==============================] - 1s 5ms/step - loss: 0.7920 - accuracy: 0.7524 - val_loss: 1.4365 - val_accuracy: 0.4960
Epoch 8/20
141/141 [==============================] - 1s 5ms/step - loss: 0.7195 - accuracy: 0.7882 - val_loss: 1.4109 - val_accuracy: 0.5100
Epoch 9/20
141/141 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.8140 - val_loss: 1.4271 - val_accuracy: 0.5020
Epoch 10/20
141/141 [==============================] - 1s 6ms/step - loss: 0.5962 - accuracy: 0.8369 - val_loss: 1.4128 - val_accuracy: 0.5040
Epoch 11/20
141/141 [==============================] - 1s 5ms/step - loss: 0.5429 - accuracy: 0.8607 - val_loss: 1.3887 - val_accuracy: 0.5160
Epoch 12/20
141/141 [==============================] - 1s 6ms/step - loss: 0.4942 - accuracy: 0.8851 - val_loss: 1.3566 - val_accuracy: 0.5360
Epoch 13/20
141/141 [==============================] - 1s 7ms/step - loss: 0.4492 - accuracy: 0.9027 - val_loss: 1.3395 - val_accuracy: 0.5480
Epoch 14/20
141/141 [==============================] - 1s 6ms/step - loss: 0.4090 - accuracy: 0.9191 - val_loss: 1.3364 - val_accuracy: 0.5620
Epoch 15/20
141/141 [==============================] - 1s 5ms/step - loss: 0.3740 - accuracy: 0.9309 - val_loss: 1.3676 - val_accuracy: 0.5600
Epoch 16/20
141/141 [==============================] - 1s 5ms/step - loss: 0.3403 - accuracy: 0.9449 - val_loss: 1.3567 - val_accuracy: 0.5540
Epoch 17/20
141/141 [==============================] - 1s 5ms/step - loss: 0.3129 - accuracy: 0.9540 - val_loss: 1.3934 - val_accuracy: 0.5400
Epoch 18/20
141/141 [==============================] - 1s 6ms/step - loss: 0.2879 - accuracy: 0.9613 - val_loss: 1.4009 - val_accuracy: 0.5380
Epoch 19/20
141/141 [==============================] - 1s 5ms/step - loss: 0.2654 - accuracy: 0.9709 - val_loss: 1.4073 - val_accuracy: 0.5500
Epoch 20/20
141/141 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9762 - val_loss: 1.3865 - val_accuracy: 0.5580
Training model with num_blocks=2, use_skip=False, and learning_rate=0.0001
Epoch 1/20
141/141 [==============================] - 2s 7ms/step - loss: 2.2027 - accuracy: 0.2073 - val_loss: 2.3339 - val_accuracy: 0.0960
Epoch 2/20
141/141 [==============================] - 1s 7ms/step - loss: 1.8676 - accuracy: 0.3216 - val_loss: 2.2317 - val_accuracy: 0.1620
Epoch 3/20
141/141 [==============================] - 1s 6ms/step - loss: 1.7387 - accuracy: 0.3762 - val_loss: 1.9520 - val_accuracy: 0.2960
Epoch 4/20
141/141 [==============================] - 1s 5ms/step - loss: 1.6540 - accuracy: 0.4116 - val_loss: 1.7425 - val_accuracy: 0.3540
Epoch 5/20
141/141 [==============================] - 1s 6ms/step - loss: 1.5878 - accuracy: 0.4349 - val_loss: 1.6662 - val_accuracy: 0.3840
Epoch 6/20
141/141 [==============================] - 1s 6ms/step - loss: 1.5322 - accuracy: 0.4582 - val_loss: 1.6237 - val_accuracy: 0.4100
Epoch 7/20
141/141 [==============================] - 1s 5ms/step - loss: 1.4841 - accuracy: 0.4769 - val_loss: 1.5901 - val_accuracy: 0.4240
Epoch 8/20
141/141 [==============================] - 1s 5ms/step - loss: 1.4418 - accuracy: 0.4936 - val_loss: 1.5582 - val_accuracy: 0.4240
Epoch 9/20
141/141 [==============================] - 1s 5ms/step - loss: 1.4034 - accuracy: 0.5113 - val_loss: 1.5300 - val_accuracy: 0.4400
Epoch 10/20
141/141 [==============================] - 1s 5ms/step - loss: 1.3680 - accuracy: 0.5251 - val_loss: 1.5036 - val_accuracy: 0.4500
Epoch 11/20
141/141 [==============================] - 1s 6ms/step - loss: 1.3355 - accuracy: 0.5402 - val_loss: 1.4833 - val_accuracy: 0.4540
Epoch 12/20
141/141 [==============================] - 1s 5ms/step - loss: 1.3054 - accuracy: 0.5549 - val_loss: 1.4634 - val_accuracy: 0.4640
Epoch 13/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2772 - accuracy: 0.5622 - val_loss: 1.4443 - val_accuracy: 0.4700
Epoch 14/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2508 - accuracy: 0.5718 - val_loss: 1.4279 - val_accuracy: 0.4780
Epoch 15/20
141/141 [==============================] - 1s 5ms/step - loss: 1.2258 - accuracy: 0.5809 - val_loss: 1.4139 - val_accuracy: 0.4860
Epoch 16/20
141/141 [==============================] - 1s 6ms/step - loss: 1.2022 - accuracy: 0.5924 - val_loss: 1.4026 - val_accuracy: 0.4920
Epoch 17/20
141/141 [==============================] - 1s 5ms/step - loss: 1.1799 - accuracy: 0.6020 - val_loss: 1.3903 - val_accuracy: 0.5040
Epoch 18/20
141/141 [==============================] - 1s 5ms/step - loss: 1.1586 - accuracy: 0.6102 - val_loss: 1.3763 - val_accuracy: 0.5040
Epoch 19/20
141/141 [==============================] - 1s 5ms/step - loss: 1.1381 - accuracy: 0.6187 - val_loss: 1.3666 - val_accuracy: 0.5140
Epoch 20/20
141/141 [==============================] - 1s 5ms/step - loss: 1.1185 - accuracy: 0.6267 - val_loss: 1.3551 - val_accuracy: 0.5120
Training model with num_blocks=3, use_skip=True, and learning_rate=0.001
Epoch 1/20
141/141 [==============================] - 3s 8ms/step - loss: 1.7995 - accuracy: 0.3520 - val_loss: 2.7965 - val_accuracy: 0.1380
Epoch 2/20
141/141 [==============================] - 1s 7ms/step - loss: 1.3253 - accuracy: 0.5176 - val_loss: 3.0005 - val_accuracy: 0.2200
Epoch 3/20
141/141 [==============================] - 1s 8ms/step - loss: 1.1307 - accuracy: 0.5900 - val_loss: 2.1731 - val_accuracy: 0.3180
Epoch 4/20
141/141 [==============================] - 1s 9ms/step - loss: 0.9842 - accuracy: 0.6544 - val_loss: 1.5053 - val_accuracy: 0.4600
Epoch 5/20
141/141 [==============================] - 1s 7ms/step - loss: 0.8627 - accuracy: 0.7124 - val_loss: 1.3924 - val_accuracy: 0.4860
Epoch 6/20
141/141 [==============================] - 1s 7ms/step - loss: 0.7649 - accuracy: 0.7527 - val_loss: 1.5152 - val_accuracy: 0.4720
Epoch 7/20
141/141 [==============================] - 1s 7ms/step - loss: 0.6734 - accuracy: 0.7929 - val_loss: 1.6826 - val_accuracy: 0.4540
Epoch 8/20
141/141 [==============================] - 1s 7ms/step - loss: 0.5861 - accuracy: 0.8293 - val_loss: 1.6894 - val_accuracy: 0.4420
Epoch 9/20
141/141 [==============================] - 1s 7ms/step - loss: 0.5076 - accuracy: 0.8687 - val_loss: 1.5566 - val_accuracy: 0.4820
Epoch 10/20
141/141 [==============================] - 1s 7ms/step - loss: 0.4415 - accuracy: 0.8998 - val_loss: 1.5980 - val_accuracy: 0.4820
Epoch 11/20
141/141 [==============================] - 1s 7ms/step - loss: 0.3833 - accuracy: 0.9176 - val_loss: 1.5905 - val_accuracy: 0.4900
Epoch 12/20
141/141 [==============================] - 1s 7ms/step - loss: 0.3310 - accuracy: 0.9424 - val_loss: 1.4653 - val_accuracy: 0.5420
Epoch 13/20
141/141 [==============================] - 1s 9ms/step - loss: 0.2898 - accuracy: 0.9573 - val_loss: 1.4417 - val_accuracy: 0.5500
Epoch 14/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2537 - accuracy: 0.9678 - val_loss: 1.4241 - val_accuracy: 0.5500
Epoch 15/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2234 - accuracy: 0.9760 - val_loss: 1.5265 - val_accuracy: 0.5420
Epoch 16/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9822 - val_loss: 1.6417 - val_accuracy: 0.5260
Epoch 17/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1692 - accuracy: 0.9878 - val_loss: 1.6998 - val_accuracy: 0.5100
Epoch 18/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1464 - accuracy: 0.9936 - val_loss: 1.4843 - val_accuracy: 0.5500
Epoch 19/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1236 - accuracy: 0.9969 - val_loss: 1.3803 - val_accuracy: 0.5860
Epoch 20/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1045 - accuracy: 0.9980 - val_loss: 1.3364 - val_accuracy: 0.6160
Training model with num_blocks=3, use_skip=True, and learning_rate=0.0001
Epoch 1/20
141/141 [==============================] - 4s 13ms/step - loss: 2.2147 - accuracy: 0.2167 - val_loss: 2.3670 - val_accuracy: 0.1000
Epoch 2/20
141/141 [==============================] - 1s 7ms/step - loss: 1.7773 - accuracy: 0.3569 - val_loss: 2.3118 - val_accuracy: 0.1840
Epoch 3/20
141/141 [==============================] - 1s 7ms/step - loss: 1.6247 - accuracy: 0.4164 - val_loss: 1.9523 - val_accuracy: 0.2760
Epoch 4/20
141/141 [==============================] - 1s 7ms/step - loss: 1.5251 - accuracy: 0.4567 - val_loss: 1.6289 - val_accuracy: 0.3940
Epoch 5/20
141/141 [==============================] - 1s 7ms/step - loss: 1.4491 - accuracy: 0.4842 - val_loss: 1.5233 - val_accuracy: 0.4380
Epoch 6/20
141/141 [==============================] - 1s 7ms/step - loss: 1.3862 - accuracy: 0.5062 - val_loss: 1.4841 - val_accuracy: 0.4440
Epoch 7/20
141/141 [==============================] - 1s 7ms/step - loss: 1.3328 - accuracy: 0.5289 - val_loss: 1.4552 - val_accuracy: 0.4420
Epoch 8/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2861 - accuracy: 0.5473 - val_loss: 1.4340 - val_accuracy: 0.4440
Epoch 9/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2443 - accuracy: 0.5618 - val_loss: 1.4127 - val_accuracy: 0.4740
Epoch 10/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2066 - accuracy: 0.5747 - val_loss: 1.3934 - val_accuracy: 0.4780
Epoch 11/20
141/141 [==============================] - 1s 10ms/step - loss: 1.1719 - accuracy: 0.5896 - val_loss: 1.3751 - val_accuracy: 0.4820
Epoch 12/20
141/141 [==============================] - 1s 7ms/step - loss: 1.1399 - accuracy: 0.6062 - val_loss: 1.3582 - val_accuracy: 0.4880
Epoch 13/20
141/141 [==============================] - 1s 7ms/step - loss: 1.1103 - accuracy: 0.6189 - val_loss: 1.3473 - val_accuracy: 0.4900
Epoch 14/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0821 - accuracy: 0.6353 - val_loss: 1.3325 - val_accuracy: 0.4960
Epoch 15/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0557 - accuracy: 0.6473 - val_loss: 1.3231 - val_accuracy: 0.5040
Epoch 16/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0307 - accuracy: 0.6578 - val_loss: 1.3144 - val_accuracy: 0.5000
Epoch 17/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0068 - accuracy: 0.6702 - val_loss: 1.3044 - val_accuracy: 0.5140
Epoch 18/20
141/141 [==============================] - 1s 7ms/step - loss: 0.9839 - accuracy: 0.6796 - val_loss: 1.2964 - val_accuracy: 0.5140
Epoch 19/20
141/141 [==============================] - 1s 7ms/step - loss: 0.9617 - accuracy: 0.6900 - val_loss: 1.2892 - val_accuracy: 0.5220
Epoch 20/20
141/141 [==============================] - 1s 9ms/step - loss: 0.9404 - accuracy: 0.7013 - val_loss: 1.2806 - val_accuracy: 0.5240
Training model with num_blocks=3, use_skip=False, and learning_rate=0.001
Epoch 1/20
141/141 [==============================] - 3s 9ms/step - loss: 1.7919 - accuracy: 0.3500 - val_loss: 2.6187 - val_accuracy: 0.1320
Epoch 2/20
141/141 [==============================] - 1s 7ms/step - loss: 1.3576 - accuracy: 0.5151 - val_loss: 2.5274 - val_accuracy: 0.1800
Epoch 3/20
141/141 [==============================] - 1s 7ms/step - loss: 1.1632 - accuracy: 0.5942 - val_loss: 1.7175 - val_accuracy: 0.3780
Epoch 4/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0162 - accuracy: 0.6573 - val_loss: 1.4810 - val_accuracy: 0.4860
Epoch 5/20
141/141 [==============================] - 1s 7ms/step - loss: 0.8937 - accuracy: 0.7164 - val_loss: 1.6572 - val_accuracy: 0.4440
Epoch 6/20
141/141 [==============================] - 1s 9ms/step - loss: 0.7865 - accuracy: 0.7638 - val_loss: 1.6164 - val_accuracy: 0.4520
Epoch 7/20
141/141 [==============================] - 1s 9ms/step - loss: 0.6920 - accuracy: 0.8033 - val_loss: 1.5467 - val_accuracy: 0.4760
Epoch 8/20
141/141 [==============================] - 1s 9ms/step - loss: 0.6186 - accuracy: 0.8382 - val_loss: 1.6237 - val_accuracy: 0.4760
Epoch 9/20
141/141 [==============================] - 1s 7ms/step - loss: 0.5561 - accuracy: 0.8609 - val_loss: 1.4653 - val_accuracy: 0.4760
Epoch 10/20
141/141 [==============================] - 1s 7ms/step - loss: 0.5024 - accuracy: 0.8802 - val_loss: 1.5936 - val_accuracy: 0.4720
Epoch 11/20
141/141 [==============================] - 1s 7ms/step - loss: 0.4391 - accuracy: 0.9107 - val_loss: 1.5274 - val_accuracy: 0.4860
Epoch 12/20
141/141 [==============================] - 1s 7ms/step - loss: 0.3817 - accuracy: 0.9327 - val_loss: 1.3972 - val_accuracy: 0.5040
Epoch 13/20
141/141 [==============================] - 1s 7ms/step - loss: 0.3332 - accuracy: 0.9489 - val_loss: 1.3769 - val_accuracy: 0.5280
Epoch 14/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2918 - accuracy: 0.9633 - val_loss: 1.4067 - val_accuracy: 0.5020
Epoch 15/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2529 - accuracy: 0.9769 - val_loss: 1.5056 - val_accuracy: 0.4800
Epoch 16/20
141/141 [==============================] - 1s 7ms/step - loss: 0.2209 - accuracy: 0.9842 - val_loss: 1.6467 - val_accuracy: 0.4760
Epoch 17/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1952 - accuracy: 0.9900 - val_loss: 1.6863 - val_accuracy: 0.4640
Epoch 18/20
141/141 [==============================] - 1s 9ms/step - loss: 0.1732 - accuracy: 0.9927 - val_loss: 1.5936 - val_accuracy: 0.4860
Epoch 19/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1512 - accuracy: 0.9964 - val_loss: 1.5589 - val_accuracy: 0.4860
Epoch 20/20
141/141 [==============================] - 1s 7ms/step - loss: 0.1317 - accuracy: 0.9982 - val_loss: 1.5157 - val_accuracy: 0.5160
Training model with num_blocks=3, use_skip=False, and learning_rate=0.0001
Epoch 1/20
141/141 [==============================] - 3s 9ms/step - loss: 2.1465 - accuracy: 0.2236 - val_loss: 2.3016 - val_accuracy: 0.1040
Epoch 2/20
141/141 [==============================] - 1s 7ms/step - loss: 1.8046 - accuracy: 0.3469 - val_loss: 2.2845 - val_accuracy: 0.1660
Epoch 3/20
141/141 [==============================] - 1s 7ms/step - loss: 1.6620 - accuracy: 0.3976 - val_loss: 1.9542 - val_accuracy: 0.2960
Epoch 4/20
141/141 [==============================] - 1s 7ms/step - loss: 1.5682 - accuracy: 0.4367 - val_loss: 1.6419 - val_accuracy: 0.4020
Epoch 5/20
141/141 [==============================] - 1s 9ms/step - loss: 1.4970 - accuracy: 0.4664 - val_loss: 1.5449 - val_accuracy: 0.4400
Epoch 6/20
141/141 [==============================] - 1s 10ms/step - loss: 1.4399 - accuracy: 0.4949 - val_loss: 1.4979 - val_accuracy: 0.4600
Epoch 7/20
141/141 [==============================] - 1s 7ms/step - loss: 1.3920 - accuracy: 0.5160 - val_loss: 1.4633 - val_accuracy: 0.4740
Epoch 8/20
141/141 [==============================] - 1s 7ms/step - loss: 1.3503 - accuracy: 0.5358 - val_loss: 1.4373 - val_accuracy: 0.4800
Epoch 9/20
141/141 [==============================] - 1s 7ms/step - loss: 1.3128 - accuracy: 0.5533 - val_loss: 1.4142 - val_accuracy: 0.4880
Epoch 10/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2784 - accuracy: 0.5702 - val_loss: 1.3931 - val_accuracy: 0.5000
Epoch 11/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2467 - accuracy: 0.5796 - val_loss: 1.3750 - val_accuracy: 0.5020
Epoch 12/20
141/141 [==============================] - 1s 7ms/step - loss: 1.2171 - accuracy: 0.5944 - val_loss: 1.3591 - val_accuracy: 0.5040
Epoch 13/20
141/141 [==============================] - 1s 7ms/step - loss: 1.1890 - accuracy: 0.6016 - val_loss: 1.3464 - val_accuracy: 0.5120
Epoch 14/20
141/141 [==============================] - 1s 8ms/step - loss: 1.1625 - accuracy: 0.6118 - val_loss: 1.3360 - val_accuracy: 0.5160
Epoch 15/20
141/141 [==============================] - 1s 9ms/step - loss: 1.1370 - accuracy: 0.6256 - val_loss: 1.3234 - val_accuracy: 0.5200
Epoch 16/20
141/141 [==============================] - 1s 7ms/step - loss: 1.1127 - accuracy: 0.6371 - val_loss: 1.3119 - val_accuracy: 0.5200
Epoch 17/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0893 - accuracy: 0.6502 - val_loss: 1.3022 - val_accuracy: 0.5200
Epoch 18/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0668 - accuracy: 0.6624 - val_loss: 1.2934 - val_accuracy: 0.5260
Epoch 19/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0448 - accuracy: 0.6740 - val_loss: 1.2837 - val_accuracy: 0.5280
Epoch 20/20
141/141 [==============================] - 1s 7ms/step - loss: 1.0234 - accuracy: 0.6851 - val_loss: 1.2750 - val_accuracy: 0.5320
Training model with num_blocks=4, use_skip=True, and learning_rate=0.001
Epoch 1/20
141/141 [==============================] - 5s 15ms/step - loss: 1.8263 - accuracy: 0.3364 - val_loss: 2.8242 - val_accuracy: 0.1120
Epoch 2/20
141/141 [==============================] - 1s 9ms/step - loss: 1.3530 - accuracy: 0.5000 - val_loss: 3.0550 - val_accuracy: 0.1300
Epoch 3/20
141/141 [==============================] - 1s 8ms/step - loss: 1.1058 - accuracy: 0.6002 - val_loss: 2.0405 - val_accuracy: 0.3000
Epoch 4/20
141/141 [==============================] - 1s 9ms/step - loss: 0.9042 - accuracy: 0.6938 - val_loss: 1.6576 - val_accuracy: 0.4500
Epoch 5/20
141/141 [==============================] - 1s 9ms/step - loss: 0.7417 - accuracy: 0.7649 - val_loss: 1.5670 - val_accuracy: 0.4640
Epoch 6/20
141/141 [==============================] - 1s 8ms/step - loss: 0.6125 - accuracy: 0.8229 - val_loss: 1.5563 - val_accuracy: 0.4700
Epoch 7/20
141/141 [==============================] - 1s 8ms/step - loss: 0.5179 - accuracy: 0.8671 - val_loss: 2.4837 - val_accuracy: 0.3900
Epoch 8/20
141/141 [==============================] - 1s 9ms/step - loss: 0.4439 - accuracy: 0.8922 - val_loss: 3.8181 - val_accuracy: 0.3420
Epoch 9/20
141/141 [==============================] - 1s 8ms/step - loss: 0.3611 - accuracy: 0.9247 - val_loss: 3.1617 - val_accuracy: 0.3700
Epoch 10/20
141/141 [==============================] - 1s 8ms/step - loss: 0.2806 - accuracy: 0.9524 - val_loss: 2.5320 - val_accuracy: 0.4100
Epoch 11/20
141/141 [==============================] - 1s 8ms/step - loss: 0.2236 - accuracy: 0.9689 - val_loss: 1.9231 - val_accuracy: 0.4640
Epoch 12/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1874 - accuracy: 0.9756 - val_loss: 1.7596 - val_accuracy: 0.5040
Epoch 13/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1607 - accuracy: 0.9793 - val_loss: 1.7542 - val_accuracy: 0.4820
Epoch 14/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1397 - accuracy: 0.9847 - val_loss: 2.0238 - val_accuracy: 0.4840
Epoch 15/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1143 - accuracy: 0.9880 - val_loss: 1.7279 - val_accuracy: 0.5160
Epoch 16/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0941 - accuracy: 0.9909 - val_loss: 1.7014 - val_accuracy: 0.5300
Epoch 17/20
141/141 [==============================] - 1s 10ms/step - loss: 0.0737 - accuracy: 0.9949 - val_loss: 1.5870 - val_accuracy: 0.5700
Epoch 18/20
141/141 [==============================] - 1s 9ms/step - loss: 0.0539 - accuracy: 0.9971 - val_loss: 1.4798 - val_accuracy: 0.5960
Epoch 19/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0398 - accuracy: 0.9991 - val_loss: 1.5578 - val_accuracy: 0.5820
Epoch 20/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0286 - accuracy: 0.9993 - val_loss: 1.6076 - val_accuracy: 0.5780
Training model with num_blocks=4, use_skip=True, and learning_rate=0.0001
Epoch 1/20
141/141 [==============================] - 4s 10ms/step - loss: 2.0299 - accuracy: 0.2631 - val_loss: 2.4121 - val_accuracy: 0.1100
Epoch 2/20
141/141 [==============================] - 1s 8ms/step - loss: 1.6729 - accuracy: 0.3880 - val_loss: 2.5415 - val_accuracy: 0.1120
Epoch 3/20
141/141 [==============================] - 1s 10ms/step - loss: 1.5266 - accuracy: 0.4462 - val_loss: 1.9680 - val_accuracy: 0.2860
Epoch 4/20
141/141 [==============================] - 1s 10ms/step - loss: 1.4239 - accuracy: 0.4851 - val_loss: 1.5731 - val_accuracy: 0.4340
Epoch 5/20
141/141 [==============================] - 1s 8ms/step - loss: 1.3428 - accuracy: 0.5171 - val_loss: 1.4764 - val_accuracy: 0.4540
Epoch 6/20
141/141 [==============================] - 1s 8ms/step - loss: 1.2749 - accuracy: 0.5538 - val_loss: 1.4382 - val_accuracy: 0.4760
Epoch 7/20
141/141 [==============================] - 1s 8ms/step - loss: 1.2159 - accuracy: 0.5800 - val_loss: 1.4089 - val_accuracy: 0.4940
Epoch 8/20
141/141 [==============================] - 1s 9ms/step - loss: 1.1629 - accuracy: 0.5991 - val_loss: 1.3883 - val_accuracy: 0.5000
Epoch 9/20
141/141 [==============================] - 1s 8ms/step - loss: 1.1132 - accuracy: 0.6204 - val_loss: 1.3682 - val_accuracy: 0.5100
Epoch 10/20
141/141 [==============================] - 1s 8ms/step - loss: 1.0674 - accuracy: 0.6373 - val_loss: 1.3468 - val_accuracy: 0.5220
Epoch 11/20
141/141 [==============================] - 1s 8ms/step - loss: 1.0244 - accuracy: 0.6587 - val_loss: 1.3303 - val_accuracy: 0.5220
Epoch 12/20
141/141 [==============================] - 2s 11ms/step - loss: 0.9834 - accuracy: 0.6767 - val_loss: 1.3169 - val_accuracy: 0.5160
Epoch 13/20
141/141 [==============================] - 1s 8ms/step - loss: 0.9445 - accuracy: 0.6931 - val_loss: 1.2995 - val_accuracy: 0.5240
Epoch 14/20
141/141 [==============================] - 1s 8ms/step - loss: 0.9074 - accuracy: 0.7107 - val_loss: 1.2881 - val_accuracy: 0.5320
Epoch 15/20
141/141 [==============================] - 1s 8ms/step - loss: 0.8717 - accuracy: 0.7269 - val_loss: 1.2732 - val_accuracy: 0.5420
Epoch 16/20
141/141 [==============================] - 1s 8ms/step - loss: 0.8374 - accuracy: 0.7478 - val_loss: 1.2640 - val_accuracy: 0.5420
Epoch 17/20
141/141 [==============================] - 1s 8ms/step - loss: 0.8041 - accuracy: 0.7622 - val_loss: 1.2550 - val_accuracy: 0.5380
Epoch 18/20
141/141 [==============================] - 1s 8ms/step - loss: 0.7721 - accuracy: 0.7778 - val_loss: 1.2504 - val_accuracy: 0.5480
Epoch 19/20
141/141 [==============================] - 1s 8ms/step - loss: 0.7413 - accuracy: 0.7953 - val_loss: 1.2438 - val_accuracy: 0.5460
Epoch 20/20
141/141 [==============================] - 1s 9ms/step - loss: 0.7113 - accuracy: 0.8122 - val_loss: 1.2378 - val_accuracy: 0.5520
Training model with num_blocks=4, use_skip=False, and learning_rate=0.001
Epoch 1/20
141/141 [==============================] - 4s 10ms/step - loss: 1.7467 - accuracy: 0.3518 - val_loss: 2.4245 - val_accuracy: 0.1060
Epoch 2/20
141/141 [==============================] - 1s 9ms/step - loss: 1.3043 - accuracy: 0.5218 - val_loss: 2.8335 - val_accuracy: 0.1520
Epoch 3/20
141/141 [==============================] - 1s 8ms/step - loss: 1.0767 - accuracy: 0.6238 - val_loss: 2.1762 - val_accuracy: 0.2960
Epoch 4/20
141/141 [==============================] - 1s 8ms/step - loss: 0.9019 - accuracy: 0.6996 - val_loss: 1.5227 - val_accuracy: 0.4480
Epoch 5/20
141/141 [==============================] - 1s 8ms/step - loss: 0.7540 - accuracy: 0.7680 - val_loss: 1.4505 - val_accuracy: 0.4560
Epoch 6/20
141/141 [==============================] - 1s 8ms/step - loss: 0.6320 - accuracy: 0.8216 - val_loss: 1.4151 - val_accuracy: 0.4960
Epoch 7/20
141/141 [==============================] - 1s 9ms/step - loss: 0.5225 - accuracy: 0.8653 - val_loss: 1.5505 - val_accuracy: 0.4680
Epoch 8/20
141/141 [==============================] - 1s 10ms/step - loss: 0.4262 - accuracy: 0.9033 - val_loss: 1.5769 - val_accuracy: 0.4700
Epoch 9/20
141/141 [==============================] - 1s 10ms/step - loss: 0.3423 - accuracy: 0.9351 - val_loss: 1.5709 - val_accuracy: 0.4920
Epoch 10/20
141/141 [==============================] - 1s 8ms/step - loss: 0.2663 - accuracy: 0.9640 - val_loss: 2.3646 - val_accuracy: 0.3600
Epoch 11/20
141/141 [==============================] - 1s 8ms/step - loss: 0.2170 - accuracy: 0.9756 - val_loss: 2.5921 - val_accuracy: 0.3280
Epoch 12/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1750 - accuracy: 0.9833 - val_loss: 2.0552 - val_accuracy: 0.4300
Epoch 13/20
141/141 [==============================] - 1s 8ms/step - loss: 0.1356 - accuracy: 0.9907 - val_loss: 1.7173 - val_accuracy: 0.5140
Epoch 14/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0994 - accuracy: 0.9953 - val_loss: 1.6295 - val_accuracy: 0.5240
Epoch 15/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0753 - accuracy: 0.9980 - val_loss: 1.7714 - val_accuracy: 0.5240
Epoch 16/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0542 - accuracy: 0.9996 - val_loss: 1.7023 - val_accuracy: 0.5520
Epoch 17/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0400 - accuracy: 0.9996 - val_loss: 1.8312 - val_accuracy: 0.5340
Epoch 18/20
141/141 [==============================] - 2s 11ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 1.6550 - val_accuracy: 0.5600
Epoch 19/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 1.5509 - val_accuracy: 0.5660
Epoch 20/20
141/141 [==============================] - 1s 8ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 1.5023 - val_accuracy: 0.5720
Training model with num_blocks=4, use_skip=False, and learning_rate=0.0001
Epoch 1/20
141/141 [==============================] - 4s 11ms/step - loss: 2.1205 - accuracy: 0.2304 - val_loss: 2.3534 - val_accuracy: 0.0880
Epoch 2/20
141/141 [==============================] - 1s 8ms/step - loss: 1.7792 - accuracy: 0.3682 - val_loss: 2.4094 - val_accuracy: 0.1020
Epoch 3/20
141/141 [==============================] - 1s 8ms/step - loss: 1.6251 - accuracy: 0.4247 - val_loss: 2.0627 - val_accuracy: 0.2360
Epoch 4/20
141/141 [==============================] - 1s 10ms/step - loss: 1.5212 - accuracy: 0.4638 - val_loss: 1.7005 - val_accuracy: 0.3820
Epoch 5/20
141/141 [==============================] - 2s 12ms/step - loss: 1.4425 - accuracy: 0.4956 - val_loss: 1.5809 - val_accuracy: 0.4340
Epoch 6/20
141/141 [==============================] - 1s 8ms/step - loss: 1.3778 - accuracy: 0.5220 - val_loss: 1.5305 - val_accuracy: 0.4460
Epoch 7/20
141/141 [==============================] - 1s 8ms/step - loss: 1.3219 - accuracy: 0.5456 - val_loss: 1.4987 - val_accuracy: 0.4600
Epoch 8/20
141/141 [==============================] - 1s 8ms/step - loss: 1.2712 - accuracy: 0.5691 - val_loss: 1.4723 - val_accuracy: 0.4620
Epoch 9/20
141/141 [==============================] - 1s 9ms/step - loss: 1.2244 - accuracy: 0.5940 - val_loss: 1.4470 - val_accuracy: 0.4720
Epoch 10/20
141/141 [==============================] - 1s 8ms/step - loss: 1.1808 - accuracy: 0.6116 - val_loss: 1.4280 - val_accuracy: 0.4760
Epoch 11/20
141/141 [==============================] - 1s 8ms/step - loss: 1.1394 - accuracy: 0.6302 - val_loss: 1.4109 - val_accuracy: 0.4800
Epoch 12/20
141/141 [==============================] - 1s 9ms/step - loss: 1.1003 - accuracy: 0.6480 - val_loss: 1.3933 - val_accuracy: 0.4860
Epoch 13/20
141/141 [==============================] - 2s 11ms/step - loss: 1.0629 - accuracy: 0.6633 - val_loss: 1.3827 - val_accuracy: 0.4920
Epoch 14/20
141/141 [==============================] - 2s 11ms/step - loss: 1.0269 - accuracy: 0.6776 - val_loss: 1.3685 - val_accuracy: 0.4900
Epoch 15/20
141/141 [==============================] - 1s 8ms/step - loss: 0.9922 - accuracy: 0.6960 - val_loss: 1.3606 - val_accuracy: 0.4900
Epoch 16/20
141/141 [==============================] - 1s 9ms/step - loss: 0.9585 - accuracy: 0.7129 - val_loss: 1.3465 - val_accuracy: 0.4940
Epoch 17/20
141/141 [==============================] - 1s 9ms/step - loss: 0.9259 - accuracy: 0.7287 - val_loss: 1.3301 - val_accuracy: 0.5000
Epoch 18/20
141/141 [==============================] - 1s 9ms/step - loss: 0.8944 - accuracy: 0.7433 - val_loss: 1.3156 - val_accuracy: 0.5040
Epoch 19/20
141/141 [==============================] - 1s 9ms/step - loss: 0.8640 - accuracy: 0.7558 - val_loss: 1.3010 - val_accuracy: 0.5220
Epoch 20/20
141/141 [==============================] - 1s 9ms/step - loss: 0.8345 - accuracy: 0.7687 - val_loss: 1.2893 - val_accuracy: 0.5300
Best validation accuracy is 0.6159999966621399 with params: {&#39;num_blocks&#39;: 3, &#39;use_skip&#39;: True, &#39;learning_rate&#39;: 0.001}
</code></pre>
</div>
</div>
<div class="cell markdown" id="WqEs0TVmZzBR">
<p>Result: The best validation accuracy is 0.598 with num_blocks=4,
use_skip=true,learning_rate=0.001</p>
<p>comments: the next time I run the same code, the result changes to:
num_block=3,use_skip=True,Learning_rate=0.001. validation accuracy
becomes 0.616</p>
<p>It is noticed that the overfitting problem has been solved a bit</p>
</div>
<section id="question-35-apply-data-augmentation" class="cell markdown"
id="jReVkBb3--xc">
<h3><span style="color:#0b486b">Question 3.5: Apply data
augmentation</span></h3>
<p>We now try to apply data augmentation to improve the performance.
Extend the code of the class <code>YourModel</code> so that if the
attribute <code>is_augmentation</code> is set to <code>True</code>, we
apply the data augmentation. Also you need to incorporate early stopping
to your training process. Specifically, you early stop the training if
the valid accuracy cannot increase in three consecutive epochs.</p>
<div style="text-align: right"> <span style="color:red">[4 points]</span> </div>
</section>
<div class="cell code" id="rKJ3Z6qO--xd">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span></code></pre></div>
</div>
<div class="cell markdown" id="Cx4yb1oe--xe">
<p>Wtire your code in the cell below. Hint that you can rewrite the code
of the <code>fit</code> method to apply the data augmentation. In
addition, you can copy the code of <code>build_cnn</code> method above
to reuse here.</p>
</div>
<div class="cell code" id="u-zODTM_--xe">
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> YourModel(BaseImageClassifier):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>                 name<span class="op">=</span><span class="st">&#39;network1&#39;</span>,</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>                 width<span class="op">=</span><span class="dv">32</span>, height<span class="op">=</span><span class="dv">32</span>, depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>                 num_blocks<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>                 feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>                 num_classes<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>                 drop_rate<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>                 batch_norm <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>                 is_augmentation <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>                 activation_func<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>                 use_skip <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>,</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>                 batch_size<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>                 num_epochs<span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="fl">0.0001</span>):</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(YourModel, <span class="va">self</span>).<span class="fu">__init__</span>(name, width, height, depth, num_blocks, feature_maps, num_classes, drop_rate, batch_norm, is_augmentation,</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>                                        activation_func, use_skip, optimizer, batch_size, num_epochs, learning_rate)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> custom_block(<span class="va">self</span>, x, filters):</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># skip connection</span></span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>        shortcut <span class="op">=</span> x</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First Conv layer</span></span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span>filters, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second Conv layer</span></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span>filters, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the shortcut to the output</span></span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_skip:</span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> tf.keras.layers.add([shortcut, x])</span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mean Pooling layer</span></span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.AveragePooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout</span></span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(rate<span class="op">=</span><span class="va">self</span>.drop_rate)(x)</span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-47"><a href="#cb45-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb45-48"><a href="#cb45-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-49"><a href="#cb45-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_cnn(<span class="va">self</span>):</span>
<span id="cb45-50"><a href="#cb45-50" aria-hidden="true" tabindex="-1"></a>      input_tensor <span class="op">=</span> layers.Input(shape<span class="op">=</span>(<span class="va">self</span>.height, <span class="va">self</span>.width, <span class="va">self</span>.depth))</span>
<span id="cb45-51"><a href="#cb45-51" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> input_tensor</span>
<span id="cb45-52"><a href="#cb45-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-53"><a href="#cb45-53" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> current_feature_maps <span class="kw">in</span> <span class="va">self</span>.feature_maps:</span>
<span id="cb45-54"><a href="#cb45-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial convolution</span></span>
<span id="cb45-55"><a href="#cb45-55" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(current_feature_maps, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, activation<span class="op">=</span><span class="va">None</span>)(x)</span>
<span id="cb45-56"><a href="#cb45-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb45-57"><a href="#cb45-57" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb45-58"><a href="#cb45-58" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb45-59"><a href="#cb45-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-60"><a href="#cb45-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second convolution with skip connection</span></span>
<span id="cb45-61"><a href="#cb45-61" aria-hidden="true" tabindex="-1"></a>        shortcut <span class="op">=</span> x</span>
<span id="cb45-62"><a href="#cb45-62" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(current_feature_maps, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, activation<span class="op">=</span><span class="va">None</span>)(x)</span>
<span id="cb45-63"><a href="#cb45-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb45-64"><a href="#cb45-64" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb45-65"><a href="#cb45-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_skip:</span>
<span id="cb45-66"><a href="#cb45-66" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.Add()([shortcut, x])</span>
<span id="cb45-67"><a href="#cb45-67" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb45-68"><a href="#cb45-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-69"><a href="#cb45-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mean Pooling and Dropout</span></span>
<span id="cb45-70"><a href="#cb45-70" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.AveragePooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb45-71"><a href="#cb45-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.drop_rate <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb45-72"><a href="#cb45-72" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.Dropout(<span class="va">self</span>.drop_rate)(x)</span>
<span id="cb45-73"><a href="#cb45-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-74"><a href="#cb45-74" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> layers.Flatten()(x)</span>
<span id="cb45-75"><a href="#cb45-75" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> layers.Dense(<span class="va">self</span>.num_classes, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)(x)</span>
<span id="cb45-76"><a href="#cb45-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-77"><a href="#cb45-77" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.model <span class="op">=</span> models.Model(inputs<span class="op">=</span>input_tensor, outputs<span class="op">=</span>x)</span>
<span id="cb45-78"><a href="#cb45-78" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="va">self</span>.optimizer, loss<span class="op">=</span><span class="st">&#39;sparse_categorical_crossentropy&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb45-79"><a href="#cb45-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-80"><a href="#cb45-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-81"><a href="#cb45-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-82"><a href="#cb45-82" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, data_manager, batch_size<span class="op">=</span><span class="va">None</span>, num_epochs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb45-83"><a href="#cb45-83" aria-hidden="true" tabindex="-1"></a>      batch_size <span class="op">=</span> <span class="va">self</span>.batch_size <span class="cf">if</span> batch_size <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> batch_size</span>
<span id="cb45-84"><a href="#cb45-84" aria-hidden="true" tabindex="-1"></a>      num_epochs <span class="op">=</span> <span class="va">self</span>.num_epochs <span class="cf">if</span> num_epochs <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> num_epochs</span>
<span id="cb45-85"><a href="#cb45-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-86"><a href="#cb45-86" aria-hidden="true" tabindex="-1"></a>      x_train_batch <span class="op">=</span> <span class="va">self</span>.optimize_data_pipeline(data_manager.ds_train, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb45-87"><a href="#cb45-87" aria-hidden="true" tabindex="-1"></a>      x_val_batch <span class="op">=</span> <span class="va">self</span>.optimize_data_pipeline(data_manager.ds_val, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb45-88"><a href="#cb45-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-89"><a href="#cb45-89" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="va">self</span>.is_augmentation:<span class="co">#added code for data augmentation</span></span>
<span id="cb45-90"><a href="#cb45-90" aria-hidden="true" tabindex="-1"></a>        train_data_list, train_labels_list <span class="op">=</span> [], []</span>
<span id="cb45-91"><a href="#cb45-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data, labels <span class="kw">in</span> x_train_batch:</span>
<span id="cb45-92"><a href="#cb45-92" aria-hidden="true" tabindex="-1"></a>            train_data_list.append(data.numpy())</span>
<span id="cb45-93"><a href="#cb45-93" aria-hidden="true" tabindex="-1"></a>            train_labels_list.append(labels.numpy())</span>
<span id="cb45-94"><a href="#cb45-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-95"><a href="#cb45-95" aria-hidden="true" tabindex="-1"></a>        train_data <span class="op">=</span> np.concatenate(train_data_list, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb45-96"><a href="#cb45-96" aria-hidden="true" tabindex="-1"></a>        train_labels <span class="op">=</span> np.concatenate(train_labels_list, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb45-97"><a href="#cb45-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-98"><a href="#cb45-98" aria-hidden="true" tabindex="-1"></a>        datagen <span class="op">=</span> ImageDataGenerator(rotation_range<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb45-99"><a href="#cb45-99" aria-hidden="true" tabindex="-1"></a>                                     width_shift_range<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb45-100"><a href="#cb45-100" aria-hidden="true" tabindex="-1"></a>                                     height_shift_range<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb45-101"><a href="#cb45-101" aria-hidden="true" tabindex="-1"></a>                                     horizontal_flip<span class="op">=</span><span class="va">True</span>)<span class="co">#data augmentation</span></span>
<span id="cb45-102"><a href="#cb45-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-103"><a href="#cb45-103" aria-hidden="true" tabindex="-1"></a>        datagen.fit(train_data)</span>
<span id="cb45-104"><a href="#cb45-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-105"><a href="#cb45-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.fit(datagen.flow(train_data, train_labels, batch_size<span class="op">=</span><span class="va">self</span>.batch_size),</span>
<span id="cb45-106"><a href="#cb45-106" aria-hidden="true" tabindex="-1"></a>                       validation_data<span class="op">=</span>x_val_batch,</span>
<span id="cb45-107"><a href="#cb45-107" aria-hidden="true" tabindex="-1"></a>                       epochs<span class="op">=</span><span class="va">self</span>.num_epochs,</span>
<span id="cb45-108"><a href="#cb45-108" aria-hidden="true" tabindex="-1"></a>                       verbose<span class="op">=</span><span class="va">self</span>.verbose)</span>
<span id="cb45-109"><a href="#cb45-109" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb45-110"><a href="#cb45-110" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.fit(x_train_batch, validation_data<span class="op">=</span>x_val_batch, epochs<span class="op">=</span><span class="va">self</span>.num_epochs, batch_size<span class="op">=</span><span class="va">self</span>.batch_size, verbose<span class="op">=</span><span class="va">self</span>.verbose)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="O7MoOsTaoaeu" data-outputId="f1e51170-671d-4aaa-f728-1589b5043bc5">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>model_3_5<span class="op">=</span>YourModel(name<span class="op">=</span><span class="st">&#39;q3_5&#39;</span>,</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>                     feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>                     num_classes<span class="op">=</span>data_manager.n_classes,</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>                     num_blocks<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>                     drop_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>                     batch_norm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>                     is_augmentation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>                     use_skip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>                     optimizer<span class="op">=</span><span class="st">&#39;sgd&#39;</span>,</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>                     learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>model_3_5.build_cnn()</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>model_3_5.fit(data_manager)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/20
450/450 [==============================] - 9s 12ms/step - loss: 2.0184 - accuracy: 0.3062 - val_loss: 1.7373 - val_accuracy: 0.3420
Epoch 2/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6659 - accuracy: 0.4024 - val_loss: 1.5998 - val_accuracy: 0.3980
Epoch 3/20
450/450 [==============================] - 5s 11ms/step - loss: 1.5359 - accuracy: 0.4438 - val_loss: 1.6073 - val_accuracy: 0.3860
Epoch 4/20
450/450 [==============================] - 6s 12ms/step - loss: 1.4367 - accuracy: 0.4691 - val_loss: 1.4128 - val_accuracy: 0.4860
Epoch 5/20
450/450 [==============================] - 6s 12ms/step - loss: 1.3578 - accuracy: 0.5047 - val_loss: 1.3745 - val_accuracy: 0.4980
Epoch 6/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2977 - accuracy: 0.5216 - val_loss: 1.7192 - val_accuracy: 0.4480
Epoch 7/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2417 - accuracy: 0.5478 - val_loss: 1.1789 - val_accuracy: 0.5520
Epoch 8/20
450/450 [==============================] - 5s 11ms/step - loss: 1.1722 - accuracy: 0.5644 - val_loss: 1.5165 - val_accuracy: 0.5140
Epoch 9/20
450/450 [==============================] - 5s 11ms/step - loss: 1.1578 - accuracy: 0.5718 - val_loss: 1.5003 - val_accuracy: 0.5200
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0827 - accuracy: 0.6007 - val_loss: 1.5834 - val_accuracy: 0.4760
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0400 - accuracy: 0.6156 - val_loss: 1.1756 - val_accuracy: 0.5620
Epoch 12/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0200 - accuracy: 0.6300 - val_loss: 1.1522 - val_accuracy: 0.6080
Epoch 13/20
450/450 [==============================] - 5s 11ms/step - loss: 0.9788 - accuracy: 0.6467 - val_loss: 1.1999 - val_accuracy: 0.5800
Epoch 14/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9303 - accuracy: 0.6613 - val_loss: 1.2312 - val_accuracy: 0.5840
Epoch 15/20
450/450 [==============================] - 5s 11ms/step - loss: 0.9233 - accuracy: 0.6636 - val_loss: 1.8355 - val_accuracy: 0.4180
Epoch 16/20
450/450 [==============================] - 6s 13ms/step - loss: 0.8817 - accuracy: 0.6767 - val_loss: 1.5634 - val_accuracy: 0.5160
Epoch 17/20
450/450 [==============================] - 5s 11ms/step - loss: 0.8544 - accuracy: 0.6818 - val_loss: 1.3581 - val_accuracy: 0.5500
Epoch 18/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8375 - accuracy: 0.6947 - val_loss: 2.0195 - val_accuracy: 0.4120
Epoch 19/20
450/450 [==============================] - 6s 12ms/step - loss: 0.8080 - accuracy: 0.7067 - val_loss: 1.2482 - val_accuracy: 0.5800
Epoch 20/20
450/450 [==============================] - 5s 11ms/step - loss: 0.7768 - accuracy: 0.7142 - val_loss: 1.2149 - val_accuracy: 0.5880
</code></pre>
</div>
</div>
<section
id="question-36-observe-model-performance-with-data-augmentation"
class="cell markdown" id="rhXDgIKe--xf">
<h3><span style="color:#0b486b">Question 3.6: Observe model performance
with data augmentation</span></h3>
<p>Leverage your best model with the data augmentation and try to
observe the difference in performance between using data augmentation
and not using it.</p>
<div style="text-align: right"> <span style="color:red">[4 points]</span> </div>
</section>
<div class="cell markdown" id="7oviDUFcNSuU">
<p>From the observation, in terms of the val_accuracy, it is 0.588, not
improved a lot. but I noticed that, without data augmentation, the
accuracy is almost 1 while val_accuracy is only around 0.6, which
meaning that there was a huge overfitting issue.</p>
<p>With the data augmentation, although validation accuracy does not
improve a lot, the accuracy downs to 0.7142, meaning that the
overfitting issue has been improved a lot</p>
</div>
<section id="question-37-explore-data-mixup-technique"
class="cell markdown" id="33eiCIKa--xh">
<h3><span style="color:#0b486b">Question 3.7: Explore data mixup
technique</span></h3>
<div style="text-align: right"> <span style="color:red">[4 points]</span> </div>
<p>Data mixup is another super-simple technique used to boost the
generalization ability of deep learning models. You need to incoroporate
data mixup technique to the above deep learning model and experiment its
performance. There are some papers and documents for data mixup as
follows:</p>
<ul>
<li>Main paper for data mixup <a
href="https://openreview.net/pdf?id=r1Ddp1-Rb">link for main paper</a>
and a good article <a
href="https://www.inference.vc/mixup-data-dependent-data-augmentation/">article
link</a>.</li>
</ul>
<p>You need to extend your model developed above, train a model using
data mixup, and write your observations and comments about the
result.</p>
</section>
<div class="cell code" id="0vDeh-MY--xi" data-tags="[]">
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You can add more cells if necessary</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> YourModel(BaseImageClassifier):</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>                 name<span class="op">=</span><span class="st">&#39;network1&#39;</span>,</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>                 width<span class="op">=</span><span class="dv">32</span>, height<span class="op">=</span><span class="dv">32</span>, depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>                 num_blocks<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>                 feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>                 num_classes<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>                 drop_rate<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>                 batch_norm <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>                 is_augmentation <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>                 activation_func<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>                 use_skip <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>,</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>                 batch_size<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>                 num_epochs<span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="fl">0.0001</span>):</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(YourModel, <span class="va">self</span>).<span class="fu">__init__</span>(name, width, height, depth, num_blocks, feature_maps, num_classes, drop_rate, batch_norm, is_augmentation,</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>                                        activation_func, use_skip, optimizer, batch_size, num_epochs, learning_rate)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> custom_block(<span class="va">self</span>, x, filters):</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># skip connection</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>        shortcut <span class="op">=</span> x</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First Conv layer</span></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span>filters, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second Conv layer</span></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span>filters, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the shortcut to the output</span></span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_skip:</span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> tf.keras.layers.add([shortcut, x])</span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mean Pooling layer</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.AveragePooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)(x)</span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout</span></span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(rate<span class="op">=</span><span class="va">self</span>.drop_rate)(x)</span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_cnn(<span class="va">self</span>):</span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>      input_tensor <span class="op">=</span> layers.Input(shape<span class="op">=</span>(<span class="va">self</span>.height, <span class="va">self</span>.width, <span class="va">self</span>.depth))</span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> input_tensor</span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-55"><a href="#cb48-55" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> current_feature_maps <span class="kw">in</span> <span class="va">self</span>.feature_maps:</span>
<span id="cb48-56"><a href="#cb48-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial convolution</span></span>
<span id="cb48-57"><a href="#cb48-57" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(current_feature_maps, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, activation<span class="op">=</span><span class="va">None</span>)(x)</span>
<span id="cb48-58"><a href="#cb48-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb48-59"><a href="#cb48-59" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb48-60"><a href="#cb48-60" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb48-61"><a href="#cb48-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-62"><a href="#cb48-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second convolution with skip connection</span></span>
<span id="cb48-63"><a href="#cb48-63" aria-hidden="true" tabindex="-1"></a>        shortcut <span class="op">=</span> x</span>
<span id="cb48-64"><a href="#cb48-64" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Conv2D(current_feature_maps, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, activation<span class="op">=</span><span class="va">None</span>)(x)</span>
<span id="cb48-65"><a href="#cb48-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.batch_norm:</span>
<span id="cb48-66"><a href="#cb48-66" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb48-67"><a href="#cb48-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_skip:</span>
<span id="cb48-68"><a href="#cb48-68" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.Add()([shortcut, x])</span>
<span id="cb48-69"><a href="#cb48-69" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Activation(<span class="va">self</span>.activation_func)(x)</span>
<span id="cb48-70"><a href="#cb48-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-71"><a href="#cb48-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mean Pooling and Dropout</span></span>
<span id="cb48-72"><a href="#cb48-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.AveragePooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb48-73"><a href="#cb48-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.drop_rate <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb48-74"><a href="#cb48-74" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layers.Dropout(<span class="va">self</span>.drop_rate)(x)</span>
<span id="cb48-75"><a href="#cb48-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-76"><a href="#cb48-76" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> layers.Flatten()(x)</span>
<span id="cb48-77"><a href="#cb48-77" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> layers.Dense(<span class="va">self</span>.num_classes, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)(x)</span>
<span id="cb48-78"><a href="#cb48-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-79"><a href="#cb48-79" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.model <span class="op">=</span> models.Model(inputs<span class="op">=</span>input_tensor, outputs<span class="op">=</span>x)</span>
<span id="cb48-80"><a href="#cb48-80" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="va">self</span>.optimizer, loss<span class="op">=</span><span class="st">&#39;sparse_categorical_crossentropy&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb48-81"><a href="#cb48-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-82"><a href="#cb48-82" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_mixup_data(<span class="va">self</span>,x, y, alpha<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb48-83"><a href="#cb48-83" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb48-84"><a href="#cb48-84" aria-hidden="true" tabindex="-1"></a><span class="co">      Apply mixup technique</span></span>
<span id="cb48-85"><a href="#cb48-85" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb48-86"><a href="#cb48-86" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> alpha <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb48-87"><a href="#cb48-87" aria-hidden="true" tabindex="-1"></a>        lam <span class="op">=</span> np.random.beta(alpha, alpha)</span>
<span id="cb48-88"><a href="#cb48-88" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb48-89"><a href="#cb48-89" aria-hidden="true" tabindex="-1"></a>        lam <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb48-90"><a href="#cb48-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-91"><a href="#cb48-91" aria-hidden="true" tabindex="-1"></a>      batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb48-92"><a href="#cb48-92" aria-hidden="true" tabindex="-1"></a>      index <span class="op">=</span> np.random.permutation(batch_size)</span>
<span id="cb48-93"><a href="#cb48-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-94"><a href="#cb48-94" aria-hidden="true" tabindex="-1"></a>      mixed_x <span class="op">=</span> lam <span class="op">*</span> x <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> lam) <span class="op">*</span> x[index]<span class="co">#mixup</span></span>
<span id="cb48-95"><a href="#cb48-95" aria-hidden="true" tabindex="-1"></a>      mixed_y <span class="op">=</span> lam <span class="op">*</span> y <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> lam) <span class="op">*</span> y[index]<span class="co">#mixup</span></span>
<span id="cb48-96"><a href="#cb48-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-97"><a href="#cb48-97" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> mixed_x, mixed_y</span>
<span id="cb48-98"><a href="#cb48-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-99"><a href="#cb48-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-100"><a href="#cb48-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-101"><a href="#cb48-101" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, data_manager, batch_size<span class="op">=</span><span class="va">None</span>, num_epochs<span class="op">=</span><span class="va">None</span>, use_mixup<span class="op">=</span><span class="va">False</span>, alpha<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb48-102"><a href="#cb48-102" aria-hidden="true" tabindex="-1"></a>      batch_size <span class="op">=</span> <span class="va">self</span>.batch_size <span class="cf">if</span> batch_size <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> batch_size</span>
<span id="cb48-103"><a href="#cb48-103" aria-hidden="true" tabindex="-1"></a>      num_epochs <span class="op">=</span> <span class="va">self</span>.num_epochs <span class="cf">if</span> num_epochs <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> num_epochs</span>
<span id="cb48-104"><a href="#cb48-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-105"><a href="#cb48-105" aria-hidden="true" tabindex="-1"></a>      x_train_batch <span class="op">=</span> <span class="va">self</span>.optimize_data_pipeline(data_manager.ds_train, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb48-106"><a href="#cb48-106" aria-hidden="true" tabindex="-1"></a>      x_val_batch <span class="op">=</span> <span class="va">self</span>.optimize_data_pipeline(data_manager.ds_val, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb48-107"><a href="#cb48-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-108"><a href="#cb48-108" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="va">self</span>.is_augmentation:<span class="co">#code for data augmentation</span></span>
<span id="cb48-109"><a href="#cb48-109" aria-hidden="true" tabindex="-1"></a>        train_data_list, train_labels_list <span class="op">=</span> [], []</span>
<span id="cb48-110"><a href="#cb48-110" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data, labels <span class="kw">in</span> x_train_batch:</span>
<span id="cb48-111"><a href="#cb48-111" aria-hidden="true" tabindex="-1"></a>            train_data_list.append(data.numpy())</span>
<span id="cb48-112"><a href="#cb48-112" aria-hidden="true" tabindex="-1"></a>            train_labels_list.append(labels.numpy())</span>
<span id="cb48-113"><a href="#cb48-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-114"><a href="#cb48-114" aria-hidden="true" tabindex="-1"></a>        train_data <span class="op">=</span> np.concatenate(train_data_list, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb48-115"><a href="#cb48-115" aria-hidden="true" tabindex="-1"></a>        train_labels <span class="op">=</span> np.concatenate(train_labels_list, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb48-116"><a href="#cb48-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-117"><a href="#cb48-117" aria-hidden="true" tabindex="-1"></a>        datagen <span class="op">=</span> ImageDataGenerator(rotation_range<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb48-118"><a href="#cb48-118" aria-hidden="true" tabindex="-1"></a>                                     width_shift_range<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb48-119"><a href="#cb48-119" aria-hidden="true" tabindex="-1"></a>                                     height_shift_range<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb48-120"><a href="#cb48-120" aria-hidden="true" tabindex="-1"></a>                                     horizontal_flip<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-121"><a href="#cb48-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-122"><a href="#cb48-122" aria-hidden="true" tabindex="-1"></a>        datagen.fit(train_data)</span>
<span id="cb48-123"><a href="#cb48-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-124"><a href="#cb48-124" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.fit(datagen.flow(train_data, train_labels, batch_size<span class="op">=</span><span class="va">self</span>.batch_size),</span>
<span id="cb48-125"><a href="#cb48-125" aria-hidden="true" tabindex="-1"></a>                       validation_data<span class="op">=</span>x_val_batch,</span>
<span id="cb48-126"><a href="#cb48-126" aria-hidden="true" tabindex="-1"></a>                       epochs<span class="op">=</span><span class="va">self</span>.num_epochs,</span>
<span id="cb48-127"><a href="#cb48-127" aria-hidden="true" tabindex="-1"></a>                       verbose<span class="op">=</span><span class="va">self</span>.verbose)</span>
<span id="cb48-128"><a href="#cb48-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-129"><a href="#cb48-129" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> use_mixup:</span>
<span id="cb48-130"><a href="#cb48-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb48-131"><a href="#cb48-131" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> step, (x_batch, y_batch) <span class="kw">in</span> <span class="bu">enumerate</span>(x_train_batch):</span>
<span id="cb48-132"><a href="#cb48-132" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Apply mixup</span></span>
<span id="cb48-133"><a href="#cb48-133" aria-hidden="true" tabindex="-1"></a>                mixed_x, mixed_y <span class="op">=</span> <span class="va">self</span>.generate_mixup_data(x_batch.numpy(), y_batch.numpy(), alpha<span class="op">=</span>alpha)</span>
<span id="cb48-134"><a href="#cb48-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-135"><a href="#cb48-135" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Training step</span></span>
<span id="cb48-136"><a href="#cb48-136" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.model.train_on_batch(mixed_x, mixed_y)</span>
<span id="cb48-137"><a href="#cb48-137" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb48-138"><a href="#cb48-138" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.fit(x_train_batch, validation_data<span class="op">=</span>x_val_batch, epochs<span class="op">=</span><span class="va">self</span>.num_epochs, batch_size<span class="op">=</span><span class="va">self</span>.batch_size, verbose<span class="op">=</span><span class="va">self</span>.verbose)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="S0joWeDGUhjG" data-outputId="dc0f8789-71cf-477a-ef38-54983a9aaad9">
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>model_3_7<span class="op">=</span>YourModel(name<span class="op">=</span><span class="st">&#39;q3_7&#39;</span>,</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>                     feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>                     num_classes<span class="op">=</span>data_manager.n_classes,</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>                     num_blocks<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>                     drop_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>                     batch_norm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>                     is_augmentation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>                     use_skip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>                     optimizer<span class="op">=</span><span class="st">&#39;sgd&#39;</span>,</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>                     learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>model_3_7.build_cnn()</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>model_3_7.fit(data_manager,use_mixup<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/20
450/450 [==============================] - 9s 13ms/step - loss: 2.0255 - accuracy: 0.2962 - val_loss: 2.0315 - val_accuracy: 0.3100
Epoch 2/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6694 - accuracy: 0.3902 - val_loss: 1.5157 - val_accuracy: 0.4380
Epoch 3/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5102 - accuracy: 0.4531 - val_loss: 2.1353 - val_accuracy: 0.3340
Epoch 4/20
450/450 [==============================] - 5s 12ms/step - loss: 1.4502 - accuracy: 0.4640 - val_loss: 1.8327 - val_accuracy: 0.4120
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.3412 - accuracy: 0.5102 - val_loss: 1.9333 - val_accuracy: 0.3320
Epoch 6/20
450/450 [==============================] - 5s 11ms/step - loss: 1.2960 - accuracy: 0.5253 - val_loss: 1.2722 - val_accuracy: 0.5240
Epoch 7/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2365 - accuracy: 0.5409 - val_loss: 1.7553 - val_accuracy: 0.4080
Epoch 8/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1713 - accuracy: 0.5664 - val_loss: 1.1427 - val_accuracy: 0.5820
Epoch 9/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1278 - accuracy: 0.5904 - val_loss: 1.2963 - val_accuracy: 0.5280
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1182 - accuracy: 0.5798 - val_loss: 1.2589 - val_accuracy: 0.5900
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0554 - accuracy: 0.6100 - val_loss: 1.2019 - val_accuracy: 0.5740
Epoch 12/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0309 - accuracy: 0.6213 - val_loss: 1.8021 - val_accuracy: 0.4760
Epoch 13/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9816 - accuracy: 0.6318 - val_loss: 1.5859 - val_accuracy: 0.4980
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9381 - accuracy: 0.6631 - val_loss: 1.3570 - val_accuracy: 0.5500
Epoch 15/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9351 - accuracy: 0.6580 - val_loss: 1.3212 - val_accuracy: 0.5620
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8765 - accuracy: 0.6804 - val_loss: 1.5741 - val_accuracy: 0.4840
Epoch 17/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8558 - accuracy: 0.6867 - val_loss: 1.2008 - val_accuracy: 0.5880
Epoch 18/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8264 - accuracy: 0.7022 - val_loss: 1.1152 - val_accuracy: 0.6000
Epoch 19/20
450/450 [==============================] - 6s 13ms/step - loss: 0.7997 - accuracy: 0.7091 - val_loss: 1.2903 - val_accuracy: 0.5880
Epoch 20/20
450/450 [==============================] - 5s 12ms/step - loss: 0.7928 - accuracy: 0.7131 - val_loss: 1.0881 - val_accuracy: 0.6220
</code></pre>
</div>
</div>
<div class="cell markdown" id="17vNCVnQXzIn">
<p>Result: "loss: 0.7928 - accuracy: 0.7131 - val_loss: 1.0881 -
val_accuracy: 0.6220"</p>
<p>It is noticed that val_accuracy improves.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="2HzlbSRSYEnL" data-outputId="770b36f1-6174-40ab-b440-29038c6e3a49">
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">#######The following code is just for further fine tuning the parameters############</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="co"># try to further fine tune the parameters to find the best combination</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>drop_rates <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>]</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> [<span class="st">&#39;sgd&#39;</span>, <span class="st">&#39;adam&#39;</span>, <span class="st">&#39;rmsprop&#39;</span>]</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>]</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Placeholder</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> <span class="va">None</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>best_val_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over all combinations</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> drop_rate <span class="kw">in</span> drop_rates:</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> optimizer <span class="kw">in</span> optimizers:</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a model with the current parameter combination</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>            model_3_7_2 <span class="op">=</span> YourModel(name<span class="op">=</span><span class="st">&#39;q3_7_2&#39;</span>,</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>                              feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>                              num_classes<span class="op">=</span>data_manager.n_classes,</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>                              num_blocks<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>                              drop_rate<span class="op">=</span>drop_rate,</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>                              batch_norm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>                              is_augmentation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>                              use_skip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>                              optimizer<span class="op">=</span>optimizer,</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>                              learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Build and train the model</span></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>            model_3_7_2.build_cnn()</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>            model_3_7_2.fit(data_manager, use_mixup<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span>alpha)</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Evaluate the model on the validation set</span></span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>            val_accuracy <span class="op">=</span> <span class="bu">max</span>(model_3_7_2.model.history.history[<span class="st">&#39;val_accuracy&#39;</span>])</span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check if this performance is the best so far</span></span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> val_accuracy<span class="op">&gt;</span> best_val_accuracy:</span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>                best_val_performance <span class="op">=</span> val_accuracy</span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>                best_params <span class="op">=</span> {<span class="st">&#39;drop_rate&#39;</span>: drop_rate, <span class="st">&#39;optimizer&#39;</span>: optimizer, <span class="st">&#39;alpha&#39;</span>: alpha}</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Parameters:&quot;</span>, best_params)</span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Validation Performance:&quot;</span>, best_val_accuracy)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/20
450/450 [==============================] - 9s 13ms/step - loss: 2.0354 - accuracy: 0.3029 - val_loss: 1.7770 - val_accuracy: 0.3340
Epoch 2/20
450/450 [==============================] - 6s 13ms/step - loss: 1.7012 - accuracy: 0.3849 - val_loss: 1.5626 - val_accuracy: 0.4180
Epoch 3/20
450/450 [==============================] - 5s 12ms/step - loss: 1.5254 - accuracy: 0.4424 - val_loss: 1.7653 - val_accuracy: 0.3640
Epoch 4/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4283 - accuracy: 0.4678 - val_loss: 1.5592 - val_accuracy: 0.4520
Epoch 5/20
450/450 [==============================] - 5s 12ms/step - loss: 1.3566 - accuracy: 0.5080 - val_loss: 1.5697 - val_accuracy: 0.4660
Epoch 6/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2920 - accuracy: 0.5360 - val_loss: 1.3516 - val_accuracy: 0.5020
Epoch 7/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2373 - accuracy: 0.5447 - val_loss: 1.2674 - val_accuracy: 0.5360
Epoch 8/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1924 - accuracy: 0.5624 - val_loss: 1.5309 - val_accuracy: 0.4540
Epoch 9/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1282 - accuracy: 0.5951 - val_loss: 1.3653 - val_accuracy: 0.5280
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1126 - accuracy: 0.5949 - val_loss: 1.7026 - val_accuracy: 0.4520
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0546 - accuracy: 0.6198 - val_loss: 1.8052 - val_accuracy: 0.4560
Epoch 12/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0238 - accuracy: 0.6189 - val_loss: 1.4236 - val_accuracy: 0.5300
Epoch 13/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9884 - accuracy: 0.6373 - val_loss: 1.1623 - val_accuracy: 0.5960
Epoch 14/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9452 - accuracy: 0.6544 - val_loss: 1.4605 - val_accuracy: 0.5200
Epoch 15/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9289 - accuracy: 0.6598 - val_loss: 1.7715 - val_accuracy: 0.4540
Epoch 16/20
450/450 [==============================] - 6s 12ms/step - loss: 0.8675 - accuracy: 0.6827 - val_loss: 1.1270 - val_accuracy: 0.6220
Epoch 17/20
450/450 [==============================] - 6s 13ms/step - loss: 0.8843 - accuracy: 0.6780 - val_loss: 1.2103 - val_accuracy: 0.5900
Epoch 18/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8475 - accuracy: 0.6967 - val_loss: 2.5453 - val_accuracy: 0.3960
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 0.7988 - accuracy: 0.7047 - val_loss: 1.0827 - val_accuracy: 0.6320
Epoch 20/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8029 - accuracy: 0.7038 - val_loss: 1.4081 - val_accuracy: 0.5740
Epoch 1/20
450/450 [==============================] - 10s 17ms/step - loss: 2.0276 - accuracy: 0.3073 - val_loss: 1.7634 - val_accuracy: 0.3740
Epoch 2/20
450/450 [==============================] - 5s 12ms/step - loss: 1.6939 - accuracy: 0.3880 - val_loss: 1.6321 - val_accuracy: 0.4100
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5266 - accuracy: 0.4431 - val_loss: 2.4063 - val_accuracy: 0.2760
Epoch 4/20
450/450 [==============================] - 5s 12ms/step - loss: 1.4411 - accuracy: 0.4747 - val_loss: 1.5643 - val_accuracy: 0.4740
Epoch 5/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3557 - accuracy: 0.4949 - val_loss: 1.4572 - val_accuracy: 0.5040
Epoch 6/20
450/450 [==============================] - 5s 12ms/step - loss: 1.3041 - accuracy: 0.5211 - val_loss: 1.5791 - val_accuracy: 0.4600
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2475 - accuracy: 0.5384 - val_loss: 1.4807 - val_accuracy: 0.4800
Epoch 8/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2113 - accuracy: 0.5629 - val_loss: 1.4128 - val_accuracy: 0.5000
Epoch 9/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1169 - accuracy: 0.5940 - val_loss: 1.1977 - val_accuracy: 0.5540
Epoch 10/20
450/450 [==============================] - 6s 12ms/step - loss: 1.1066 - accuracy: 0.5933 - val_loss: 1.2769 - val_accuracy: 0.5420
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0626 - accuracy: 0.6171 - val_loss: 1.8713 - val_accuracy: 0.4620
Epoch 12/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0526 - accuracy: 0.6167 - val_loss: 1.7249 - val_accuracy: 0.4420
Epoch 13/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0185 - accuracy: 0.6304 - val_loss: 1.3825 - val_accuracy: 0.5100
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9636 - accuracy: 0.6531 - val_loss: 1.5294 - val_accuracy: 0.5140
Epoch 15/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9518 - accuracy: 0.6504 - val_loss: 1.2967 - val_accuracy: 0.5620
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9205 - accuracy: 0.6691 - val_loss: 1.0866 - val_accuracy: 0.5940
Epoch 17/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8674 - accuracy: 0.6829 - val_loss: 1.2322 - val_accuracy: 0.5700
Epoch 18/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8370 - accuracy: 0.6951 - val_loss: 1.8385 - val_accuracy: 0.4560
Epoch 19/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8487 - accuracy: 0.6987 - val_loss: 1.3269 - val_accuracy: 0.5420
Epoch 20/20
450/450 [==============================] - 6s 13ms/step - loss: 0.7964 - accuracy: 0.7082 - val_loss: 1.2248 - val_accuracy: 0.5980
Epoch 1/20
450/450 [==============================] - 8s 13ms/step - loss: 2.0375 - accuracy: 0.3084 - val_loss: 2.2273 - val_accuracy: 0.3380
Epoch 2/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6589 - accuracy: 0.3953 - val_loss: 2.6954 - val_accuracy: 0.2200
Epoch 3/20
450/450 [==============================] - 5s 12ms/step - loss: 1.5299 - accuracy: 0.4418 - val_loss: 1.4348 - val_accuracy: 0.4540
Epoch 4/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4245 - accuracy: 0.4831 - val_loss: 2.1491 - val_accuracy: 0.3300
Epoch 5/20
450/450 [==============================] - 5s 12ms/step - loss: 1.3310 - accuracy: 0.5113 - val_loss: 1.8443 - val_accuracy: 0.4200
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2622 - accuracy: 0.5358 - val_loss: 1.3673 - val_accuracy: 0.5080
Epoch 7/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2205 - accuracy: 0.5533 - val_loss: 1.6660 - val_accuracy: 0.4160
Epoch 8/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1630 - accuracy: 0.5733 - val_loss: 1.2215 - val_accuracy: 0.5580
Epoch 9/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1237 - accuracy: 0.5844 - val_loss: 1.1851 - val_accuracy: 0.5780
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1110 - accuracy: 0.5929 - val_loss: 1.1893 - val_accuracy: 0.5720
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0519 - accuracy: 0.6153 - val_loss: 1.5565 - val_accuracy: 0.4820
Epoch 12/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0166 - accuracy: 0.6329 - val_loss: 1.0919 - val_accuracy: 0.5980
Epoch 13/20
450/450 [==============================] - 6s 12ms/step - loss: 0.9756 - accuracy: 0.6451 - val_loss: 1.6290 - val_accuracy: 0.4820
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9385 - accuracy: 0.6647 - val_loss: 1.1805 - val_accuracy: 0.5720
Epoch 15/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9199 - accuracy: 0.6689 - val_loss: 1.1943 - val_accuracy: 0.5740
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8931 - accuracy: 0.6767 - val_loss: 1.1112 - val_accuracy: 0.6000
Epoch 17/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8654 - accuracy: 0.6871 - val_loss: 1.1290 - val_accuracy: 0.6020
Epoch 18/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8332 - accuracy: 0.6969 - val_loss: 1.0585 - val_accuracy: 0.6140
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8083 - accuracy: 0.7047 - val_loss: 1.5422 - val_accuracy: 0.5420
Epoch 20/20
450/450 [==============================] - 6s 13ms/step - loss: 0.7943 - accuracy: 0.7098 - val_loss: 1.3984 - val_accuracy: 0.5540
Epoch 1/20
450/450 [==============================] - 13s 14ms/step - loss: 2.1137 - accuracy: 0.2707 - val_loss: 1.7632 - val_accuracy: 0.3660
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7832 - accuracy: 0.3436 - val_loss: 1.8600 - val_accuracy: 0.3700
Epoch 3/20
450/450 [==============================] - 6s 12ms/step - loss: 1.6610 - accuracy: 0.3898 - val_loss: 1.5037 - val_accuracy: 0.4460
Epoch 4/20
450/450 [==============================] - 7s 14ms/step - loss: 1.5607 - accuracy: 0.4240 - val_loss: 1.4796 - val_accuracy: 0.4440
Epoch 5/20
450/450 [==============================] - 6s 12ms/step - loss: 1.4926 - accuracy: 0.4482 - val_loss: 1.4034 - val_accuracy: 0.4700
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4231 - accuracy: 0.4744 - val_loss: 1.5006 - val_accuracy: 0.4240
Epoch 7/20
450/450 [==============================] - 6s 12ms/step - loss: 1.3639 - accuracy: 0.4900 - val_loss: 1.5254 - val_accuracy: 0.4620
Epoch 8/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2941 - accuracy: 0.5158 - val_loss: 1.3231 - val_accuracy: 0.5100
Epoch 9/20
450/450 [==============================] - 6s 12ms/step - loss: 1.2272 - accuracy: 0.5476 - val_loss: 1.6451 - val_accuracy: 0.4380
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1920 - accuracy: 0.5569 - val_loss: 1.6471 - val_accuracy: 0.4220
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1457 - accuracy: 0.5851 - val_loss: 1.8055 - val_accuracy: 0.3800
Epoch 12/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1070 - accuracy: 0.5951 - val_loss: 1.6724 - val_accuracy: 0.4480
Epoch 13/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0424 - accuracy: 0.6116 - val_loss: 1.0807 - val_accuracy: 0.6140
Epoch 14/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0056 - accuracy: 0.6293 - val_loss: 1.3152 - val_accuracy: 0.5120
Epoch 15/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9842 - accuracy: 0.6407 - val_loss: 1.1997 - val_accuracy: 0.5900
Epoch 16/20
450/450 [==============================] - 7s 14ms/step - loss: 0.9537 - accuracy: 0.6460 - val_loss: 1.1192 - val_accuracy: 0.6100
Epoch 17/20
450/450 [==============================] - 6s 12ms/step - loss: 0.9025 - accuracy: 0.6698 - val_loss: 1.5659 - val_accuracy: 0.4740
Epoch 18/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8839 - accuracy: 0.6818 - val_loss: 1.0431 - val_accuracy: 0.6540
Epoch 19/20
450/450 [==============================] - 6s 13ms/step - loss: 0.8552 - accuracy: 0.6909 - val_loss: 1.5771 - val_accuracy: 0.4940
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8052 - accuracy: 0.6998 - val_loss: 1.3468 - val_accuracy: 0.5360
Epoch 1/20
450/450 [==============================] - 13s 15ms/step - loss: 2.1242 - accuracy: 0.2662 - val_loss: 2.4718 - val_accuracy: 0.2660
Epoch 2/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7727 - accuracy: 0.3456 - val_loss: 1.9867 - val_accuracy: 0.2840
Epoch 3/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6398 - accuracy: 0.3967 - val_loss: 1.7196 - val_accuracy: 0.4220
Epoch 4/20
450/450 [==============================] - 7s 14ms/step - loss: 1.5126 - accuracy: 0.4433 - val_loss: 1.5613 - val_accuracy: 0.4380
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4180 - accuracy: 0.4807 - val_loss: 1.3429 - val_accuracy: 0.5180
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3524 - accuracy: 0.5022 - val_loss: 1.3083 - val_accuracy: 0.4980
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2924 - accuracy: 0.5147 - val_loss: 1.4205 - val_accuracy: 0.4740
Epoch 8/20
450/450 [==============================] - 6s 12ms/step - loss: 1.2380 - accuracy: 0.5331 - val_loss: 2.0379 - val_accuracy: 0.4020
Epoch 9/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1939 - accuracy: 0.5611 - val_loss: 1.1263 - val_accuracy: 0.6000
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1551 - accuracy: 0.5733 - val_loss: 1.4301 - val_accuracy: 0.4740
Epoch 11/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1111 - accuracy: 0.5889 - val_loss: 1.2455 - val_accuracy: 0.5660
Epoch 12/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0774 - accuracy: 0.6004 - val_loss: 1.3632 - val_accuracy: 0.5100
Epoch 13/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0208 - accuracy: 0.6198 - val_loss: 1.2225 - val_accuracy: 0.5660
Epoch 14/20
450/450 [==============================] - 6s 12ms/step - loss: 0.9744 - accuracy: 0.6424 - val_loss: 1.6167 - val_accuracy: 0.5060
Epoch 15/20
450/450 [==============================] - 7s 14ms/step - loss: 0.9561 - accuracy: 0.6511 - val_loss: 1.0791 - val_accuracy: 0.6220
Epoch 16/20
450/450 [==============================] - 6s 13ms/step - loss: 0.9172 - accuracy: 0.6627 - val_loss: 1.0447 - val_accuracy: 0.6240
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8756 - accuracy: 0.6791 - val_loss: 1.1586 - val_accuracy: 0.6160
Epoch 18/20
450/450 [==============================] - 6s 12ms/step - loss: 0.8612 - accuracy: 0.6871 - val_loss: 1.1174 - val_accuracy: 0.6120
Epoch 19/20
450/450 [==============================] - 7s 15ms/step - loss: 0.8359 - accuracy: 0.6918 - val_loss: 1.8062 - val_accuracy: 0.4760
Epoch 20/20
450/450 [==============================] - 6s 12ms/step - loss: 0.7982 - accuracy: 0.7031 - val_loss: 1.2887 - val_accuracy: 0.5780
Epoch 1/20
450/450 [==============================] - 13s 14ms/step - loss: 2.1532 - accuracy: 0.2756 - val_loss: 1.9820 - val_accuracy: 0.2940
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7762 - accuracy: 0.3469 - val_loss: 2.4789 - val_accuracy: 0.2040
Epoch 3/20
450/450 [==============================] - 5s 12ms/step - loss: 1.6531 - accuracy: 0.3898 - val_loss: 2.7400 - val_accuracy: 0.2960
Epoch 4/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5396 - accuracy: 0.4251 - val_loss: 1.9080 - val_accuracy: 0.3280
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4304 - accuracy: 0.4662 - val_loss: 1.6940 - val_accuracy: 0.3920
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3760 - accuracy: 0.4813 - val_loss: 3.2806 - val_accuracy: 0.2300
Epoch 7/20
450/450 [==============================] - 6s 12ms/step - loss: 1.3063 - accuracy: 0.5227 - val_loss: 1.4182 - val_accuracy: 0.4960
Epoch 8/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2339 - accuracy: 0.5373 - val_loss: 1.6909 - val_accuracy: 0.4320
Epoch 9/20
450/450 [==============================] - 6s 12ms/step - loss: 1.1874 - accuracy: 0.5573 - val_loss: 1.4353 - val_accuracy: 0.5320
Epoch 10/20
450/450 [==============================] - 7s 14ms/step - loss: 1.1393 - accuracy: 0.5753 - val_loss: 1.3127 - val_accuracy: 0.5220
Epoch 11/20
450/450 [==============================] - 6s 12ms/step - loss: 1.1001 - accuracy: 0.5980 - val_loss: 1.1823 - val_accuracy: 0.5380
Epoch 12/20
450/450 [==============================] - 7s 14ms/step - loss: 1.0573 - accuracy: 0.6102 - val_loss: 1.2395 - val_accuracy: 0.5720
Epoch 13/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0240 - accuracy: 0.6204 - val_loss: 1.7087 - val_accuracy: 0.4740
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9672 - accuracy: 0.6433 - val_loss: 1.1337 - val_accuracy: 0.6340
Epoch 15/20
450/450 [==============================] - 6s 12ms/step - loss: 0.9508 - accuracy: 0.6522 - val_loss: 1.7439 - val_accuracy: 0.4740
Epoch 16/20
450/450 [==============================] - 7s 15ms/step - loss: 0.9058 - accuracy: 0.6656 - val_loss: 1.1081 - val_accuracy: 0.5900
Epoch 17/20
450/450 [==============================] - 6s 12ms/step - loss: 0.8605 - accuracy: 0.6789 - val_loss: 1.2743 - val_accuracy: 0.5580
Epoch 18/20
450/450 [==============================] - 7s 15ms/step - loss: 0.8406 - accuracy: 0.6891 - val_loss: 1.1196 - val_accuracy: 0.6180
Epoch 19/20
450/450 [==============================] - 6s 12ms/step - loss: 0.8105 - accuracy: 0.7016 - val_loss: 1.2957 - val_accuracy: 0.5540
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 0.7908 - accuracy: 0.7016 - val_loss: 1.3962 - val_accuracy: 0.5580
Epoch 1/20
450/450 [==============================] - 11s 14ms/step - loss: 2.2347 - accuracy: 0.2516 - val_loss: 2.2594 - val_accuracy: 0.2440
Epoch 2/20
450/450 [==============================] - 5s 12ms/step - loss: 1.8123 - accuracy: 0.3407 - val_loss: 1.6445 - val_accuracy: 0.3840
Epoch 3/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6761 - accuracy: 0.3796 - val_loss: 1.9524 - val_accuracy: 0.3660
Epoch 4/20
450/450 [==============================] - 5s 12ms/step - loss: 1.5618 - accuracy: 0.4220 - val_loss: 1.5073 - val_accuracy: 0.4420
Epoch 5/20
450/450 [==============================] - 5s 12ms/step - loss: 1.4692 - accuracy: 0.4542 - val_loss: 2.3699 - val_accuracy: 0.3000
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3974 - accuracy: 0.4800 - val_loss: 2.2974 - val_accuracy: 0.3520
Epoch 7/20
450/450 [==============================] - 5s 12ms/step - loss: 1.3239 - accuracy: 0.5131 - val_loss: 2.1349 - val_accuracy: 0.3820
Epoch 8/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2688 - accuracy: 0.5322 - val_loss: 1.4154 - val_accuracy: 0.5040
Epoch 9/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2362 - accuracy: 0.5371 - val_loss: 1.6561 - val_accuracy: 0.4480
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1827 - accuracy: 0.5582 - val_loss: 1.7021 - val_accuracy: 0.4480
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1562 - accuracy: 0.5731 - val_loss: 1.6370 - val_accuracy: 0.4780
Epoch 12/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1095 - accuracy: 0.5922 - val_loss: 2.7953 - val_accuracy: 0.3160
Epoch 13/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0570 - accuracy: 0.6129 - val_loss: 1.7767 - val_accuracy: 0.4960
Epoch 14/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0465 - accuracy: 0.6189 - val_loss: 1.9187 - val_accuracy: 0.4540
Epoch 15/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0199 - accuracy: 0.6251 - val_loss: 1.0610 - val_accuracy: 0.6340
Epoch 16/20
450/450 [==============================] - 6s 12ms/step - loss: 0.9839 - accuracy: 0.6367 - val_loss: 1.4212 - val_accuracy: 0.5380
Epoch 17/20
450/450 [==============================] - 6s 13ms/step - loss: 0.9423 - accuracy: 0.6560 - val_loss: 1.1197 - val_accuracy: 0.6260
Epoch 18/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9156 - accuracy: 0.6678 - val_loss: 1.3959 - val_accuracy: 0.5300
Epoch 19/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8951 - accuracy: 0.6720 - val_loss: 1.1148 - val_accuracy: 0.6180
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 0.8747 - accuracy: 0.6816 - val_loss: 1.0702 - val_accuracy: 0.6260
Epoch 1/20
450/450 [==============================] - 11s 14ms/step - loss: 2.2262 - accuracy: 0.2522 - val_loss: 1.9462 - val_accuracy: 0.2900
Epoch 2/20
450/450 [==============================] - 5s 12ms/step - loss: 1.8246 - accuracy: 0.3378 - val_loss: 1.6210 - val_accuracy: 0.4000
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6568 - accuracy: 0.3898 - val_loss: 1.5594 - val_accuracy: 0.4180
Epoch 4/20
450/450 [==============================] - 5s 12ms/step - loss: 1.5673 - accuracy: 0.4262 - val_loss: 2.2974 - val_accuracy: 0.2980
Epoch 5/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4817 - accuracy: 0.4593 - val_loss: 1.3954 - val_accuracy: 0.4820
Epoch 6/20
450/450 [==============================] - 5s 12ms/step - loss: 1.4050 - accuracy: 0.4771 - val_loss: 1.4313 - val_accuracy: 0.5060
Epoch 7/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3603 - accuracy: 0.4942 - val_loss: 1.5945 - val_accuracy: 0.4440
Epoch 8/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2967 - accuracy: 0.5213 - val_loss: 1.2634 - val_accuracy: 0.5360
Epoch 9/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2393 - accuracy: 0.5373 - val_loss: 1.4708 - val_accuracy: 0.4800
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1984 - accuracy: 0.5513 - val_loss: 1.3789 - val_accuracy: 0.5180
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1469 - accuracy: 0.5780 - val_loss: 1.1787 - val_accuracy: 0.5680
Epoch 12/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1252 - accuracy: 0.5791 - val_loss: 1.4794 - val_accuracy: 0.5060
Epoch 13/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0845 - accuracy: 0.6033 - val_loss: 1.5160 - val_accuracy: 0.4860
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0525 - accuracy: 0.6142 - val_loss: 1.3910 - val_accuracy: 0.5440
Epoch 15/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0242 - accuracy: 0.6160 - val_loss: 1.7600 - val_accuracy: 0.4880
Epoch 16/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9876 - accuracy: 0.6482 - val_loss: 1.2783 - val_accuracy: 0.5620
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9623 - accuracy: 0.6491 - val_loss: 1.6105 - val_accuracy: 0.4980
Epoch 18/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9393 - accuracy: 0.6609 - val_loss: 1.4535 - val_accuracy: 0.5200
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9060 - accuracy: 0.6744 - val_loss: 1.1326 - val_accuracy: 0.6240
Epoch 20/20
450/450 [==============================] - 5s 12ms/step - loss: 0.8991 - accuracy: 0.6704 - val_loss: 1.1843 - val_accuracy: 0.6280
Epoch 1/20
450/450 [==============================] - 10s 13ms/step - loss: 2.2104 - accuracy: 0.2636 - val_loss: 2.5101 - val_accuracy: 0.2780
Epoch 2/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7879 - accuracy: 0.3387 - val_loss: 2.6973 - val_accuracy: 0.2820
Epoch 3/20
450/450 [==============================] - 5s 12ms/step - loss: 1.6782 - accuracy: 0.3733 - val_loss: 3.3359 - val_accuracy: 0.1840
Epoch 4/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5512 - accuracy: 0.4251 - val_loss: 1.4515 - val_accuracy: 0.4600
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4819 - accuracy: 0.4424 - val_loss: 1.8120 - val_accuracy: 0.3900
Epoch 6/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4198 - accuracy: 0.4776 - val_loss: 2.3389 - val_accuracy: 0.3000
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.3391 - accuracy: 0.5049 - val_loss: 1.3621 - val_accuracy: 0.5120
Epoch 8/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2726 - accuracy: 0.5269 - val_loss: 1.2940 - val_accuracy: 0.5440
Epoch 9/20
450/450 [==============================] - 5s 12ms/step - loss: 1.2359 - accuracy: 0.5476 - val_loss: 1.8727 - val_accuracy: 0.4100
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1881 - accuracy: 0.5513 - val_loss: 1.2728 - val_accuracy: 0.5100
Epoch 11/20
450/450 [==============================] - 5s 12ms/step - loss: 1.1469 - accuracy: 0.5804 - val_loss: 1.2281 - val_accuracy: 0.5520
Epoch 12/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1189 - accuracy: 0.5813 - val_loss: 1.4507 - val_accuracy: 0.5080
Epoch 13/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0768 - accuracy: 0.6100 - val_loss: 1.0950 - val_accuracy: 0.6020
Epoch 14/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0618 - accuracy: 0.6129 - val_loss: 1.4317 - val_accuracy: 0.5660
Epoch 15/20
450/450 [==============================] - 5s 12ms/step - loss: 1.0129 - accuracy: 0.6316 - val_loss: 1.0892 - val_accuracy: 0.6300
Epoch 16/20
450/450 [==============================] - 6s 12ms/step - loss: 1.0033 - accuracy: 0.6300 - val_loss: 1.2042 - val_accuracy: 0.5900
Epoch 17/20
450/450 [==============================] - 6s 12ms/step - loss: 0.9634 - accuracy: 0.6509 - val_loss: 1.8299 - val_accuracy: 0.4560
Epoch 18/20
450/450 [==============================] - 6s 13ms/step - loss: 0.9452 - accuracy: 0.6607 - val_loss: 1.1356 - val_accuracy: 0.6120
Epoch 19/20
450/450 [==============================] - 5s 12ms/step - loss: 0.9272 - accuracy: 0.6589 - val_loss: 1.3072 - val_accuracy: 0.5800
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9116 - accuracy: 0.6673 - val_loss: 1.0930 - val_accuracy: 0.6180
Epoch 1/20
450/450 [==============================] - 9s 15ms/step - loss: 2.1962 - accuracy: 0.2596 - val_loss: 1.7909 - val_accuracy: 0.2980
Epoch 2/20
450/450 [==============================] - 6s 13ms/step - loss: 1.8666 - accuracy: 0.3247 - val_loss: 2.1943 - val_accuracy: 0.2660
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7486 - accuracy: 0.3536 - val_loss: 2.0690 - val_accuracy: 0.3300
Epoch 4/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6602 - accuracy: 0.3878 - val_loss: 1.9508 - val_accuracy: 0.3540
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5763 - accuracy: 0.4138 - val_loss: 1.5519 - val_accuracy: 0.4360
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5061 - accuracy: 0.4449 - val_loss: 2.3199 - val_accuracy: 0.3520
Epoch 7/20
450/450 [==============================] - 6s 12ms/step - loss: 1.5075 - accuracy: 0.4413 - val_loss: 1.6965 - val_accuracy: 0.4120
Epoch 8/20
450/450 [==============================] - 7s 14ms/step - loss: 1.4493 - accuracy: 0.4682 - val_loss: 1.7184 - val_accuracy: 0.4420
Epoch 9/20
450/450 [==============================] - 6s 12ms/step - loss: 1.4069 - accuracy: 0.4771 - val_loss: 1.4615 - val_accuracy: 0.4820
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3738 - accuracy: 0.4898 - val_loss: 1.3040 - val_accuracy: 0.5360
Epoch 11/20
450/450 [==============================] - 6s 13ms/step - loss: 1.3305 - accuracy: 0.5013 - val_loss: 2.4997 - val_accuracy: 0.3680
Epoch 12/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3334 - accuracy: 0.5049 - val_loss: 1.4311 - val_accuracy: 0.4920
Epoch 13/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2870 - accuracy: 0.5231 - val_loss: 1.3582 - val_accuracy: 0.5260
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2899 - accuracy: 0.5278 - val_loss: 1.2296 - val_accuracy: 0.5580
Epoch 15/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2610 - accuracy: 0.5402 - val_loss: 1.4789 - val_accuracy: 0.5040
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2314 - accuracy: 0.5507 - val_loss: 1.2562 - val_accuracy: 0.5580
Epoch 17/20
450/450 [==============================] - 6s 12ms/step - loss: 1.1846 - accuracy: 0.5562 - val_loss: 1.2892 - val_accuracy: 0.5440
Epoch 18/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1665 - accuracy: 0.5731 - val_loss: 2.1407 - val_accuracy: 0.3940
Epoch 19/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1645 - accuracy: 0.5762 - val_loss: 1.2523 - val_accuracy: 0.5460
Epoch 20/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1485 - accuracy: 0.5840 - val_loss: 1.5985 - val_accuracy: 0.4920
Epoch 1/20
450/450 [==============================] - 9s 14ms/step - loss: 2.1464 - accuracy: 0.2867 - val_loss: 2.1429 - val_accuracy: 0.2520
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 1.8704 - accuracy: 0.3380 - val_loss: 2.2154 - val_accuracy: 0.2780
Epoch 3/20
450/450 [==============================] - 6s 12ms/step - loss: 1.6835 - accuracy: 0.3876 - val_loss: 1.5998 - val_accuracy: 0.4120
Epoch 4/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5907 - accuracy: 0.4142 - val_loss: 1.6238 - val_accuracy: 0.4380
Epoch 5/20
450/450 [==============================] - 6s 12ms/step - loss: 1.5617 - accuracy: 0.4278 - val_loss: 1.6735 - val_accuracy: 0.4300
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4707 - accuracy: 0.4636 - val_loss: 1.3083 - val_accuracy: 0.5240
Epoch 7/20
450/450 [==============================] - 6s 12ms/step - loss: 1.4709 - accuracy: 0.4658 - val_loss: 1.2445 - val_accuracy: 0.5180
Epoch 8/20
450/450 [==============================] - 7s 14ms/step - loss: 1.4039 - accuracy: 0.4838 - val_loss: 1.2951 - val_accuracy: 0.5440
Epoch 9/20
450/450 [==============================] - 6s 12ms/step - loss: 1.3799 - accuracy: 0.4860 - val_loss: 1.5167 - val_accuracy: 0.4740
Epoch 10/20
450/450 [==============================] - 6s 12ms/step - loss: 1.3333 - accuracy: 0.5098 - val_loss: 1.4785 - val_accuracy: 0.4700
Epoch 11/20
450/450 [==============================] - 7s 14ms/step - loss: 1.2915 - accuracy: 0.5336 - val_loss: 1.4632 - val_accuracy: 0.4940
Epoch 12/20
450/450 [==============================] - 6s 12ms/step - loss: 1.2998 - accuracy: 0.5204 - val_loss: 1.3469 - val_accuracy: 0.5280
Epoch 13/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2550 - accuracy: 0.5376 - val_loss: 1.3886 - val_accuracy: 0.5200
Epoch 14/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2293 - accuracy: 0.5520 - val_loss: 1.4196 - val_accuracy: 0.5020
Epoch 15/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2298 - accuracy: 0.5513 - val_loss: 1.6946 - val_accuracy: 0.4480
Epoch 16/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2066 - accuracy: 0.5522 - val_loss: 1.4682 - val_accuracy: 0.5100
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1609 - accuracy: 0.5667 - val_loss: 1.7355 - val_accuracy: 0.4380
Epoch 18/20
450/450 [==============================] - 6s 12ms/step - loss: 1.1650 - accuracy: 0.5718 - val_loss: 1.2265 - val_accuracy: 0.5700
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1173 - accuracy: 0.5989 - val_loss: 1.2678 - val_accuracy: 0.5460
Epoch 20/20
450/450 [==============================] - 6s 12ms/step - loss: 1.1475 - accuracy: 0.5780 - val_loss: 1.3032 - val_accuracy: 0.5600
Epoch 1/20
450/450 [==============================] - 10s 14ms/step - loss: 2.1304 - accuracy: 0.2751 - val_loss: 1.9710 - val_accuracy: 0.3080
Epoch 2/20
450/450 [==============================] - 6s 14ms/step - loss: 1.8220 - accuracy: 0.3484 - val_loss: 2.5522 - val_accuracy: 0.2420
Epoch 3/20
450/450 [==============================] - 6s 12ms/step - loss: 1.7116 - accuracy: 0.3884 - val_loss: 1.6028 - val_accuracy: 0.4020
Epoch 4/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6057 - accuracy: 0.4200 - val_loss: 1.5387 - val_accuracy: 0.4840
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5308 - accuracy: 0.4329 - val_loss: 1.4979 - val_accuracy: 0.4260
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4641 - accuracy: 0.4656 - val_loss: 1.4542 - val_accuracy: 0.4740
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4211 - accuracy: 0.4816 - val_loss: 1.6783 - val_accuracy: 0.4500
Epoch 8/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3972 - accuracy: 0.4920 - val_loss: 1.3543 - val_accuracy: 0.5020
Epoch 9/20
450/450 [==============================] - 6s 13ms/step - loss: 1.3387 - accuracy: 0.5093 - val_loss: 1.2256 - val_accuracy: 0.5600
Epoch 10/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3237 - accuracy: 0.5142 - val_loss: 1.4115 - val_accuracy: 0.5120
Epoch 11/20
450/450 [==============================] - 6s 12ms/step - loss: 1.2907 - accuracy: 0.5240 - val_loss: 1.3837 - val_accuracy: 0.5180
Epoch 12/20
450/450 [==============================] - 7s 14ms/step - loss: 1.2733 - accuracy: 0.5358 - val_loss: 1.3216 - val_accuracy: 0.5440
Epoch 13/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2231 - accuracy: 0.5484 - val_loss: 1.1485 - val_accuracy: 0.5740
Epoch 14/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2128 - accuracy: 0.5538 - val_loss: 1.2171 - val_accuracy: 0.5580
Epoch 15/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1891 - accuracy: 0.5680 - val_loss: 1.2815 - val_accuracy: 0.5480
Epoch 16/20
450/450 [==============================] - 6s 12ms/step - loss: 1.1667 - accuracy: 0.5658 - val_loss: 1.1443 - val_accuracy: 0.5980
Epoch 17/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1310 - accuracy: 0.5802 - val_loss: 1.2311 - val_accuracy: 0.5840
Epoch 18/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1186 - accuracy: 0.5900 - val_loss: 1.5499 - val_accuracy: 0.4860
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1129 - accuracy: 0.5904 - val_loss: 1.4807 - val_accuracy: 0.5280
Epoch 20/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0809 - accuracy: 0.6058 - val_loss: 1.3984 - val_accuracy: 0.5300
Epoch 1/20
450/450 [==============================] - 13s 14ms/step - loss: 2.2635 - accuracy: 0.2509 - val_loss: 1.8797 - val_accuracy: 0.2500
Epoch 2/20
450/450 [==============================] - 7s 16ms/step - loss: 1.8533 - accuracy: 0.3260 - val_loss: 2.0486 - val_accuracy: 0.3200
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6932 - accuracy: 0.3756 - val_loss: 2.0868 - val_accuracy: 0.2900
Epoch 4/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5910 - accuracy: 0.4127 - val_loss: 1.4830 - val_accuracy: 0.4420
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4967 - accuracy: 0.4440 - val_loss: 1.6783 - val_accuracy: 0.3780
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4337 - accuracy: 0.4649 - val_loss: 1.7998 - val_accuracy: 0.3320
Epoch 7/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3942 - accuracy: 0.4798 - val_loss: 2.0606 - val_accuracy: 0.3800
Epoch 8/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3491 - accuracy: 0.4913 - val_loss: 4.2999 - val_accuracy: 0.1740
Epoch 9/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2911 - accuracy: 0.5178 - val_loss: 1.4714 - val_accuracy: 0.4640
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2628 - accuracy: 0.5287 - val_loss: 1.1700 - val_accuracy: 0.5700
Epoch 11/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2219 - accuracy: 0.5469 - val_loss: 1.4337 - val_accuracy: 0.4740
Epoch 12/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2024 - accuracy: 0.5553 - val_loss: 1.5398 - val_accuracy: 0.4780
Epoch 13/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1542 - accuracy: 0.5718 - val_loss: 1.1351 - val_accuracy: 0.5760
Epoch 14/20
450/450 [==============================] - 7s 16ms/step - loss: 1.1335 - accuracy: 0.5811 - val_loss: 1.2475 - val_accuracy: 0.5400
Epoch 15/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1182 - accuracy: 0.5824 - val_loss: 1.1590 - val_accuracy: 0.5940
Epoch 16/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0863 - accuracy: 0.6042 - val_loss: 1.1417 - val_accuracy: 0.5900
Epoch 17/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0533 - accuracy: 0.6127 - val_loss: 1.2680 - val_accuracy: 0.5820
Epoch 18/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0240 - accuracy: 0.6173 - val_loss: 1.3053 - val_accuracy: 0.5560
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0034 - accuracy: 0.6351 - val_loss: 1.2813 - val_accuracy: 0.5240
Epoch 20/20
450/450 [==============================] - 7s 16ms/step - loss: 0.9900 - accuracy: 0.6449 - val_loss: 1.1672 - val_accuracy: 0.5900
Epoch 1/20
450/450 [==============================] - 13s 16ms/step - loss: 2.2121 - accuracy: 0.2640 - val_loss: 1.7624 - val_accuracy: 0.3340
Epoch 2/20
450/450 [==============================] - 6s 13ms/step - loss: 1.8658 - accuracy: 0.3269 - val_loss: 2.2417 - val_accuracy: 0.2860
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7258 - accuracy: 0.3711 - val_loss: 1.4517 - val_accuracy: 0.4540
Epoch 4/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6088 - accuracy: 0.4007 - val_loss: 2.0467 - val_accuracy: 0.3020
Epoch 5/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5363 - accuracy: 0.4218 - val_loss: 1.5731 - val_accuracy: 0.4240
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4784 - accuracy: 0.4540 - val_loss: 1.4232 - val_accuracy: 0.4700
Epoch 7/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4235 - accuracy: 0.4738 - val_loss: 1.6543 - val_accuracy: 0.4120
Epoch 8/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3700 - accuracy: 0.4902 - val_loss: 1.1865 - val_accuracy: 0.5740
Epoch 9/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3058 - accuracy: 0.5129 - val_loss: 1.7424 - val_accuracy: 0.3800
Epoch 10/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2750 - accuracy: 0.5300 - val_loss: 1.2818 - val_accuracy: 0.5120
Epoch 11/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2497 - accuracy: 0.5438 - val_loss: 1.2423 - val_accuracy: 0.5460
Epoch 12/20
450/450 [==============================] - 7s 16ms/step - loss: 1.2243 - accuracy: 0.5464 - val_loss: 1.3115 - val_accuracy: 0.5020
Epoch 13/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1555 - accuracy: 0.5647 - val_loss: 1.3146 - val_accuracy: 0.5500
Epoch 14/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1490 - accuracy: 0.5751 - val_loss: 1.1561 - val_accuracy: 0.5840
Epoch 15/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1106 - accuracy: 0.5922 - val_loss: 1.1359 - val_accuracy: 0.5620
Epoch 16/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0876 - accuracy: 0.6060 - val_loss: 1.1461 - val_accuracy: 0.5960
Epoch 17/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0595 - accuracy: 0.6067 - val_loss: 1.0617 - val_accuracy: 0.5940
Epoch 18/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0293 - accuracy: 0.6227 - val_loss: 1.3986 - val_accuracy: 0.5180
Epoch 19/20
450/450 [==============================] - 7s 16ms/step - loss: 0.9960 - accuracy: 0.6313 - val_loss: 1.2651 - val_accuracy: 0.5340
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 0.9764 - accuracy: 0.6402 - val_loss: 1.5625 - val_accuracy: 0.4940
Epoch 1/20
450/450 [==============================] - 13s 15ms/step - loss: 2.2939 - accuracy: 0.2484 - val_loss: 2.1683 - val_accuracy: 0.2580
Epoch 2/20
450/450 [==============================] - 7s 16ms/step - loss: 1.8471 - accuracy: 0.3358 - val_loss: 3.1973 - val_accuracy: 0.2340
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7073 - accuracy: 0.3767 - val_loss: 1.7782 - val_accuracy: 0.3340
Epoch 4/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6182 - accuracy: 0.3904 - val_loss: 1.7549 - val_accuracy: 0.3580
Epoch 5/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5464 - accuracy: 0.4302 - val_loss: 1.9235 - val_accuracy: 0.3160
Epoch 6/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4843 - accuracy: 0.4462 - val_loss: 1.6467 - val_accuracy: 0.4220
Epoch 7/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4270 - accuracy: 0.4691 - val_loss: 1.4983 - val_accuracy: 0.4460
Epoch 8/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3702 - accuracy: 0.4913 - val_loss: 1.5015 - val_accuracy: 0.4580
Epoch 9/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3195 - accuracy: 0.5120 - val_loss: 1.4252 - val_accuracy: 0.4980
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2926 - accuracy: 0.5222 - val_loss: 1.2697 - val_accuracy: 0.5440
Epoch 11/20
450/450 [==============================] - 7s 16ms/step - loss: 1.2392 - accuracy: 0.5451 - val_loss: 1.2201 - val_accuracy: 0.5460
Epoch 12/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2217 - accuracy: 0.5478 - val_loss: 1.7812 - val_accuracy: 0.4320
Epoch 13/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1835 - accuracy: 0.5631 - val_loss: 1.2314 - val_accuracy: 0.6040
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1592 - accuracy: 0.5776 - val_loss: 1.1544 - val_accuracy: 0.5900
Epoch 15/20
450/450 [==============================] - 7s 16ms/step - loss: 1.1336 - accuracy: 0.5813 - val_loss: 1.1526 - val_accuracy: 0.5800
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0931 - accuracy: 0.5938 - val_loss: 1.2468 - val_accuracy: 0.5540
Epoch 17/20
450/450 [==============================] - 7s 16ms/step - loss: 1.0721 - accuracy: 0.6060 - val_loss: 1.3423 - val_accuracy: 0.5140
Epoch 18/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0506 - accuracy: 0.6147 - val_loss: 1.0784 - val_accuracy: 0.6120
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0364 - accuracy: 0.6187 - val_loss: 1.1621 - val_accuracy: 0.5620
Epoch 20/20
450/450 [==============================] - 7s 16ms/step - loss: 1.0056 - accuracy: 0.6353 - val_loss: 1.0974 - val_accuracy: 0.6100
Epoch 1/20
450/450 [==============================] - 12s 15ms/step - loss: 2.3543 - accuracy: 0.2404 - val_loss: 1.7775 - val_accuracy: 0.3380
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 1.9088 - accuracy: 0.3011 - val_loss: 2.1206 - val_accuracy: 0.3220
Epoch 3/20
450/450 [==============================] - 6s 13ms/step - loss: 1.7246 - accuracy: 0.3611 - val_loss: 2.2499 - val_accuracy: 0.3140
Epoch 4/20
450/450 [==============================] - 7s 15ms/step - loss: 1.6331 - accuracy: 0.3867 - val_loss: 1.7057 - val_accuracy: 0.3740
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5617 - accuracy: 0.4316 - val_loss: 1.3836 - val_accuracy: 0.4820
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5051 - accuracy: 0.4416 - val_loss: 1.7223 - val_accuracy: 0.3540
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4299 - accuracy: 0.4718 - val_loss: 1.6971 - val_accuracy: 0.4160
Epoch 8/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3887 - accuracy: 0.4827 - val_loss: 1.8132 - val_accuracy: 0.4420
Epoch 9/20
450/450 [==============================] - 6s 13ms/step - loss: 1.3512 - accuracy: 0.4962 - val_loss: 1.3524 - val_accuracy: 0.5100
Epoch 10/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2973 - accuracy: 0.5198 - val_loss: 1.2045 - val_accuracy: 0.5720
Epoch 11/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2749 - accuracy: 0.5360 - val_loss: 1.1673 - val_accuracy: 0.5820
Epoch 12/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2507 - accuracy: 0.5391 - val_loss: 1.6164 - val_accuracy: 0.4500
Epoch 13/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2225 - accuracy: 0.5398 - val_loss: 1.3208 - val_accuracy: 0.5320
Epoch 14/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1879 - accuracy: 0.5640 - val_loss: 1.4733 - val_accuracy: 0.4800
Epoch 15/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1834 - accuracy: 0.5671 - val_loss: 1.2811 - val_accuracy: 0.5500
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1496 - accuracy: 0.5816 - val_loss: 1.4910 - val_accuracy: 0.5220
Epoch 17/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1355 - accuracy: 0.5838 - val_loss: 1.2061 - val_accuracy: 0.5680
Epoch 18/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1049 - accuracy: 0.6051 - val_loss: 2.0394 - val_accuracy: 0.4100
Epoch 19/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0898 - accuracy: 0.5980 - val_loss: 1.4095 - val_accuracy: 0.5660
Epoch 20/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0848 - accuracy: 0.6013 - val_loss: 1.5566 - val_accuracy: 0.5040
Epoch 1/20
450/450 [==============================] - 11s 16ms/step - loss: 2.3601 - accuracy: 0.2380 - val_loss: 2.0943 - val_accuracy: 0.2640
Epoch 2/20
450/450 [==============================] - 6s 13ms/step - loss: 1.8501 - accuracy: 0.3280 - val_loss: 1.8725 - val_accuracy: 0.3560
Epoch 3/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6969 - accuracy: 0.3738 - val_loss: 1.6307 - val_accuracy: 0.3880
Epoch 4/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5782 - accuracy: 0.4171 - val_loss: 1.7486 - val_accuracy: 0.3700
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4969 - accuracy: 0.4387 - val_loss: 1.7330 - val_accuracy: 0.4400
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4361 - accuracy: 0.4642 - val_loss: 1.5843 - val_accuracy: 0.4240
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.3584 - accuracy: 0.4927 - val_loss: 1.6401 - val_accuracy: 0.4000
Epoch 8/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3349 - accuracy: 0.5042 - val_loss: 1.5856 - val_accuracy: 0.4600
Epoch 9/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2697 - accuracy: 0.5267 - val_loss: 1.3777 - val_accuracy: 0.5300
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2632 - accuracy: 0.5320 - val_loss: 1.5808 - val_accuracy: 0.4660
Epoch 11/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2074 - accuracy: 0.5533 - val_loss: 1.2768 - val_accuracy: 0.5340
Epoch 12/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1871 - accuracy: 0.5562 - val_loss: 2.7392 - val_accuracy: 0.3100
Epoch 13/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1634 - accuracy: 0.5771 - val_loss: 1.1452 - val_accuracy: 0.5860
Epoch 14/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1338 - accuracy: 0.5884 - val_loss: 1.3755 - val_accuracy: 0.4960
Epoch 15/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1107 - accuracy: 0.5918 - val_loss: 1.3395 - val_accuracy: 0.5620
Epoch 16/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0927 - accuracy: 0.5991 - val_loss: 1.1603 - val_accuracy: 0.5940
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0623 - accuracy: 0.6071 - val_loss: 1.6576 - val_accuracy: 0.4800
Epoch 18/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0547 - accuracy: 0.6171 - val_loss: 1.1621 - val_accuracy: 0.5960
Epoch 19/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0483 - accuracy: 0.6251 - val_loss: 2.0975 - val_accuracy: 0.4400
Epoch 20/20
450/450 [==============================] - 6s 13ms/step - loss: 1.0329 - accuracy: 0.6180 - val_loss: 1.2791 - val_accuracy: 0.5660
Epoch 1/20
450/450 [==============================] - 13s 16ms/step - loss: 2.3083 - accuracy: 0.2327 - val_loss: 1.8645 - val_accuracy: 0.2600
Epoch 2/20
450/450 [==============================] - 6s 14ms/step - loss: 1.9012 - accuracy: 0.3202 - val_loss: 2.0664 - val_accuracy: 0.3040
Epoch 3/20
450/450 [==============================] - 6s 13ms/step - loss: 1.7378 - accuracy: 0.3571 - val_loss: 2.1954 - val_accuracy: 0.2720
Epoch 4/20
450/450 [==============================] - 7s 15ms/step - loss: 1.6359 - accuracy: 0.3947 - val_loss: 1.7279 - val_accuracy: 0.3760
Epoch 5/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5506 - accuracy: 0.4178 - val_loss: 1.7001 - val_accuracy: 0.4560
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5031 - accuracy: 0.4451 - val_loss: 1.4683 - val_accuracy: 0.4460
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4313 - accuracy: 0.4680 - val_loss: 2.4538 - val_accuracy: 0.3160
Epoch 8/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3960 - accuracy: 0.4860 - val_loss: 2.1260 - val_accuracy: 0.3120
Epoch 9/20
450/450 [==============================] - 6s 13ms/step - loss: 1.3404 - accuracy: 0.5022 - val_loss: 1.2735 - val_accuracy: 0.5420
Epoch 10/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2904 - accuracy: 0.5251 - val_loss: 1.5240 - val_accuracy: 0.4600
Epoch 11/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2704 - accuracy: 0.5240 - val_loss: 1.4069 - val_accuracy: 0.5180
Epoch 12/20
450/450 [==============================] - 7s 15ms/step - loss: 1.2546 - accuracy: 0.5364 - val_loss: 1.2891 - val_accuracy: 0.5320
Epoch 13/20
450/450 [==============================] - 6s 13ms/step - loss: 1.2035 - accuracy: 0.5569 - val_loss: 1.1302 - val_accuracy: 0.5820
Epoch 14/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1805 - accuracy: 0.5624 - val_loss: 1.1863 - val_accuracy: 0.5980
Epoch 15/20
450/450 [==============================] - 6s 14ms/step - loss: 1.1629 - accuracy: 0.5682 - val_loss: 1.2411 - val_accuracy: 0.5780
Epoch 16/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1413 - accuracy: 0.5811 - val_loss: 1.1660 - val_accuracy: 0.5820
Epoch 17/20
450/450 [==============================] - 7s 15ms/step - loss: 1.1285 - accuracy: 0.5882 - val_loss: 1.1493 - val_accuracy: 0.5960
Epoch 18/20
450/450 [==============================] - 6s 13ms/step - loss: 1.1087 - accuracy: 0.5942 - val_loss: 1.5144 - val_accuracy: 0.4960
Epoch 19/20
450/450 [==============================] - 7s 15ms/step - loss: 1.0755 - accuracy: 0.6062 - val_loss: 1.2106 - val_accuracy: 0.5820
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 1.0645 - accuracy: 0.6087 - val_loss: 1.1192 - val_accuracy: 0.6040
Epoch 1/20
450/450 [==============================] - 10s 17ms/step - loss: 2.5665 - accuracy: 0.2004 - val_loss: 2.7995 - val_accuracy: 0.1320
Epoch 2/20
450/450 [==============================] - 6s 13ms/step - loss: 2.1599 - accuracy: 0.2693 - val_loss: 1.6630 - val_accuracy: 0.4000
Epoch 3/20
450/450 [==============================] - 7s 15ms/step - loss: 2.0276 - accuracy: 0.2713 - val_loss: 1.7291 - val_accuracy: 0.3760
Epoch 4/20
450/450 [==============================] - 6s 13ms/step - loss: 1.8972 - accuracy: 0.3136 - val_loss: 1.7352 - val_accuracy: 0.3560
Epoch 5/20
450/450 [==============================] - 7s 15ms/step - loss: 1.8038 - accuracy: 0.3362 - val_loss: 1.6340 - val_accuracy: 0.4040
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7673 - accuracy: 0.3569 - val_loss: 2.5412 - val_accuracy: 0.2460
Epoch 7/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7641 - accuracy: 0.3571 - val_loss: 1.5293 - val_accuracy: 0.4240
Epoch 8/20
450/450 [==============================] - 6s 13ms/step - loss: 1.7103 - accuracy: 0.3671 - val_loss: 1.6540 - val_accuracy: 0.3840
Epoch 9/20
450/450 [==============================] - 7s 15ms/step - loss: 1.6678 - accuracy: 0.3769 - val_loss: 1.4944 - val_accuracy: 0.4340
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6567 - accuracy: 0.3891 - val_loss: 1.6068 - val_accuracy: 0.3820
Epoch 11/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6441 - accuracy: 0.3880 - val_loss: 1.6601 - val_accuracy: 0.3780
Epoch 12/20
450/450 [==============================] - 7s 15ms/step - loss: 1.6332 - accuracy: 0.3947 - val_loss: 1.5207 - val_accuracy: 0.3900
Epoch 13/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6144 - accuracy: 0.3969 - val_loss: 1.4486 - val_accuracy: 0.4720
Epoch 14/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5744 - accuracy: 0.4033 - val_loss: 1.7728 - val_accuracy: 0.3720
Epoch 15/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5797 - accuracy: 0.4164 - val_loss: 1.5218 - val_accuracy: 0.4240
Epoch 16/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5662 - accuracy: 0.4129 - val_loss: 1.5139 - val_accuracy: 0.4200
Epoch 17/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5360 - accuracy: 0.4260 - val_loss: 1.4518 - val_accuracy: 0.4660
Epoch 18/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5035 - accuracy: 0.4391 - val_loss: 1.4066 - val_accuracy: 0.4580
Epoch 19/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5192 - accuracy: 0.4420 - val_loss: 1.6251 - val_accuracy: 0.4160
Epoch 20/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4908 - accuracy: 0.4498 - val_loss: 1.5508 - val_accuracy: 0.4220
Epoch 1/20
450/450 [==============================] - 9s 14ms/step - loss: 2.5868 - accuracy: 0.2100 - val_loss: 2.1528 - val_accuracy: 0.1960
Epoch 2/20
450/450 [==============================] - 6s 13ms/step - loss: 2.2179 - accuracy: 0.2629 - val_loss: 1.6317 - val_accuracy: 0.3780
Epoch 3/20
450/450 [==============================] - 7s 15ms/step - loss: 1.9905 - accuracy: 0.2991 - val_loss: 1.7236 - val_accuracy: 0.3360
Epoch 4/20
450/450 [==============================] - 6s 13ms/step - loss: 1.8963 - accuracy: 0.3136 - val_loss: 1.7423 - val_accuracy: 0.3460
Epoch 5/20
450/450 [==============================] - 7s 15ms/step - loss: 1.8472 - accuracy: 0.3282 - val_loss: 1.5677 - val_accuracy: 0.4000
Epoch 6/20
450/450 [==============================] - 6s 13ms/step - loss: 1.7843 - accuracy: 0.3484 - val_loss: 1.7485 - val_accuracy: 0.3920
Epoch 7/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7513 - accuracy: 0.3647 - val_loss: 1.7023 - val_accuracy: 0.3800
Epoch 8/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6989 - accuracy: 0.3642 - val_loss: 1.7353 - val_accuracy: 0.3620
Epoch 9/20
450/450 [==============================] - 7s 15ms/step - loss: 1.6759 - accuracy: 0.3716 - val_loss: 1.7685 - val_accuracy: 0.3800
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6589 - accuracy: 0.3851 - val_loss: 1.3760 - val_accuracy: 0.4920
Epoch 11/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6415 - accuracy: 0.3884 - val_loss: 1.4699 - val_accuracy: 0.4480
Epoch 12/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6010 - accuracy: 0.4104 - val_loss: 1.4315 - val_accuracy: 0.4620
Epoch 13/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5951 - accuracy: 0.4011 - val_loss: 1.4784 - val_accuracy: 0.4180
Epoch 14/20
450/450 [==============================] - 7s 14ms/step - loss: 1.5801 - accuracy: 0.4069 - val_loss: 1.4735 - val_accuracy: 0.4640
Epoch 15/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5677 - accuracy: 0.4193 - val_loss: 1.3822 - val_accuracy: 0.4620
Epoch 16/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5554 - accuracy: 0.4267 - val_loss: 1.3892 - val_accuracy: 0.4860
Epoch 17/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5514 - accuracy: 0.4191 - val_loss: 1.3401 - val_accuracy: 0.5080
Epoch 18/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5021 - accuracy: 0.4349 - val_loss: 1.3720 - val_accuracy: 0.5160
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4916 - accuracy: 0.4416 - val_loss: 1.3114 - val_accuracy: 0.5140
Epoch 20/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4967 - accuracy: 0.4387 - val_loss: 1.2931 - val_accuracy: 0.5140
Epoch 1/20
450/450 [==============================] - 10s 17ms/step - loss: 2.5633 - accuracy: 0.2144 - val_loss: 2.0449 - val_accuracy: 0.3020
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 2.1736 - accuracy: 0.2727 - val_loss: 1.6892 - val_accuracy: 0.3460
Epoch 3/20
450/450 [==============================] - 6s 13ms/step - loss: 2.0044 - accuracy: 0.3058 - val_loss: 2.1766 - val_accuracy: 0.2500
Epoch 4/20
450/450 [==============================] - 6s 13ms/step - loss: 1.8685 - accuracy: 0.3278 - val_loss: 1.5869 - val_accuracy: 0.3880
Epoch 5/20
450/450 [==============================] - 7s 15ms/step - loss: 1.8238 - accuracy: 0.3358 - val_loss: 1.7463 - val_accuracy: 0.3340
Epoch 6/20
450/450 [==============================] - 6s 13ms/step - loss: 1.7783 - accuracy: 0.3460 - val_loss: 1.7712 - val_accuracy: 0.3720
Epoch 7/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7539 - accuracy: 0.3636 - val_loss: 1.7977 - val_accuracy: 0.3500
Epoch 8/20
450/450 [==============================] - 6s 13ms/step - loss: 1.7213 - accuracy: 0.3693 - val_loss: 2.0569 - val_accuracy: 0.3560
Epoch 9/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6760 - accuracy: 0.3740 - val_loss: 1.4328 - val_accuracy: 0.4520
Epoch 10/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6529 - accuracy: 0.3871 - val_loss: 1.8422 - val_accuracy: 0.3800
Epoch 11/20
450/450 [==============================] - 7s 15ms/step - loss: 1.6437 - accuracy: 0.3967 - val_loss: 1.6570 - val_accuracy: 0.4060
Epoch 12/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6272 - accuracy: 0.3929 - val_loss: 1.5762 - val_accuracy: 0.4140
Epoch 13/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5855 - accuracy: 0.4127 - val_loss: 1.5994 - val_accuracy: 0.3940
Epoch 14/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5985 - accuracy: 0.4113 - val_loss: 1.6559 - val_accuracy: 0.4180
Epoch 15/20
450/450 [==============================] - 6s 13ms/step - loss: 1.5858 - accuracy: 0.4049 - val_loss: 1.4932 - val_accuracy: 0.4440
Epoch 16/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5470 - accuracy: 0.4116 - val_loss: 1.5915 - val_accuracy: 0.4500
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5313 - accuracy: 0.4236 - val_loss: 1.5504 - val_accuracy: 0.4400
Epoch 18/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4994 - accuracy: 0.4453 - val_loss: 1.4782 - val_accuracy: 0.4360
Epoch 19/20
450/450 [==============================] - 6s 13ms/step - loss: 1.4963 - accuracy: 0.4382 - val_loss: 1.6498 - val_accuracy: 0.4000
Epoch 20/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5019 - accuracy: 0.4404 - val_loss: 1.3240 - val_accuracy: 0.5000
Epoch 1/20
450/450 [==============================] - 13s 15ms/step - loss: 2.5475 - accuracy: 0.2322 - val_loss: 1.9661 - val_accuracy: 0.2860
Epoch 2/20
450/450 [==============================] - 7s 16ms/step - loss: 2.0037 - accuracy: 0.2927 - val_loss: 1.7624 - val_accuracy: 0.3040
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.8329 - accuracy: 0.3260 - val_loss: 1.6290 - val_accuracy: 0.3680
Epoch 4/20
450/450 [==============================] - 7s 16ms/step - loss: 1.7437 - accuracy: 0.3436 - val_loss: 1.7639 - val_accuracy: 0.3800
Epoch 5/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6700 - accuracy: 0.3731 - val_loss: 1.6377 - val_accuracy: 0.3880
Epoch 6/20
450/450 [==============================] - 7s 15ms/step - loss: 1.6099 - accuracy: 0.3949 - val_loss: 1.7196 - val_accuracy: 0.3780
Epoch 7/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6016 - accuracy: 0.3953 - val_loss: 1.5057 - val_accuracy: 0.4420
Epoch 8/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5423 - accuracy: 0.4182 - val_loss: 1.4811 - val_accuracy: 0.4620
Epoch 9/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5071 - accuracy: 0.4367 - val_loss: 1.5056 - val_accuracy: 0.4460
Epoch 10/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4809 - accuracy: 0.4460 - val_loss: 1.4777 - val_accuracy: 0.4440
Epoch 11/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4764 - accuracy: 0.4491 - val_loss: 1.5137 - val_accuracy: 0.4460
Epoch 12/20
450/450 [==============================] - 7s 14ms/step - loss: 1.4597 - accuracy: 0.4536 - val_loss: 1.3566 - val_accuracy: 0.4760
Epoch 13/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4153 - accuracy: 0.4813 - val_loss: 1.5391 - val_accuracy: 0.4540
Epoch 14/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3857 - accuracy: 0.4780 - val_loss: 1.6236 - val_accuracy: 0.3980
Epoch 15/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3833 - accuracy: 0.4831 - val_loss: 1.5213 - val_accuracy: 0.4600
Epoch 16/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3632 - accuracy: 0.4909 - val_loss: 1.2884 - val_accuracy: 0.5280
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3263 - accuracy: 0.5042 - val_loss: 1.3422 - val_accuracy: 0.5140
Epoch 18/20
450/450 [==============================] - 7s 16ms/step - loss: 1.2948 - accuracy: 0.5133 - val_loss: 1.5975 - val_accuracy: 0.4600
Epoch 19/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3082 - accuracy: 0.5164 - val_loss: 1.3008 - val_accuracy: 0.5180
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 1.2771 - accuracy: 0.5191 - val_loss: 1.0809 - val_accuracy: 0.6100
Epoch 1/20
450/450 [==============================] - 13s 17ms/step - loss: 2.5657 - accuracy: 0.2100 - val_loss: 2.5462 - val_accuracy: 0.1740
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 2.0054 - accuracy: 0.2873 - val_loss: 2.4650 - val_accuracy: 0.2320
Epoch 3/20
450/450 [==============================] - 7s 16ms/step - loss: 1.8632 - accuracy: 0.3136 - val_loss: 1.7847 - val_accuracy: 0.3580
Epoch 4/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7553 - accuracy: 0.3382 - val_loss: 1.5284 - val_accuracy: 0.4060
Epoch 5/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7323 - accuracy: 0.3460 - val_loss: 1.4928 - val_accuracy: 0.4180
Epoch 6/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6570 - accuracy: 0.3769 - val_loss: 1.8045 - val_accuracy: 0.3000
Epoch 7/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6327 - accuracy: 0.3898 - val_loss: 1.3850 - val_accuracy: 0.4760
Epoch 8/20
450/450 [==============================] - 7s 14ms/step - loss: 1.5809 - accuracy: 0.4078 - val_loss: 1.7877 - val_accuracy: 0.3380
Epoch 9/20
450/450 [==============================] - 7s 14ms/step - loss: 1.5444 - accuracy: 0.4078 - val_loss: 1.4520 - val_accuracy: 0.4520
Epoch 10/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5166 - accuracy: 0.4187 - val_loss: 1.3696 - val_accuracy: 0.5100
Epoch 11/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4916 - accuracy: 0.4422 - val_loss: 1.5882 - val_accuracy: 0.4400
Epoch 12/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4805 - accuracy: 0.4471 - val_loss: 1.3564 - val_accuracy: 0.4980
Epoch 13/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4377 - accuracy: 0.4627 - val_loss: 1.5282 - val_accuracy: 0.4320
Epoch 14/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4299 - accuracy: 0.4609 - val_loss: 1.6250 - val_accuracy: 0.4520
Epoch 15/20
450/450 [==============================] - 7s 17ms/step - loss: 1.4097 - accuracy: 0.4822 - val_loss: 1.3405 - val_accuracy: 0.5060
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3898 - accuracy: 0.4829 - val_loss: 1.6742 - val_accuracy: 0.4020
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3480 - accuracy: 0.4962 - val_loss: 1.2651 - val_accuracy: 0.5220
Epoch 18/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3302 - accuracy: 0.5049 - val_loss: 1.4665 - val_accuracy: 0.4440
Epoch 19/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3113 - accuracy: 0.5200 - val_loss: 1.1807 - val_accuracy: 0.5600
Epoch 20/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3057 - accuracy: 0.5149 - val_loss: 1.4363 - val_accuracy: 0.5060
Epoch 1/20
450/450 [==============================] - 14s 15ms/step - loss: 2.5873 - accuracy: 0.1973 - val_loss: 2.6974 - val_accuracy: 0.2340
Epoch 2/20
450/450 [==============================] - 7s 16ms/step - loss: 2.0393 - accuracy: 0.2764 - val_loss: 1.9451 - val_accuracy: 0.2580
Epoch 3/20
450/450 [==============================] - 7s 16ms/step - loss: 1.8640 - accuracy: 0.3107 - val_loss: 1.6483 - val_accuracy: 0.3540
Epoch 4/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7776 - accuracy: 0.3322 - val_loss: 1.5553 - val_accuracy: 0.3860
Epoch 5/20
450/450 [==============================] - 8s 17ms/step - loss: 1.7089 - accuracy: 0.3567 - val_loss: 2.2110 - val_accuracy: 0.2840
Epoch 6/20
450/450 [==============================] - 7s 17ms/step - loss: 1.6678 - accuracy: 0.3760 - val_loss: 1.6545 - val_accuracy: 0.3700
Epoch 7/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6316 - accuracy: 0.3891 - val_loss: 1.6232 - val_accuracy: 0.4020
Epoch 8/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5892 - accuracy: 0.4027 - val_loss: 1.4825 - val_accuracy: 0.4700
Epoch 9/20
450/450 [==============================] - 7s 17ms/step - loss: 1.5640 - accuracy: 0.4118 - val_loss: 1.9248 - val_accuracy: 0.3960
Epoch 10/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5277 - accuracy: 0.4244 - val_loss: 1.5967 - val_accuracy: 0.3860
Epoch 11/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4979 - accuracy: 0.4462 - val_loss: 1.3474 - val_accuracy: 0.5100
Epoch 12/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4672 - accuracy: 0.4422 - val_loss: 1.3908 - val_accuracy: 0.4540
Epoch 13/20
450/450 [==============================] - 7s 17ms/step - loss: 1.4392 - accuracy: 0.4698 - val_loss: 1.3484 - val_accuracy: 0.5020
Epoch 14/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4286 - accuracy: 0.4560 - val_loss: 1.3638 - val_accuracy: 0.4860
Epoch 15/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4078 - accuracy: 0.4756 - val_loss: 1.3330 - val_accuracy: 0.4920
Epoch 16/20
450/450 [==============================] - 7s 17ms/step - loss: 1.3776 - accuracy: 0.4820 - val_loss: 1.4396 - val_accuracy: 0.4880
Epoch 17/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3573 - accuracy: 0.4913 - val_loss: 1.1705 - val_accuracy: 0.5580
Epoch 18/20
450/450 [==============================] - 8s 17ms/step - loss: 1.3345 - accuracy: 0.4984 - val_loss: 1.2380 - val_accuracy: 0.5140
Epoch 19/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3246 - accuracy: 0.5036 - val_loss: 1.3344 - val_accuracy: 0.5100
Epoch 20/20
450/450 [==============================] - 7s 15ms/step - loss: 1.3069 - accuracy: 0.5136 - val_loss: 1.4068 - val_accuracy: 0.4900
Epoch 1/20
450/450 [==============================] - 12s 17ms/step - loss: 2.5791 - accuracy: 0.2207 - val_loss: 1.8342 - val_accuracy: 0.3040
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 2.0722 - accuracy: 0.2864 - val_loss: 2.2624 - val_accuracy: 0.2480
Epoch 3/20
450/450 [==============================] - 7s 16ms/step - loss: 1.8987 - accuracy: 0.3171 - val_loss: 2.6109 - val_accuracy: 0.2660
Epoch 4/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7938 - accuracy: 0.3362 - val_loss: 1.6085 - val_accuracy: 0.3980
Epoch 5/20
450/450 [==============================] - 7s 16ms/step - loss: 1.7234 - accuracy: 0.3649 - val_loss: 1.5861 - val_accuracy: 0.4420
Epoch 6/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6852 - accuracy: 0.3780 - val_loss: 1.5338 - val_accuracy: 0.4000
Epoch 7/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6515 - accuracy: 0.3909 - val_loss: 1.5358 - val_accuracy: 0.4180
Epoch 8/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6000 - accuracy: 0.4051 - val_loss: 1.4676 - val_accuracy: 0.4420
Epoch 9/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5683 - accuracy: 0.4142 - val_loss: 1.4970 - val_accuracy: 0.4600
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5530 - accuracy: 0.4240 - val_loss: 1.5933 - val_accuracy: 0.3920
Epoch 11/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5369 - accuracy: 0.4238 - val_loss: 1.4796 - val_accuracy: 0.4700
Epoch 12/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5044 - accuracy: 0.4396 - val_loss: 1.5923 - val_accuracy: 0.4040
Epoch 13/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4762 - accuracy: 0.4464 - val_loss: 1.3921 - val_accuracy: 0.4740
Epoch 14/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4877 - accuracy: 0.4509 - val_loss: 1.2703 - val_accuracy: 0.5260
Epoch 15/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4654 - accuracy: 0.4602 - val_loss: 1.2739 - val_accuracy: 0.5120
Epoch 16/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4482 - accuracy: 0.4549 - val_loss: 1.4724 - val_accuracy: 0.4960
Epoch 17/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4216 - accuracy: 0.4776 - val_loss: 1.2853 - val_accuracy: 0.5240
Epoch 18/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4066 - accuracy: 0.4816 - val_loss: 2.5824 - val_accuracy: 0.3540
Epoch 19/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3969 - accuracy: 0.4782 - val_loss: 1.3408 - val_accuracy: 0.5160
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3933 - accuracy: 0.4902 - val_loss: 1.2774 - val_accuracy: 0.5500
Epoch 1/20
450/450 [==============================] - 12s 16ms/step - loss: 2.5922 - accuracy: 0.2224 - val_loss: 1.9549 - val_accuracy: 0.2660
Epoch 2/20
450/450 [==============================] - 7s 16ms/step - loss: 2.0475 - accuracy: 0.2849 - val_loss: 1.9337 - val_accuracy: 0.3220
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.8982 - accuracy: 0.3087 - val_loss: 1.6656 - val_accuracy: 0.3800
Epoch 4/20
450/450 [==============================] - 7s 16ms/step - loss: 1.8048 - accuracy: 0.3402 - val_loss: 2.5178 - val_accuracy: 0.2320
Epoch 5/20
450/450 [==============================] - 6s 14ms/step - loss: 1.7365 - accuracy: 0.3658 - val_loss: 3.3059 - val_accuracy: 0.1440
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6624 - accuracy: 0.3840 - val_loss: 1.9980 - val_accuracy: 0.3060
Epoch 7/20
450/450 [==============================] - 7s 16ms/step - loss: 1.6347 - accuracy: 0.3853 - val_loss: 2.1807 - val_accuracy: 0.2860
Epoch 8/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5959 - accuracy: 0.4122 - val_loss: 1.5730 - val_accuracy: 0.4040
Epoch 9/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5646 - accuracy: 0.4184 - val_loss: 1.4593 - val_accuracy: 0.4920
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5366 - accuracy: 0.4271 - val_loss: 2.6299 - val_accuracy: 0.3360
Epoch 11/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5113 - accuracy: 0.4444 - val_loss: 2.8390 - val_accuracy: 0.2900
Epoch 12/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4816 - accuracy: 0.4518 - val_loss: 1.3703 - val_accuracy: 0.4900
Epoch 13/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4534 - accuracy: 0.4611 - val_loss: 2.0454 - val_accuracy: 0.3460
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4404 - accuracy: 0.4598 - val_loss: 1.3010 - val_accuracy: 0.5260
Epoch 15/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4310 - accuracy: 0.4793 - val_loss: 1.3318 - val_accuracy: 0.5240
Epoch 16/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4079 - accuracy: 0.4702 - val_loss: 1.3448 - val_accuracy: 0.5280
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3968 - accuracy: 0.4804 - val_loss: 1.3468 - val_accuracy: 0.5020
Epoch 18/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3760 - accuracy: 0.4844 - val_loss: 1.1821 - val_accuracy: 0.5540
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3759 - accuracy: 0.4938 - val_loss: 1.5980 - val_accuracy: 0.4240
Epoch 20/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3675 - accuracy: 0.4978 - val_loss: 1.8367 - val_accuracy: 0.3760
Epoch 1/20
450/450 [==============================] - 12s 15ms/step - loss: 2.5707 - accuracy: 0.2224 - val_loss: 4.9271 - val_accuracy: 0.1920
Epoch 2/20
450/450 [==============================] - 7s 15ms/step - loss: 2.0397 - accuracy: 0.2838 - val_loss: 2.7546 - val_accuracy: 0.2740
Epoch 3/20
450/450 [==============================] - 6s 14ms/step - loss: 1.8596 - accuracy: 0.3176 - val_loss: 1.7542 - val_accuracy: 0.3240
Epoch 4/20
450/450 [==============================] - 7s 16ms/step - loss: 1.7611 - accuracy: 0.3373 - val_loss: 1.7150 - val_accuracy: 0.3300
Epoch 5/20
450/450 [==============================] - 7s 15ms/step - loss: 1.7109 - accuracy: 0.3618 - val_loss: 1.8955 - val_accuracy: 0.3340
Epoch 6/20
450/450 [==============================] - 6s 14ms/step - loss: 1.6648 - accuracy: 0.3858 - val_loss: 1.4522 - val_accuracy: 0.4320
Epoch 7/20
450/450 [==============================] - 6s 13ms/step - loss: 1.6135 - accuracy: 0.3918 - val_loss: 1.5633 - val_accuracy: 0.3880
Epoch 8/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5989 - accuracy: 0.4089 - val_loss: 1.4988 - val_accuracy: 0.4460
Epoch 9/20
450/450 [==============================] - 7s 15ms/step - loss: 1.5539 - accuracy: 0.4207 - val_loss: 1.5793 - val_accuracy: 0.4220
Epoch 10/20
450/450 [==============================] - 6s 14ms/step - loss: 1.5289 - accuracy: 0.4300 - val_loss: 1.8832 - val_accuracy: 0.3600
Epoch 11/20
450/450 [==============================] - 7s 16ms/step - loss: 1.5053 - accuracy: 0.4380 - val_loss: 2.0407 - val_accuracy: 0.3360
Epoch 12/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4815 - accuracy: 0.4473 - val_loss: 2.8356 - val_accuracy: 0.3020
Epoch 13/20
450/450 [==============================] - 7s 15ms/step - loss: 1.4644 - accuracy: 0.4593 - val_loss: 1.5464 - val_accuracy: 0.4240
Epoch 14/20
450/450 [==============================] - 6s 14ms/step - loss: 1.4515 - accuracy: 0.4553 - val_loss: 1.2489 - val_accuracy: 0.5260
Epoch 15/20
450/450 [==============================] - 7s 16ms/step - loss: 1.4290 - accuracy: 0.4724 - val_loss: 1.3826 - val_accuracy: 0.5120
Epoch 16/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3940 - accuracy: 0.4822 - val_loss: 1.4307 - val_accuracy: 0.4900
Epoch 17/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3936 - accuracy: 0.4793 - val_loss: 1.3970 - val_accuracy: 0.4600
Epoch 18/20
450/450 [==============================] - 7s 16ms/step - loss: 1.3632 - accuracy: 0.4960 - val_loss: 1.3327 - val_accuracy: 0.5140
Epoch 19/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3593 - accuracy: 0.5033 - val_loss: 1.2468 - val_accuracy: 0.5280
Epoch 20/20
450/450 [==============================] - 6s 14ms/step - loss: 1.3635 - accuracy: 0.5027 - val_loss: 1.3052 - val_accuracy: 0.5040
Best Parameters: {&#39;drop_rate&#39;: 0.5, &#39;optimizer&#39;: &#39;rmsprop&#39;, &#39;alpha&#39;: 0.5}
Best Validation Performance: 0
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="u3iMnS8tAQYJ" data-outputId="826184a3-5570-450a-cf04-41b6d8e8dfa5">
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(best_val_performance)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>0.527999997138977
</code></pre>
</div>
</div>
<div class="cell markdown" id="LGDOFJ2mApSi">
<p>parameters: {'drop_rate': 0.5, 'optimizer': 'rmsprop', 'alpha': 0.5}
validation accuracy:0.528</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="84BJSTuwA29i" data-outputId="672835ee-33c5-443b-b2b2-ef09060b456a">
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>model_3_7_3 <span class="op">=</span> YourModel(name<span class="op">=</span><span class="st">&#39;q3_7_3&#39;</span>,</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>                              feature_maps<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>                              num_classes<span class="op">=</span>data_manager.n_classes,</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>                              num_blocks<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>                              drop_rate<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>                              batch_norm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>                              is_augmentation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>                              use_skip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>                              optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>,</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>                              num_epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>                              learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Build and train the model</span></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>model_3_7_3.build_cnn()</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>model_3_7_3.fit(data_manager, use_mixup<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/100
450/450 [==============================] - 10s 13ms/step - loss: 2.5998 - accuracy: 0.2149 - val_loss: 2.2869 - val_accuracy: 0.2760
Epoch 2/100
450/450 [==============================] - 6s 14ms/step - loss: 2.0635 - accuracy: 0.2860 - val_loss: 4.0022 - val_accuracy: 0.2400
Epoch 3/100
450/450 [==============================] - 6s 12ms/step - loss: 1.9129 - accuracy: 0.3156 - val_loss: 2.3024 - val_accuracy: 0.3080
Epoch 4/100
450/450 [==============================] - 6s 14ms/step - loss: 1.7950 - accuracy: 0.3300 - val_loss: 1.6124 - val_accuracy: 0.3740
Epoch 5/100
450/450 [==============================] - 6s 12ms/step - loss: 1.7564 - accuracy: 0.3529 - val_loss: 1.6863 - val_accuracy: 0.3440
Epoch 6/100
450/450 [==============================] - 6s 13ms/step - loss: 1.6869 - accuracy: 0.3771 - val_loss: 1.5898 - val_accuracy: 0.3840
Epoch 7/100
450/450 [==============================] - 6s 14ms/step - loss: 1.6699 - accuracy: 0.3789 - val_loss: 1.5121 - val_accuracy: 0.4100
Epoch 8/100
450/450 [==============================] - 6s 13ms/step - loss: 1.6155 - accuracy: 0.4049 - val_loss: 1.9935 - val_accuracy: 0.3580
Epoch 9/100
450/450 [==============================] - 6s 12ms/step - loss: 1.5722 - accuracy: 0.4047 - val_loss: 2.1283 - val_accuracy: 0.2840
Epoch 10/100
450/450 [==============================] - 6s 14ms/step - loss: 1.5400 - accuracy: 0.4204 - val_loss: 1.4777 - val_accuracy: 0.4460
Epoch 11/100
450/450 [==============================] - 6s 12ms/step - loss: 1.5519 - accuracy: 0.4187 - val_loss: 1.4593 - val_accuracy: 0.4300
Epoch 12/100
450/450 [==============================] - 7s 15ms/step - loss: 1.5240 - accuracy: 0.4273 - val_loss: 2.0205 - val_accuracy: 0.3300
Epoch 13/100
450/450 [==============================] - 6s 13ms/step - loss: 1.4983 - accuracy: 0.4409 - val_loss: 1.7936 - val_accuracy: 0.3660
Epoch 14/100
450/450 [==============================] - 7s 15ms/step - loss: 1.4722 - accuracy: 0.4547 - val_loss: 3.5521 - val_accuracy: 0.2660
Epoch 15/100
450/450 [==============================] - 6s 12ms/step - loss: 1.4589 - accuracy: 0.4660 - val_loss: 1.4798 - val_accuracy: 0.4680
Epoch 16/100
450/450 [==============================] - 6s 14ms/step - loss: 1.4256 - accuracy: 0.4616 - val_loss: 1.3579 - val_accuracy: 0.4800
Epoch 17/100
450/450 [==============================] - 6s 13ms/step - loss: 1.4090 - accuracy: 0.4682 - val_loss: 1.2221 - val_accuracy: 0.5460
Epoch 18/100
450/450 [==============================] - 7s 15ms/step - loss: 1.4117 - accuracy: 0.4831 - val_loss: 1.7445 - val_accuracy: 0.3980
Epoch 19/100
450/450 [==============================] - 6s 12ms/step - loss: 1.3881 - accuracy: 0.4824 - val_loss: 1.3782 - val_accuracy: 0.4720
Epoch 20/100
450/450 [==============================] - 6s 14ms/step - loss: 1.3933 - accuracy: 0.4800 - val_loss: 1.4160 - val_accuracy: 0.4960
Epoch 21/100
450/450 [==============================] - 7s 15ms/step - loss: 1.3732 - accuracy: 0.4982 - val_loss: 1.3235 - val_accuracy: 0.5220
Epoch 22/100
450/450 [==============================] - 6s 12ms/step - loss: 1.3691 - accuracy: 0.5000 - val_loss: 1.4279 - val_accuracy: 0.4780
Epoch 23/100
450/450 [==============================] - 6s 14ms/step - loss: 1.3445 - accuracy: 0.5056 - val_loss: 1.1892 - val_accuracy: 0.5640
Epoch 24/100
450/450 [==============================] - 6s 12ms/step - loss: 1.3320 - accuracy: 0.5089 - val_loss: 1.4513 - val_accuracy: 0.4880
Epoch 25/100
450/450 [==============================] - 7s 15ms/step - loss: 1.3317 - accuracy: 0.5104 - val_loss: 1.3240 - val_accuracy: 0.5220
Epoch 26/100
450/450 [==============================] - 6s 12ms/step - loss: 1.3116 - accuracy: 0.5207 - val_loss: 1.2255 - val_accuracy: 0.5340
Epoch 27/100
450/450 [==============================] - 7s 15ms/step - loss: 1.3161 - accuracy: 0.5162 - val_loss: 1.2525 - val_accuracy: 0.5180
Epoch 28/100
450/450 [==============================] - 6s 13ms/step - loss: 1.3053 - accuracy: 0.5180 - val_loss: 1.1635 - val_accuracy: 0.5980
Epoch 29/100
450/450 [==============================] - 7s 15ms/step - loss: 1.2903 - accuracy: 0.5229 - val_loss: 1.1538 - val_accuracy: 0.6000
Epoch 30/100
450/450 [==============================] - 6s 12ms/step - loss: 1.2791 - accuracy: 0.5298 - val_loss: 1.7265 - val_accuracy: 0.4480
Epoch 31/100
450/450 [==============================] - 6s 14ms/step - loss: 1.2914 - accuracy: 0.5269 - val_loss: 1.2148 - val_accuracy: 0.5680
Epoch 32/100
450/450 [==============================] - 6s 13ms/step - loss: 1.2689 - accuracy: 0.5300 - val_loss: 1.1514 - val_accuracy: 0.5920
Epoch 33/100
450/450 [==============================] - 6s 14ms/step - loss: 1.2539 - accuracy: 0.5376 - val_loss: 1.2365 - val_accuracy: 0.5460
Epoch 34/100
450/450 [==============================] - 6s 12ms/step - loss: 1.2408 - accuracy: 0.5418 - val_loss: 1.4547 - val_accuracy: 0.5100
Epoch 35/100
450/450 [==============================] - 7s 14ms/step - loss: 1.2354 - accuracy: 0.5482 - val_loss: 1.2517 - val_accuracy: 0.5640
Epoch 36/100
450/450 [==============================] - 6s 12ms/step - loss: 1.2478 - accuracy: 0.5431 - val_loss: 1.1473 - val_accuracy: 0.5820
Epoch 37/100
450/450 [==============================] - 7s 14ms/step - loss: 1.2325 - accuracy: 0.5604 - val_loss: 1.2154 - val_accuracy: 0.5660
Epoch 38/100
450/450 [==============================] - 5s 12ms/step - loss: 1.2225 - accuracy: 0.5558 - val_loss: 1.2052 - val_accuracy: 0.5960
Epoch 39/100
450/450 [==============================] - 6s 14ms/step - loss: 1.2157 - accuracy: 0.5533 - val_loss: 1.1928 - val_accuracy: 0.5640
Epoch 40/100
450/450 [==============================] - 6s 13ms/step - loss: 1.2312 - accuracy: 0.5500 - val_loss: 1.1695 - val_accuracy: 0.5980
Epoch 41/100
450/450 [==============================] - 7s 15ms/step - loss: 1.1942 - accuracy: 0.5631 - val_loss: 1.1628 - val_accuracy: 0.5740
Epoch 42/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1753 - accuracy: 0.5796 - val_loss: 1.2074 - val_accuracy: 0.6160
Epoch 43/100
450/450 [==============================] - 6s 14ms/step - loss: 1.1893 - accuracy: 0.5649 - val_loss: 1.3253 - val_accuracy: 0.5360
Epoch 44/100
450/450 [==============================] - 6s 12ms/step - loss: 1.2337 - accuracy: 0.5627 - val_loss: 1.2316 - val_accuracy: 0.5600
Epoch 45/100
450/450 [==============================] - 7s 15ms/step - loss: 1.1773 - accuracy: 0.5716 - val_loss: 1.1840 - val_accuracy: 0.5620
Epoch 46/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1712 - accuracy: 0.5744 - val_loss: 1.1397 - val_accuracy: 0.6060
Epoch 47/100
450/450 [==============================] - 7s 15ms/step - loss: 1.1687 - accuracy: 0.5778 - val_loss: 1.1424 - val_accuracy: 0.6000
Epoch 48/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1828 - accuracy: 0.5693 - val_loss: 1.1638 - val_accuracy: 0.5740
Epoch 49/100
450/450 [==============================] - 7s 14ms/step - loss: 1.1581 - accuracy: 0.5771 - val_loss: 1.3094 - val_accuracy: 0.5580
Epoch 50/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1559 - accuracy: 0.5796 - val_loss: 1.2321 - val_accuracy: 0.5600
Epoch 51/100
450/450 [==============================] - 7s 15ms/step - loss: 1.1859 - accuracy: 0.5742 - val_loss: 1.2658 - val_accuracy: 0.5740
Epoch 52/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1670 - accuracy: 0.5853 - val_loss: 1.4662 - val_accuracy: 0.5040
Epoch 53/100
450/450 [==============================] - 6s 14ms/step - loss: 1.1625 - accuracy: 0.5833 - val_loss: 1.3327 - val_accuracy: 0.5740
Epoch 54/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1430 - accuracy: 0.5791 - val_loss: 1.2842 - val_accuracy: 0.5820
Epoch 55/100
450/450 [==============================] - 6s 14ms/step - loss: 1.1210 - accuracy: 0.5909 - val_loss: 1.6690 - val_accuracy: 0.5140
Epoch 56/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1395 - accuracy: 0.5891 - val_loss: 1.2477 - val_accuracy: 0.5900
Epoch 57/100
450/450 [==============================] - 7s 15ms/step - loss: 1.1416 - accuracy: 0.5884 - val_loss: 1.0695 - val_accuracy: 0.6340
Epoch 58/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1429 - accuracy: 0.5842 - val_loss: 1.2281 - val_accuracy: 0.5820
Epoch 59/100
450/450 [==============================] - 6s 14ms/step - loss: 1.1460 - accuracy: 0.5898 - val_loss: 1.0874 - val_accuracy: 0.6200
Epoch 60/100
450/450 [==============================] - 7s 15ms/step - loss: 1.1250 - accuracy: 0.5911 - val_loss: 1.1943 - val_accuracy: 0.5940
Epoch 61/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1216 - accuracy: 0.5862 - val_loss: 1.2558 - val_accuracy: 0.5500
Epoch 62/100
450/450 [==============================] - 7s 15ms/step - loss: 1.1145 - accuracy: 0.5944 - val_loss: 1.1502 - val_accuracy: 0.5840
Epoch 63/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1128 - accuracy: 0.6020 - val_loss: 1.3086 - val_accuracy: 0.5960
Epoch 64/100
450/450 [==============================] - 7s 14ms/step - loss: 1.1131 - accuracy: 0.5976 - val_loss: 1.2359 - val_accuracy: 0.5860
Epoch 65/100
450/450 [==============================] - 6s 12ms/step - loss: 1.1120 - accuracy: 0.5993 - val_loss: 1.1795 - val_accuracy: 0.5780
Epoch 66/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0987 - accuracy: 0.6058 - val_loss: 1.1759 - val_accuracy: 0.6120
Epoch 67/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1085 - accuracy: 0.6016 - val_loss: 1.0956 - val_accuracy: 0.6280
Epoch 68/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1028 - accuracy: 0.5987 - val_loss: 1.1208 - val_accuracy: 0.6080
Epoch 69/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1054 - accuracy: 0.5993 - val_loss: 1.1493 - val_accuracy: 0.6220
Epoch 70/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0805 - accuracy: 0.6076 - val_loss: 1.3525 - val_accuracy: 0.5880
Epoch 71/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1125 - accuracy: 0.5902 - val_loss: 1.2326 - val_accuracy: 0.6320
Epoch 72/100
450/450 [==============================] - 6s 13ms/step - loss: 1.1127 - accuracy: 0.5913 - val_loss: 1.0367 - val_accuracy: 0.6380
Epoch 73/100
450/450 [==============================] - 7s 15ms/step - loss: 1.0679 - accuracy: 0.6162 - val_loss: 1.2289 - val_accuracy: 0.6060
Epoch 74/100
450/450 [==============================] - 6s 12ms/step - loss: 1.0815 - accuracy: 0.6127 - val_loss: 1.1822 - val_accuracy: 0.5960
Epoch 75/100
450/450 [==============================] - 7s 14ms/step - loss: 1.0811 - accuracy: 0.6062 - val_loss: 1.0663 - val_accuracy: 0.6120
Epoch 76/100
450/450 [==============================] - 7s 15ms/step - loss: 1.0974 - accuracy: 0.6027 - val_loss: 1.0462 - val_accuracy: 0.6380
Epoch 77/100
450/450 [==============================] - 6s 12ms/step - loss: 1.0553 - accuracy: 0.6149 - val_loss: 1.2851 - val_accuracy: 0.5880
Epoch 78/100
450/450 [==============================] - 6s 14ms/step - loss: 1.0917 - accuracy: 0.6016 - val_loss: 1.0616 - val_accuracy: 0.6220
Epoch 79/100
450/450 [==============================] - 6s 12ms/step - loss: 1.0823 - accuracy: 0.6118 - val_loss: 1.1079 - val_accuracy: 0.6180
Epoch 80/100
450/450 [==============================] - 7s 15ms/step - loss: 1.0740 - accuracy: 0.6120 - val_loss: 1.2189 - val_accuracy: 0.5940
Epoch 81/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0808 - accuracy: 0.6113 - val_loss: 1.1286 - val_accuracy: 0.6260
Epoch 82/100
450/450 [==============================] - 7s 15ms/step - loss: 1.0804 - accuracy: 0.6113 - val_loss: 1.0766 - val_accuracy: 0.6340
Epoch 83/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0702 - accuracy: 0.6171 - val_loss: 1.4214 - val_accuracy: 0.5340
Epoch 84/100
450/450 [==============================] - 7s 15ms/step - loss: 1.0908 - accuracy: 0.6096 - val_loss: 1.4015 - val_accuracy: 0.5840
Epoch 85/100
450/450 [==============================] - 6s 12ms/step - loss: 1.0674 - accuracy: 0.6156 - val_loss: 1.1400 - val_accuracy: 0.6180
Epoch 86/100
450/450 [==============================] - 6s 14ms/step - loss: 1.0827 - accuracy: 0.6076 - val_loss: 1.0858 - val_accuracy: 0.6160
Epoch 87/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0504 - accuracy: 0.6213 - val_loss: 1.1134 - val_accuracy: 0.6120
Epoch 88/100
450/450 [==============================] - 6s 14ms/step - loss: 1.0759 - accuracy: 0.6116 - val_loss: 1.1505 - val_accuracy: 0.6260
Epoch 89/100
450/450 [==============================] - 6s 12ms/step - loss: 1.0474 - accuracy: 0.6278 - val_loss: 1.0894 - val_accuracy: 0.6600
Epoch 90/100
450/450 [==============================] - 6s 14ms/step - loss: 1.0892 - accuracy: 0.6160 - val_loss: 1.1571 - val_accuracy: 0.6100
Epoch 91/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0517 - accuracy: 0.6313 - val_loss: 1.0817 - val_accuracy: 0.6440
Epoch 92/100
450/450 [==============================] - 6s 14ms/step - loss: 1.0614 - accuracy: 0.6156 - val_loss: 1.4039 - val_accuracy: 0.6140
Epoch 93/100
450/450 [==============================] - 6s 12ms/step - loss: 1.0534 - accuracy: 0.6307 - val_loss: 1.2693 - val_accuracy: 0.5960
Epoch 94/100
450/450 [==============================] - 7s 15ms/step - loss: 1.0379 - accuracy: 0.6282 - val_loss: 1.1655 - val_accuracy: 0.6300
Epoch 95/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0436 - accuracy: 0.6209 - val_loss: 1.2218 - val_accuracy: 0.6020
Epoch 96/100
450/450 [==============================] - 6s 14ms/step - loss: 1.0629 - accuracy: 0.6291 - val_loss: 1.1238 - val_accuracy: 0.6240
Epoch 97/100
450/450 [==============================] - 6s 12ms/step - loss: 1.0338 - accuracy: 0.6287 - val_loss: 1.2165 - val_accuracy: 0.6160
Epoch 98/100
450/450 [==============================] - 6s 14ms/step - loss: 1.0401 - accuracy: 0.6320 - val_loss: 1.2140 - val_accuracy: 0.6200
Epoch 99/100
450/450 [==============================] - 6s 13ms/step - loss: 1.0427 - accuracy: 0.6313 - val_loss: 1.2121 - val_accuracy: 0.5840
Epoch 100/100
450/450 [==============================] - 7s 14ms/step - loss: 1.0303 - accuracy: 0.6429 - val_loss: 1.2009 - val_accuracy: 0.6360
</code></pre>
</div>
</div>
<div class="cell markdown" id="PsGe9vN4IKN7">
<p>The best model so far parameter is: num_block=4, dropout=0.5, alpha
=0.5,num_epoch=100,optimizer='rmsprop',learning_rate=0.001,use_skip=True,is_augmentation=True,mixup=True.
Validation accuracy=0.636</p>
</div>
<div class="cell code" id="-_Z0LcA5IvvM">
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>model_3_7_3.model.save(<span class="st">&quot;model_3_7_3.h5&quot;</span>)<span class="co">#save the model, and this is the best model found so far</span></span></code></pre></div>
</div>
<div class="cell code" id="z5XIZYk3JP7t">
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> load_model</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>loaded_model <span class="op">=</span> load_model(<span class="st">&quot;model_3_7_3.h5&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="oriSNHsjJ9-E" data-outputId="088991b8-364d-43ea-ce06-2d470de84f3e">
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>sample_dataset <span class="op">=</span> data_manager.ds_test.take(num_samples)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>model_3_7_3.predict(sample_dataset.batch(num_samples), data_manager.ds_info)<span class="co"># test the model on test dataset</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>1/1 [==============================] - 1s 641ms/step
Sample 1: Predicted label - car
Sample 2: Predicted label - cat
Sample 3: Predicted label - cat
Sample 4: Predicted label - bird
Sample 5: Predicted label - monkey
Sample 6: Predicted label - cat
Sample 7: Predicted label - cat
Sample 8: Predicted label - dog
Sample 9: Predicted label - horse
Sample 10: Predicted label - deer
Sample 11: Predicted label - deer
Sample 12: Predicted label - cat
Sample 13: Predicted label - dog
Sample 14: Predicted label - ship
Sample 15: Predicted label - deer
Sample 16: Predicted label - deer
Sample 17: Predicted label - dog
Sample 18: Predicted label - airplane
Sample 19: Predicted label - airplane
Sample 20: Predicted label - monkey
Sample 21: Predicted label - horse
Sample 22: Predicted label - dog
Sample 23: Predicted label - ship
Sample 24: Predicted label - airplane
Sample 25: Predicted label - airplane
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:522}"
id="Eq5yAxHEKUvo" data-outputId="11924490-1735-45cc-c4c3-b7325154c3ef">
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>sample_dataset <span class="op">=</span> data_manager.ds_test.take(num_samples)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>model_3_7_3.plot_predictions(sample_dataset, data_manager.ds_info, num_samples<span class="op">=</span>num_samples, grid_shape<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">5</span>))<span class="co">#visualize the model on test dataset</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>1/1 [==============================] - 0s 78ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_262e2868f6474e89852d78a2fd9920d2/06816db17cd07a5d5719ff75caba6aa5274cbce0.png" /></p>
</div>
</div>
<div class="cell markdown" id="X_j0qRjP7KgE">
<p>In conclusion: the best model found so far is: number of block:4
learning rate:0.001 use skip_connection use data_augmentation
dropout=0.5 alpha for mixup=0.5 optimizer:rmsprop number of
epoches:100</p>
<p>And best model found has been saved to "model_3_7_3.h5"</p>
</div>
<div class="cell markdown" id="5SXqrbFE--xi">
<p>For questions <strong>3.8 and 3.9</strong>, you can reuse code in
lectures or labs. You should not use third-party libraries such as
ClevenHans.</p>
</div>
<section id="question-38-attack-your-model" class="cell markdown"
id="T1UCPlrK--xi" data-tags="[]">
<h3><span style="color:#0b486b">Question 3.8: Attack your
model</span></h3>
<p>Attack your best obtained model with PGD, MIM, and FGSM attacks with
<span
class="math inline"><em>Ïµ</em>â€„=â€„0.0313,â€†<em>k</em>â€„=â€„20,â€†<em>Î·</em>â€„=â€„0.002</span>
on the testing set. Write the code for the attacks and report the robust
accuracies. Also choose a random set of 20 clean images in the testing
set and visualize the original and attacked images.</p>
<div style="text-align: right"> <span style="color:red">[5 points]</span> </div>
</section>
<div class="cell code" id="kyhCeBrW--xi" data-tags="[]">
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You can add more cells if necessary</span></span></code></pre></div>
</div>
<section id="question-39-train-a-robust-model" class="cell markdown"
id="IJg1QZeT--xj">
<h3><span style="color:#0b486b">Question 3.9: Train a robust
model</span></h3>
<p>Train a robust model using adversarial training with PGD <span
class="math inline"><em>Ïµ</em>â€„=â€„0.0313,â€†<em>k</em>â€„=â€„10,â€†<em>Î·</em>â€„=â€„0.002</span>.
Write the code for the adversarial training and report the robust
accuracies. After finishing the training, you need to store your best
robust model in the folder <code>./models</code> and load the model to
evaluate the robust accuracies for PGD, MIM, and FGSM attacks with <span
class="math inline"><em>Ïµ</em>â€„=â€„0.0313,â€†<em>k</em>â€„=â€„20,â€†<em>Î·</em>â€„=â€„0.002</span>
on the testing set.</p>
<div style="text-align: right"> <span style="color:red">[5 points]</span> </div>
</section>
<div class="cell code" id="F783cEkJ--xj" data-tags="[]">
<div class="sourceCode" id="cb64"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You can add more cells if necessary</span></span></code></pre></div>
</div>
<div class="cell markdown" id="hlaA7YxS--xj">
<p>The following is an exploring question with bonus points. It is great
if you try to do this question, but it is <strong>totally
optional</strong>. In this question, we will investigate a recent SOTA
technique to improve the generalization ability of deep nets named
<em>Sharpness-Aware Minimization (SAM)</em> (<a
href="https://openreview.net/pdf?id=6Tm1mposlrM">link to the main
paper</a>). Furthermore, SAM is simple and efficient technique, but
roughly doubles the training time due to its required computation. If
you have an idea to improve SAM, it would be a great paper to top-tier
venues in machine learning and computer vision. Highly recommend to give
it a try.</p>
</div>
<section id="question-310-bonus-question" class="cell markdown"
id="Tdc5UREf--xk">
<h3><span style="color:#0b486b">Question 3.10</span> (bonus
question)</h3>
<p>Read the SAM paper (<a
href="https://openreview.net/pdf?id=6Tm1mposlrM">link to the main
paper</a>). Try to apply this technique to the best obtained model and
report the results. For the purpose of implementing SAM, we can flexibly
add more cells and extensions to the <code>model.py</code> file.</p>
<div style="text-align: right"> <span style="color:red">[5 points]</span> </div>
</section>
<div class="cell code" id="ihGuzf3A--xk" data-tags="[]">
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You can add more cells if necessary</span></span></code></pre></div>
</div>
<div class="cell markdown" id="gZImCfuM--xk">
<hr />
<p><strong><div style="text-align: center">
<span style="color:black">END OF ASSIGNMENT</span> </div></strong>
<strong><div style="text-align: center"> <span style="color:black">GOOD
LUCK WITH YOUR ASSIGNMENT 1!</span> </div></strong></p>
</div>
</body>
</html>
