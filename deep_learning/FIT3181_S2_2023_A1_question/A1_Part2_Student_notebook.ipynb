{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Deep Neural Networks (DNN) </span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[Total marks for this part: 30 points]<span></div>\n",
    "\n",
    "The first part of this assignment is for you to demonstrate your basis knowledge in deep learning that you have acquired from the lectures and tutorials materials. Most of the contents in this assignment are drawn from **the tutorials covered from weeks 1 to 5**. Going through these materials before attempting this assignment is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to work with the **EMNIST** dataset for *image recognition task*. This dataset can be installed with the command <kbd>pip install emnist</kbd>. It has the exact same format as MNIST (grayscale images of 28 Ã— 28 pixels), but the images represent handwritten letters rather than handwritten digits, so the problem is more challenging than MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Question 2.1: Load the EMNIST datasets and process data</span> \n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[4 points]</span> </div>\n",
    "\n",
    "We first use functions in the package **emnist**, namely <kbd>extract_training_samples</kbd> and <kbd>extract_test_samples</kbd>, to load the training and testing sets. We also want to encode labels using an ordinal encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emnist import extract_training_samples, extract_test_samples\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124800, 28, 28) (124800,) (20800, 28, 28) (20800,)\n"
     ]
    }
   ],
   "source": [
    "X_train_raw, y_train_raw = extract_training_samples('letters')\n",
    "X_test_raw, y_test_raw = extract_test_samples('letters')\n",
    "print(X_train_raw.shape, y_train_raw.shape, X_test_raw.shape, y_test_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23  7 16 ... 13 15 19]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22  6 15 ... 12 14 18]\n",
      "[ 0  0  0 ... 25 25 25]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_full = le.fit_transform(y_train_raw)\n",
    "print(y_train_full)\n",
    "y_test = le.transform(y_test_raw)\n",
    "print(y_test)\n",
    "\n",
    "class_ids = np.unique(y_train_full)\n",
    "n_classes = len(class_ids)\n",
    "print(class_ids)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of training and testing data are $(num\\_train, 28, 28)$ and $(num\\_test, 28, 28)$, where $num\\_train$ and $num\\_test$ are number of training and testing images respectively. We next convert them to arrays of vectors which have shape $(num\\_train, 784)$ and $(num\\_test, 784)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124800, 784) (124800,)\n",
      "(20800, 784) (20800,)\n"
     ]
    }
   ],
   "source": [
    "num_train = X_train_raw.shape[0]\n",
    "num_test = X_test_raw.shape[0]\n",
    "X_train_full = X_train_raw.reshape(num_train,-1)\n",
    "X_test = X_test_raw.reshape(num_test,-1)\n",
    "print(X_train_full.shape, y_train_full.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124800\n",
      "20800\n"
     ]
    }
   ],
   "source": [
    "print(num_train)\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Question 2.2: Split data into training, validation, and testing datasets</span>\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[2 points]</span> </div>\n",
    "\n",
    "You need to write the code to address the following requirements:\n",
    "- Use $10 \\%$ of `X_train_full`, `y_train_full` for validation and the rest of `X_train_full`, `y_train_full` for training. This splits `X_train_full` and `y_train_full` into `X_train`, `y_train` ($90 \\%$) and `X_val`, `y_val` ($10 \\%$).\n",
    "- Finally, scale the pixels of `X_train`, `X_val`, and `X_test` to $[0,1]$) (i.e., $X = X/255.0$).\n",
    "\n",
    "You have now the separate training, validation, and testing sets for training your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112320, 784) (112320,)\n",
      "(12480, 784) (12480,)\n",
      "(20800, 784) (20800,)\n",
      "0.0 1.0 0.0 1.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1,random_state=42)\n",
    "X_train, X_val, X_test = X_train/255.0, X_val/255.0, X_test/255.0\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)   \n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(X_train.min(), X_train.max(), X_val.min(), X_val.max(), X_test.min(), X_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Question 2.3: Visualize some images in the training set with labels</span>\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n",
    "\n",
    "You are required to write the code to **randomly** show $36$ images in X_train_full_img (which is an array of images) with labels as in the following figure.\n",
    "\n",
    "<img src=\"Figures/emnist_grid.png\" width=\"450\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "[ 83499  85878  60309  11450  89593 111168  20770  33064  81719  63628\n",
      "  43353  49606  86387 106363  35211  67034   1313  93929   3528  89371\n",
      "  15772  60712  73455  73904  86984 113272  81752  99129  67153  82181\n",
      "  29493   6809 106114 102723  69029  94651]\n"
     ]
    }
   ],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "# You can add more cells if necessary\n",
    "\n",
    "\n",
    "#######visualization not finished########\n",
    "\n",
    "labels = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "print(len(labels))\n",
    "\n",
    "random_36 = np.random.choice(X_train_full.shape[0],36,replace = False)\n",
    "print(random_36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(6,6)\n",
    "axes = axes.ravel()\n",
    "plot_36 = X_train_raw[random_36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3\n",
      "    2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  30 111\n",
      "   64   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1  71 223\n",
      "  126   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20 159 247\n",
      "  174   2   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20 172 255\n",
      "  218   4   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5 130 252\n",
      "  219   4   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 115 247\n",
      "  219   4   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  23 174\n",
      "  234  23   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5 130\n",
      "  250  83   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 128\n",
      "  252 140   9   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 127\n",
      "  252 206  33   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51\n",
      "  224 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  33\n",
      "  206 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "  144 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  21\n",
      "  174 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  37\n",
      "  218 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  21\n",
      "  174 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   5  30   5   0   0   0   0   0   0   0   5\n",
      "  130 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  15 136  59   7   0   0   0   0   0   0   4\n",
      "  129 219  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  34 210 162  35   1   0   0   0   0  23\n",
      "  174 217  37   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1  80 180 162  35   1   0   0   2  83\n",
      "  234 174  21   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4  80 212 163  36   2   1  12 141\n",
      "  251 116   4   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   9  91 201 173  83  53 129 225\n",
      "  234  34   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  24 155 233 243 222 172\n",
      "   79   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1  20  83 113  50  21\n",
      "    3   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   3   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(axes))\n",
    "print(plot_36[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(36):\n",
    "    axes[i].imshow(plot_36[i],cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8cf94b5f40>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axes[0][0].matshow(X_train_raw[10],cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Question 2.4: Write code for the feed-forward neural net using TF 2.x</span>\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now develop a feed-forward neural network with the architecture $784 \\rightarrow 20(ReLU) \\rightarrow 40(ReLU) \\rightarrow 10(softmax)$. You can choose your own way to implement your network and an optimizer of interest. You should train model in $20$ epochs and evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 19:44:36.206891: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "random_seed = 3181\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "# You can add more cells if necessary\n",
    "model = Sequential()\n",
    "model.add(Dense(units=20, input_shape=(784,),activation='relu'))\n",
    "model.add(Dense(units=40,activation='relu'))\n",
    "model.add(Dense(units=26,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 20)                15700     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 40)                840       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 26)                1066      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,606\n",
      "Trainable params: 17,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "num_neurons_last_layer = model.layers[-1].output_shape[-1]\n",
    "print(num_neurons_last_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(y_train)\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3510/3510 [==============================] - 8s 2ms/step - loss: 1.0945 - accuracy: 0.6768 - val_loss: 0.8191 - val_accuracy: 0.7542\n",
      "Epoch 2/20\n",
      "3510/3510 [==============================] - 5s 2ms/step - loss: 0.7432 - accuracy: 0.7791 - val_loss: 0.7000 - val_accuracy: 0.7944\n",
      "Epoch 3/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.6586 - accuracy: 0.8020 - val_loss: 0.6417 - val_accuracy: 0.8104\n",
      "Epoch 4/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.6121 - accuracy: 0.8156 - val_loss: 0.6094 - val_accuracy: 0.8125\n",
      "Epoch 5/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 0.5803 - accuracy: 0.8252 - val_loss: 0.5953 - val_accuracy: 0.8183\n",
      "Epoch 6/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.5590 - accuracy: 0.8308 - val_loss: 0.5682 - val_accuracy: 0.8285\n",
      "Epoch 7/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.5421 - accuracy: 0.8351 - val_loss: 0.5530 - val_accuracy: 0.8296\n",
      "Epoch 8/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.5272 - accuracy: 0.8389 - val_loss: 0.5517 - val_accuracy: 0.8316\n",
      "Epoch 9/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.5158 - accuracy: 0.8423 - val_loss: 0.5235 - val_accuracy: 0.8361\n",
      "Epoch 10/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 0.5063 - accuracy: 0.8445 - val_loss: 0.5301 - val_accuracy: 0.8362\n",
      "Epoch 11/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 0.4972 - accuracy: 0.8477 - val_loss: 0.5218 - val_accuracy: 0.8397\n",
      "Epoch 12/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4898 - accuracy: 0.8493 - val_loss: 0.5207 - val_accuracy: 0.8400\n",
      "Epoch 13/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4824 - accuracy: 0.8502 - val_loss: 0.5102 - val_accuracy: 0.8405\n",
      "Epoch 14/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4754 - accuracy: 0.8528 - val_loss: 0.5083 - val_accuracy: 0.8438\n",
      "Epoch 15/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4708 - accuracy: 0.8541 - val_loss: 0.5053 - val_accuracy: 0.8439\n",
      "Epoch 16/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4659 - accuracy: 0.8556 - val_loss: 0.5005 - val_accuracy: 0.8494\n",
      "Epoch 17/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4614 - accuracy: 0.8560 - val_loss: 0.5052 - val_accuracy: 0.8429\n",
      "Epoch 18/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4581 - accuracy: 0.8574 - val_loss: 0.5080 - val_accuracy: 0.8444\n",
      "Epoch 19/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.4547 - accuracy: 0.8583 - val_loss: 0.4959 - val_accuracy: 0.8462\n",
      "Epoch 20/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 0.4509 - accuracy: 0.8597 - val_loss: 0.4976 - val_accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "logdir = \"tf_logs/example01\"\n",
    "\n",
    "# Init a tensorboard_callback \n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val),callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 1s 1ms/step - loss: 0.5131 - accuracy: 0.8468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5130547285079956, 0.8468269109725952]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Question 2.5: Tuning hyper-parameters with grid search</span>\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[6 points]</span> </div>\n",
    "\n",
    "Assume that you need to tune the number of neurons on the first and second hidden layers $n_1 \\in \\{20, 40\\}$, $n_2 \\in \\{20, 40\\}$  and the used activation function  $act \\in \\{sigmoid, tanh, relu\\}$. The network has the architecture pattern $784 \\rightarrow n_1 (act) \\rightarrow n_2(act) \\rightarrow 10(softmax)$ where $n_1, n_2$, and $act$ are in their grides. Write the code to tune the hyper-parameters $n_1, n_2$, and $act$. Note that you can freely choose the optimizer and learning rate of interest for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n1': 40, 'n2': 40, 'activation': 'relu', 'val_accuracy': 0.873317301273346}\n"
     ]
    }
   ],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "# You can add more cells if necessary\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "neurons_1 = [20, 40]\n",
    "neurons_2 = [20, 40]\n",
    "activations = ['sigmoid', 'tanh', 'relu']\n",
    "\n",
    "def build_model(n1, n2, activation_function):\n",
    "    model = Sequential([\n",
    "        Dense(n1, activation=activation_function, input_shape=(784,)),\n",
    "        Dense(n2, activation=activation_function),\n",
    "        Dense(26, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "results = []\n",
    "\n",
    "for n1 in neurons_1:\n",
    "    for n2 in neurons_2:\n",
    "        for act in activations:\n",
    "            model = build_model(n1, n2, act)\n",
    "            history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), verbose=0)\n",
    "            val_accuracy = history.history['val_accuracy'][-1]  # use the last epoch's accuracy\n",
    "            results.append({\n",
    "                'n1': n1,\n",
    "                'n2': n2,\n",
    "                'activation': act,\n",
    "                'val_accuracy': val_accuracy\n",
    "            })\n",
    "            \n",
    "    best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the best parameter found is: n1:40,n2:40,activation:'relu', the validation accuracy is 0.873317301273346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Question 2.6: Experimenting with the **label smoothing** technique</span>\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[8 points]</span> </div>\n",
    "\n",
    "Implement the label smoothing technique (i.e., [link for main paper](https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf) from Goeff Hinton) by yourself. Note that you cannot use the built-in label-smoothing loss function in TF2.x. Try the label smoothing technique with $\\alpha =0.1, 0.15, 0.2$ and report the performances. You need to examine the label smoothing technique with the best architecture obtained in **Question 2.5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3510/3510 [==============================] - 12s 2ms/step - loss: 1.4916 - accuracy: 0.7159 - val_loss: 0.7900 - val_accuracy: 0.7910\n",
      "Epoch 2/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2075 - accuracy: 0.8211 - val_loss: 0.6687 - val_accuracy: 0.8299\n",
      "Epoch 3/20\n",
      "3510/3510 [==============================] - 5s 1ms/step - loss: 1.1434 - accuracy: 0.8413 - val_loss: 0.6154 - val_accuracy: 0.8456\n",
      "Epoch 4/20\n",
      "3510/3510 [==============================] - 5s 2ms/step - loss: 1.1057 - accuracy: 0.8531 - val_loss: 0.5962 - val_accuracy: 0.8517\n",
      "Epoch 5/20\n",
      "3510/3510 [==============================] - 5s 2ms/step - loss: 1.0780 - accuracy: 0.8620 - val_loss: 0.5663 - val_accuracy: 0.8577\n",
      "Epoch 6/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0606 - accuracy: 0.8668 - val_loss: 0.5613 - val_accuracy: 0.8599\n",
      "Epoch 7/20\n",
      "3510/3510 [==============================] - 5s 2ms/step - loss: 1.0484 - accuracy: 0.8710 - val_loss: 0.5443 - val_accuracy: 0.8634\n",
      "Epoch 8/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0373 - accuracy: 0.8745 - val_loss: 0.5401 - val_accuracy: 0.8659\n",
      "Epoch 9/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0297 - accuracy: 0.8769 - val_loss: 0.5256 - val_accuracy: 0.8673\n",
      "Epoch 10/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0220 - accuracy: 0.8801 - val_loss: 0.5215 - val_accuracy: 0.8681\n",
      "Epoch 11/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0170 - accuracy: 0.8815 - val_loss: 0.5270 - val_accuracy: 0.8709\n",
      "Epoch 12/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0111 - accuracy: 0.8836 - val_loss: 0.5211 - val_accuracy: 0.8720\n",
      "Epoch 13/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0064 - accuracy: 0.8838 - val_loss: 0.5196 - val_accuracy: 0.8727\n",
      "Epoch 14/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.0019 - accuracy: 0.8860 - val_loss: 0.5167 - val_accuracy: 0.8731\n",
      "Epoch 15/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.9987 - accuracy: 0.8867 - val_loss: 0.5197 - val_accuracy: 0.8706\n",
      "Epoch 16/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.9951 - accuracy: 0.8884 - val_loss: 0.5108 - val_accuracy: 0.8765\n",
      "Epoch 17/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.9919 - accuracy: 0.8891 - val_loss: 0.5181 - val_accuracy: 0.8719\n",
      "Epoch 18/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.9886 - accuracy: 0.8903 - val_loss: 0.5104 - val_accuracy: 0.8748\n",
      "Epoch 19/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.9860 - accuracy: 0.8914 - val_loss: 0.4914 - val_accuracy: 0.8769\n",
      "Epoch 20/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 0.9838 - accuracy: 0.8920 - val_loss: 0.5012 - val_accuracy: 0.8777\n",
      "390/390 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.8777\n",
      "Epoch 1/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 1.6773 - accuracy: 0.7209 - val_loss: 0.8476 - val_accuracy: 0.7934\n",
      "Epoch 2/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4304 - accuracy: 0.8187 - val_loss: 0.7353 - val_accuracy: 0.8292\n",
      "Epoch 3/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.3701 - accuracy: 0.8397 - val_loss: 0.6850 - val_accuracy: 0.8438\n",
      "Epoch 4/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.3361 - accuracy: 0.8515 - val_loss: 0.6653 - val_accuracy: 0.8504\n",
      "Epoch 5/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.3139 - accuracy: 0.8596 - val_loss: 0.6240 - val_accuracy: 0.8588\n",
      "Epoch 6/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 1.2996 - accuracy: 0.8638 - val_loss: 0.6258 - val_accuracy: 0.8603\n",
      "Epoch 7/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2875 - accuracy: 0.8682 - val_loss: 0.6121 - val_accuracy: 0.8623\n",
      "Epoch 8/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2772 - accuracy: 0.8716 - val_loss: 0.6048 - val_accuracy: 0.8671\n",
      "Epoch 9/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2688 - accuracy: 0.8739 - val_loss: 0.5921 - val_accuracy: 0.8674\n",
      "Epoch 10/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2613 - accuracy: 0.8769 - val_loss: 0.5836 - val_accuracy: 0.8683\n",
      "Epoch 11/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 1.2554 - accuracy: 0.8793 - val_loss: 0.5832 - val_accuracy: 0.8684\n",
      "Epoch 12/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2502 - accuracy: 0.8807 - val_loss: 0.5792 - val_accuracy: 0.8734\n",
      "Epoch 13/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2449 - accuracy: 0.8829 - val_loss: 0.5763 - val_accuracy: 0.8736\n",
      "Epoch 14/20\n",
      "3510/3510 [==============================] - 7s 2ms/step - loss: 1.2412 - accuracy: 0.8834 - val_loss: 0.5732 - val_accuracy: 0.8701\n",
      "Epoch 15/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.2374 - accuracy: 0.8843 - val_loss: 0.5787 - val_accuracy: 0.8688\n",
      "Epoch 16/20\n",
      "3510/3510 [==============================] - 8s 2ms/step - loss: 1.2345 - accuracy: 0.8850 - val_loss: 0.5817 - val_accuracy: 0.8756\n",
      "Epoch 17/20\n",
      "3510/3510 [==============================] - 8s 2ms/step - loss: 1.2315 - accuracy: 0.8861 - val_loss: 0.5771 - val_accuracy: 0.8704\n",
      "Epoch 18/20\n",
      "3510/3510 [==============================] - 8s 2ms/step - loss: 1.2282 - accuracy: 0.8884 - val_loss: 0.5807 - val_accuracy: 0.8732\n",
      "Epoch 19/20\n",
      "3510/3510 [==============================] - 8s 2ms/step - loss: 1.2259 - accuracy: 0.8882 - val_loss: 0.5653 - val_accuracy: 0.8728\n",
      "Epoch 20/20\n",
      "3510/3510 [==============================] - 9s 3ms/step - loss: 1.2240 - accuracy: 0.8891 - val_loss: 0.5563 - val_accuracy: 0.8744\n",
      "390/390 [==============================] - 1s 2ms/step - loss: 0.5563 - accuracy: 0.8744\n",
      "Epoch 1/20\n",
      "3510/3510 [==============================] - 9s 2ms/step - loss: 1.8343 - accuracy: 0.7285 - val_loss: 0.8802 - val_accuracy: 0.7972\n",
      "Epoch 2/20\n",
      "3510/3510 [==============================] - 8s 2ms/step - loss: 1.6145 - accuracy: 0.8235 - val_loss: 0.7745 - val_accuracy: 0.8329\n",
      "Epoch 3/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.5606 - accuracy: 0.8433 - val_loss: 0.7323 - val_accuracy: 0.8502\n",
      "Epoch 4/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.5304 - accuracy: 0.8553 - val_loss: 0.7160 - val_accuracy: 0.8553\n",
      "Epoch 5/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.5102 - accuracy: 0.8622 - val_loss: 0.6864 - val_accuracy: 0.8584\n",
      "Epoch 6/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4968 - accuracy: 0.8666 - val_loss: 0.7023 - val_accuracy: 0.8590\n",
      "Epoch 7/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4865 - accuracy: 0.8701 - val_loss: 0.6847 - val_accuracy: 0.8627\n",
      "Epoch 8/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4775 - accuracy: 0.8733 - val_loss: 0.6571 - val_accuracy: 0.8655\n",
      "Epoch 9/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4704 - accuracy: 0.8756 - val_loss: 0.6493 - val_accuracy: 0.8700\n",
      "Epoch 10/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4641 - accuracy: 0.8771 - val_loss: 0.6393 - val_accuracy: 0.8650\n",
      "Epoch 11/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4592 - accuracy: 0.8796 - val_loss: 0.6414 - val_accuracy: 0.8714\n",
      "Epoch 12/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4548 - accuracy: 0.8805 - val_loss: 0.6504 - val_accuracy: 0.8671\n",
      "Epoch 13/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4513 - accuracy: 0.8817 - val_loss: 0.6643 - val_accuracy: 0.8700\n",
      "Epoch 14/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4477 - accuracy: 0.8830 - val_loss: 0.6309 - val_accuracy: 0.8731\n",
      "Epoch 15/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4443 - accuracy: 0.8833 - val_loss: 0.6489 - val_accuracy: 0.8738\n",
      "Epoch 16/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4418 - accuracy: 0.8836 - val_loss: 0.6316 - val_accuracy: 0.8724\n",
      "Epoch 17/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4395 - accuracy: 0.8847 - val_loss: 0.6433 - val_accuracy: 0.8703\n",
      "Epoch 18/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4367 - accuracy: 0.8857 - val_loss: 0.6435 - val_accuracy: 0.8727\n",
      "Epoch 19/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4344 - accuracy: 0.8862 - val_loss: 0.6464 - val_accuracy: 0.8724\n",
      "Epoch 20/20\n",
      "3510/3510 [==============================] - 6s 2ms/step - loss: 1.4321 - accuracy: 0.8865 - val_loss: 0.6304 - val_accuracy: 0.8736\n",
      "390/390 [==============================] - 0s 1ms/step - loss: 0.6304 - accuracy: 0.8736\n",
      "Alpha: 0.1, Validation Accuracy: 0.8777243494987488\n",
      "Alpha: 0.15, Validation Accuracy: 0.8743589520454407\n",
      "Alpha: 0.2, Validation Accuracy: 0.8736377954483032\n"
     ]
    }
   ],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "# You can add more cells if necessary\n",
    "\n",
    "def label_smoothing(labels, alpha, num_classes):\n",
    "    return (1 - alpha) * labels + (alpha / num_classes)\n",
    "\n",
    "\n",
    "def build_best_model():\n",
    "    model = Sequential([\n",
    "        Dense(40, activation='relu', input_shape=(784,)),\n",
    "        Dense(40, activation='relu'),\n",
    "        Dense(26, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "alphas = [0.1, 0.15, 0.2]\n",
    "results = []\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "for alpha in alphas:\n",
    "    y_train_hot = to_categorical(y_train) #convert to hot vector format\n",
    "    y_val_hot = to_categorical(y_val)\n",
    "    \n",
    "    smoothed_labels = label_smoothing(y_train_hot, alpha, num_classes=26)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = build_best_model()\n",
    "    model.fit(X_train, smoothed_labels, epochs=20, validation_data=(X_val, y_val_hot))\n",
    "    \n",
    "    \n",
    "    _, val_accuracy = model.evaluate(X_val, y_val_hot)\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'val_accuracy': val_accuracy\n",
    "    })\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Alpha: {result['alpha']}, Validation Accuracy: {result['val_accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "Alpha: 0.1, Validation Accuracy: 0.8777243494987488\n",
    "Alpha: 0.15, Validation Accuracy: 0.8743589520454407\n",
    "Alpha: 0.2, Validation Accuracy: 0.8736377954483032\n",
    "\n",
    "Compared with previous best structure accuracy 0.873317301273346, alpha=0.1 improves the accuracy by 0.004, alpha=0.15 improves the accuracy by 0.001, alpha=0.2 improves the accuracy by 0.0003.\n",
    "\n",
    "It turns out that alpha smoothing does work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
